{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Global imports - be careful with these as they may break existing notebooks that call this common_functions notebook.\r\n",
        "from notebookutils import mssparkutils\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\r\n",
        "import sys\r\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Simple Watermark functions\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define the schema\r\n",
        "watermark_schema = StructType([\r\n",
        "    StructField(\"process_name\", StringType(), False),\r\n",
        "    StructField(\"datetime_watermark\", TimestampType(), False)\r\n",
        "])\r\n",
        "\r\n",
        "# Set datetime watermark\r\n",
        "def set_datetime_watermark_value_for_process(process_name: str, watermark: str, delta_table_path: str):\r\n",
        "    from pyspark.sql import Row\r\n",
        "    from pyspark.sql.functions import col\r\n",
        "    from datetime import datetime\r\n",
        "    from delta.tables import DeltaTable\r\n",
        "    from pyspark.sql.utils import AnalysisException\r\n",
        "\r\n",
        "    print(f\"Setting watermark value of {watermark} for {process_name} in {delta_table_path}\")\r\n",
        "    #df = spark.createDataFrame([Row(process_name=process_name, datetime_watermark=datetime.strptime(watermark, '%Y-%m-%d %H:%M:%S'))], watermark_schema)\r\n",
        "    df = spark.createDataFrame([Row(process_name=process_name, datetime_watermark=watermark)], watermark_schema)\r\n",
        "    try:\r\n",
        "        df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"True\").save(delta_table_path)\r\n",
        "    except Exception as e:\r\n",
        "        print(\"error=\",str(e))\r\n",
        "        raise Exception('Could not set watermark')\r\n",
        "    \r\n",
        "# Get datetime watermark\r\n",
        "def get_datetime_watermark_value_for_process(process_name: str, delta_table_path: str):\r\n",
        "    from pyspark.sql import Row\r\n",
        "    from pyspark.sql.functions import col\r\n",
        "    from datetime import datetime\r\n",
        "    from delta.tables import DeltaTable\r\n",
        "    from pyspark.sql.utils import AnalysisException\r\n",
        "\r\n",
        "    print(f\"Getting watermark value for {process_name} in {delta_table_path}\")\r\n",
        "    # Try to retrive the delta table watermark. Give an ancient datetime if none exists.\r\n",
        "    try:\r\n",
        "        df_watermark = spark.read.format(\"delta\").load(delta_table_path)\r\n",
        "        watermark_row = (\r\n",
        "            df_watermark.filter(col(\"process_name\") == process_name)\r\n",
        "            .select(col(\"datetime_watermark\"))\r\n",
        "            .orderBy(col(\"datetime_watermark\").desc())\r\n",
        "            .limit(1)\r\n",
        "            .collect()\r\n",
        "        )\r\n",
        "        watermark_value = watermark_row[0][\"datetime_watermark\"]\r\n",
        "        return watermark_value\r\n",
        "    except Exception as e:\r\n",
        "        print(\"error=\",str(e))\r\n",
        "        # Set the watermark to a date way in the past\r\n",
        "        watermark_value = datetime(1900, 1, 1)\r\n",
        "        return watermark_value\r\n",
        "        #raise Exception('Could not get watermark')\r\n",
        "\r\n",
        "print(\"Declared simple watermark functions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Check if File Exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {},
      "source": [
        "def file_exists(adls_path):\r\n",
        "  try:\r\n",
        "    if len(mssparkutils.fs.ls(adls_path)) > 0:\r\n",
        "      print(\"file Exists::\", adls_path)\r\n",
        "      return True\r\n",
        "  except Exception as e:\r\n",
        "    if 'java.io.FileNotFoundException' in str(e):\r\n",
        "      return False\r\n",
        "    else:\r\n",
        "      raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# write dataframe to ADLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def write_to_file(df,write_path, p_mode = 'append'):\r\n",
        "    print('writing data to '+write_path+'......')\r\n",
        "    try:\r\n",
        "      time.sleep(10)  \r\n",
        "      df.write.format(\"delta\").mode(p_mode).save(write_path)      \r\n",
        "    except Exception as e:\r\n",
        "        print(\"error::::\",e)\r\n",
        "        traceback.print_exc()\r\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create target table if not exists and return delta path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def get_target_table_ref(db, target_table, target_path):\r\n",
        "    try:\r\n",
        "      sql = f\"CREATE TABLE IF NOT EXISTS {db}.{target_table} USING DELTA LOCATION '{target_path}'\"\r\n",
        "      print(\"getTargetTable()::\",sql)\r\n",
        "      spark.sql(sql)\r\n",
        "      delta_table = DeltaTable.forPath(spark, target_path)  \r\n",
        "      return delta_table\r\n",
        "    except Exception as e:\r\n",
        "      raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Perform Merge Opertaion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def execute_merge(target_table, staged_updates, where_condition): ## for ctl_config_varaibles: where_condition :\r\n",
        "      try:\r\n",
        "        target_table.alias(\"target\").merge(staged_updates.alias(\"source\"),where_condition)\\\r\n",
        "               .whenMatchedUpdateAll()\\\r\n",
        "              .whenNotMatchedInsertAll()\\\r\n",
        "              .execute()\r\n",
        "        return target_table.toDF()\r\n",
        "      except Exception as e:\r\n",
        "         raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# remove files or delete folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def del_folder(p_folder):\r\n",
        "  if if_file_exists(p_folder):\r\n",
        "    print(f\"file already exists - deleting {p_folder}\")  \r\n",
        "    mssparkutils.fs.rm(p_folder,True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Set ADLS path based on env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "env_dict = {'dev':'azwwwnonproddevadapsyn01',\r\n",
        "            'prod': 'azwwwprodprdadapsyn01',\r\n",
        "            'prod_backup': 'azwwwprodprdadapsyn02',\r\n",
        "            'test': 'azwwwnonprodtestadapsyn01'\r\n",
        "            }\r\n",
        "env_var  = mssparkutils.env.getWorkspaceName()\r\n",
        "\r\n",
        "if env_var == env_dict['dev']:\r\n",
        "  raw_adls_path = 'abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/'\r\n",
        "  bronze_adls_path =  'abfss://bronze@azwwwnonproddevadapadls.dfs.core.windows.net/'\r\n",
        "  kv_name = 'az-www-dev-adap-kv'\r\n",
        "  jdbcHostname = 'az-www-datahub-nonprod-dev-adap-sql.database.windows.net'\r\n",
        "  silver_adls_path =  'abfss://silver@azwwwnonproddevadapadls.dfs.core.windows.net/'\r\n",
        "  gold_adls_path = 'abfss://gold@azwwwnonproddevadapadls.dfs.core.windows.net/'\r\n",
        "  export_adls_path = 'abfss://export@azwwwnonproddevadapadls.dfs.core.windows.net/'\r\n",
        "  blob_adls_path = 'https://azwwwnonproddevadapadls.blob.core.windows.net'\r\n",
        "  sftp_adls_path = 'azwwwnonproddevsftp.dfs.core.windows.net'\r\n",
        "\r\n",
        "elif env_var == env_dict['prod'] or env_var == env_dict['prod_backup']:\r\n",
        "  raw_adls_path = 'abfss://raw@azwwwprodprdadapadls.dfs.core.windows.net/'\r\n",
        "  bronze_adls_path =  'abfss://bronze@azwwwprodprdadapadls.dfs.core.windows.net/'\r\n",
        "  kv_name = 'az-www-prd-adap-kv'\r\n",
        "  jdbcHostname = 'az-www-datahub-prod-prd-adap-sql.database.windows.net'\r\n",
        "  silver_adls_path = 'abfss://silver@azwwwprodprdadapadls.dfs.core.windows.net/'\r\n",
        "  gold_adls_path = 'abfss://gold@azwwwprodprdadapadls.dfs.core.windows.net/'\r\n",
        "  export_adls_path = 'abfss://export@azwwwprodprdadapadls.dfs.core.windows.net/'\r\n",
        "  blob_adls_path = 'https://azwwwprodprdadapadls.blob.core.windows.net'\r\n",
        "  sftp_adls_path = 'azwwwprodprdsftp.dfs.core.windows.net'\r\n",
        "\r\n",
        "elif env_var == env_dict['test']:\r\n",
        "  raw_adls_path = 'abfss://raw@azwwwnonprodtestadapadls.dfs.core.windows.net/'\r\n",
        "  bronze_adls_path =  'abfss://bronze@azwwwnonprodtestadapadls.dfs.core.windows.net/'\r\n",
        "  kv_name = 'az-www-test-adap-kv'\r\n",
        "  jdbcHostname = 'az-www-datahub-nonprod-test-adap-sql.database.windows.net'\r\n",
        "  silver_adls_path = 'abfss://silver@azwwwnonprodtestadapadls.dfs.core.windows.net/'\r\n",
        "  gold_adls_path = 'abfss://gold@azwwwnonprodtestadapadls.dfs.core.windows.net/'\r\n",
        "  export_adls_path = 'abfss://export@azwwwnonprodtestadapadls.dfs.core.windows.net/'\r\n",
        "  blob_adls_path = 'https://azwwwnonprodtestadapadls.blob.core.windows.net'\r\n",
        "  sftp_adls_path = 'azwwwnonprodtestsftp.dfs.core.windows.net'\r\n",
        "\r\n",
        "print(\"raw_adls_path::\",raw_adls_path, \"\\nbronze_adls_path::\",bronze_adls_path,\"\\nsilver_adls_path::\",silver_adls_path,\"\\ngold_adls_path::\",gold_adls_path,\"\\nexport_adls_path::\",export_adls_path,\"\\nblob_adls_path::\",blob_adls_path)\r\n",
        "print(\"kv_name::\",kv_name)\r\n",
        "print(\"sftp_adls_path::\", sftp_adls_path)\r\n",
        "print(f\"jdbcHostname: {jdbcHostname}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def has_column(df, col):\r\n",
        "    from pyspark.sql.utils import AnalysisException\r\n",
        "\r\n",
        "    try:\r\n",
        "        df[col]\r\n",
        "        return True\r\n",
        "    except AnalysisException:\r\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#print(\"getting jdbc credentials...\")  \r\n",
        "# TODO: REMOVE OLD CODE\r\n",
        "# sc = SparkSession.builder.getOrCreate()  \r\n",
        "# token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary  \r\n",
        "# jdbcPassword = token_library.getSecret(kv_name, \"SqlAdmin\", \"ls_kv_adap\")  \r\n",
        "jdbcPassword = mssparkutils.credentials.getSecret(kv_name, \"SqlAdmin\", 'ls_kv_adap')\r\n",
        "#print(jdbcPassword)\r\n",
        "\r\n",
        "jdbcPort = 1433\r\n",
        "\r\n",
        "jdbcDatabase = \"dw\"\r\n",
        "jdbcUsername = \"sqlAdmin\"\r\n",
        "jdbcDriver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
        "jdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};databaseName={jdbcDatabase}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Function to tell us if a folder exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Function to tell us if a folder exists\r\n",
        "def folder_exists(abfss_path):\r\n",
        "    from notebookutils import mssparkutils\r\n",
        "    try:\r\n",
        "        mssparkutils.fs.ls(abfss_path)\r\n",
        "        return True\r\n",
        "    except:\r\n",
        "        return False\r\n",
        "\r\n",
        "print(\"delcared function: folder_exists(abfss_path)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Function to create a folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {},
      "source": [
        "def create_folder(abfss_path):\r\n",
        "    from notebookutils import mssparkutils\r\n",
        "    mssparkutils.fs.mkdirs(abfss_path)\r\n",
        "\r\n",
        "print(\"declared function: create_folder(abfss_path)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Function to deduplicate a dataframe based on key columns and a column to order partitions by in descening order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {},
      "source": [
        "# usage with ordering column:   df_deduped = deduplicate_dataframe_by_key_columns(df, key_columns=[\"id\", \"date\"], order_by_desc_column=\"updated_at\")\r\n",
        "# usage without ordeing column: df_deduped = deduplicate_dataframe_by_key_columns(df, key_columns=[\"id\", \"date\"])\r\n",
        "\r\n",
        "def deduplicate_dataframe_by_key_columns(df, key_columns, order_by_desc_column=None):\r\n",
        "    \"\"\"\r\n",
        "    Deduplicates a Spark DataFrame using key columns and an optional ordering column.\r\n",
        "\r\n",
        "    Parameters:\r\n",
        "    - df: Input Spark DataFrame.\r\n",
        "    - key_columns: List of columns to define the partition for deduplication.\r\n",
        "    - order_by_desc_column: Optional column name to order rows within partitions in descending order.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - Deduplicated DataFrame with original columns only.\r\n",
        "    \"\"\"\r\n",
        "    from pyspark.sql.window import Window\r\n",
        "    from pyspark.sql.functions import row_number, col, monotonically_increasing_id\r\n",
        "\r\n",
        "    if order_by_desc_column is not None:\r\n",
        "        window_spec = Window.partitionBy(*key_columns).orderBy(col(order_by_desc_column).desc())\r\n",
        "    else:\r\n",
        "        # Add a dummy column to force ordering so row_number() works\r\n",
        "        df = df.withColumn(\"__dummy_order_col\", monotonically_increasing_id())\r\n",
        "        window_spec = Window.partitionBy(*key_columns).orderBy(col(\"__dummy_order_col\"))\r\n",
        "\r\n",
        "    df_with_rownum = df.withColumn(\"_row_number\", row_number().over(window_spec))\r\n",
        "    deduped_df = df_with_rownum.filter(col(\"_row_number\") == 1).drop(\"_row_number\")\r\n",
        "\r\n",
        "    # Clean up dummy column if it was added\r\n",
        "    if \"__dummy_order_col\" in deduped_df.columns:\r\n",
        "        deduped_df = deduped_df.drop(\"__dummy_order_col\")\r\n",
        "\r\n",
        "    return deduped_df\r\n",
        "\r\n",
        "print(\"declared function: deduplicate_dataframe_by_key_columns(df, key_columns, order_by_desc_column=None)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Function to rename dataframe columns for delta table compatibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Example usage with dict for custom replacement logic:\r\n",
        "# replacement_dict = {\r\n",
        "#     '%': 'Pct',\r\n",
        "#     '&': 'And',\r\n",
        "#     ' ': '_'\r\n",
        "# }\r\n",
        "\r\n",
        "# df_clean = sanitize_column_names(df, replacement_dict=replacement_dict)\r\n",
        "# df_clean.printSchema()\r\n",
        "\r\n",
        "# Exmple usage without replacement_dict:\r\n",
        "# df_clean = sanitize_column_names(df)\r\n",
        "# df_clean.printSchema()\r\n",
        "\r\n",
        "\r\n",
        "def sanitize_column_names(df, replacement_dict=None):\r\n",
        "    \"\"\"\r\n",
        "    Sanitizes column names in a Spark DataFrame to be Delta-compatible.\r\n",
        "\r\n",
        "    - Applies custom replacements if `replacement_dict` is provided.\r\n",
        "    - Replaces remaining invalid characters with underscores.\r\n",
        "    - Removes leading/trailing underscores caused by cleanup.\r\n",
        "    - Prepends an underscore if the column name starts with a digit.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        df (DataFrame): Input Spark DataFrame.\r\n",
        "        replacement_dict (dict, optional): Dict of characters or substrings to replace.\r\n",
        "            Example: {'%': 'Pct', '&': 'And'}\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        DataFrame: Spark DataFrame with sanitized column names.\r\n",
        "    \"\"\"\r\n",
        "    import re\r\n",
        "    from pyspark.sql import DataFrame\r\n",
        "\r\n",
        "    if replacement_dict is None:\r\n",
        "        replacement_dict = {}\r\n",
        "\r\n",
        "    def sanitize(name):\r\n",
        "        # Apply custom replacements\r\n",
        "        for old, new in replacement_dict.items():\r\n",
        "            name = name.replace(old, new)\r\n",
        "        # Replace all remaining non-alphanumeric/underscore chars with underscores\r\n",
        "        name = re.sub(r'[^a-zA-Z0-9_]', '_', name)\r\n",
        "        # Strip underscores from beginning and end (but not middle)\r\n",
        "        name = name.strip('_')\r\n",
        "        # Prepend underscore if the cleaned name starts with a digit\r\n",
        "        if re.match(r'^\\d', name):\r\n",
        "            name = '_' + name\r\n",
        "        return name\r\n",
        "\r\n",
        "    renamed_df = df\r\n",
        "    for col_name in df.columns:\r\n",
        "        new_col_name = sanitize(col_name)\r\n",
        "        if col_name != new_col_name:\r\n",
        "            renamed_df = renamed_df.withColumnRenamed(col_name, new_col_name)\r\n",
        "\r\n",
        "    return renamed_df\r\n",
        "\r\n",
        "\r\n",
        "print(\"declared function: sanitize_column_names(df, replacement_dict=None)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# KeyValuePair getter and setter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Set up the JDBC connection to our dw Azure SQL DB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## The KeyValuePair Setter function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {},
      "source": [
        "def set_key_value(key_name, key_value):\r\n",
        "    # TODO: Get this to work with the predefined jdbc vars from above\r\n",
        "    import pyodbc\r\n",
        "    conn = pyodbc.connect(\r\n",
        "        \"Driver={ODBC Driver 18 for SQL Server};\"\r\n",
        "        f\"Server={jdbcHostname};\"\r\n",
        "        f\"Database=dw;\"\r\n",
        "        f\"UID={jdbcUsername};\"\r\n",
        "        f\"PWD={{{jdbcPassword}}};\"\r\n",
        "        f\"Encrypt=yes;\"\r\n",
        "        f\"TrustServerCertificate=no;\"\r\n",
        "    )\r\n",
        "    cursor = conn.cursor()\r\n",
        "    cursor.execute(\"EXEC etl.uspSetKeyValue @KeyName = ?, @KeyValue = ?\", (key_name, key_value))\r\n",
        "    conn.commit()\r\n",
        "    cursor.close()\r\n",
        "    conn.close()\r\n",
        "\r\n",
        "print(\"Declared function: set_key_value(key_name, key_value)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## the KeyValuePair Getter function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {},
      "source": [
        "def get_key_value(key_name):\r\n",
        "    # TODO: Get this to work with the predefined jdbc vars from above\r\n",
        "    import pyodbc\r\n",
        "\r\n",
        "    conn = pyodbc.connect(\r\n",
        "        \"Driver={ODBC Driver 18 for SQL Server};\"\r\n",
        "        f\"Server={jdbcHostname};\"\r\n",
        "        \"Database=dw;\"\r\n",
        "        f\"UID={jdbcUsername};\"\r\n",
        "        f\"PWD={{{jdbcPassword}}};\"\r\n",
        "        \"Encrypt=yes;\"\r\n",
        "        \"TrustServerCertificate=no;\"\r\n",
        "    )\r\n",
        "    cursor = conn.cursor()\r\n",
        "    cursor.execute(\"EXEC etl.uspGetKeyValue @KeyName = ?\", (key_name,))\r\n",
        "    row = cursor.fetchone()\r\n",
        "    cursor.close()\r\n",
        "    conn.close()\r\n",
        "\r\n",
        "    return row[0] if row else None\r\n",
        "\r\n",
        "print(\"Declared function: get_key_value(key_name)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## "
      ]
    }
  ]
}
