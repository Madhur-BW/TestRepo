{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Call Common Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /utils/common_functions\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from delta.tables import *\r\n",
        "from notebookutils import mssparkutils\r\n",
        "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\",\"true\")\r\n",
        "spark.sql(\"SET spark.databricks.delta.schema.autoMerge.enabled = true\")\r\n",
        "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"true\")\r\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\r\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\r\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\r\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\",\"true\")\r\n",
        "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\",\"true\")\r\n",
        "spark.conf.set(\"spark.databricks.adaptive.autoOptimizeShuffle.enabled\",\"true\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "create database if not exists structured;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Copying all common functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        " def file_exists(adls_path):\r\n",
        "      try:\r\n",
        "        if len(mssparkutils.fs.ls(adls_path)) > 0:\r\n",
        "          print(\"file Exists::\", adls_path)\r\n",
        "          return True\r\n",
        "      except Exception as e:\r\n",
        "        if 'java.io.FileNotFoundException' in str(e):\r\n",
        "          return False\r\n",
        "        else:\r\n",
        "          raise e    \r\n",
        "\r\n",
        "def get_target_table_ref(target_path):\r\n",
        "    try:\r\n",
        "      #sql = f\"CREATE TABLE IF NOT EXISTS {db}.{target_table} USING DELTA LOCATION '{target_path}'\"\r\n",
        "      print(\"get_target_table_ref i/p::\",target_path)\r\n",
        "      #spark.sql(sql)\r\n",
        "      delta_table = DeltaTable.forPath(spark, target_path)  \r\n",
        "      return delta_table\r\n",
        "    except Exception as e:\r\n",
        "      raise e\r\n",
        "\r\n",
        "def execute_merge(target_table, staged_updates, where_condition): ## for ctl_config_varaibles: where_condition :\r\n",
        "      try:\r\n",
        "        target_table.alias(\"target\").merge(staged_updates.alias(\"source\"),where_condition)\\\r\n",
        "               .whenMatchedUpdateAll()\\\r\n",
        "              .whenNotMatchedInsertAll()\\\r\n",
        "              .execute()\r\n",
        "        print(\"table upserted\")      \r\n",
        "        return target_table.toDF()\r\n",
        "      except Exception as e:\r\n",
        "         raise e                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Main( )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "def perform_merge(dict_tables):\r\n",
        "  \r\n",
        "    try: \r\n",
        "      \r\n",
        "      \r\n",
        "        \r\n",
        "      for keys,values in dict_tables.items():\r\n",
        "        \r\n",
        "        print(\"#################################perform_merge########\")\r\n",
        "        print(\"table::\", keys)\r\n",
        "        print(\"table_path::\",dict_tables[keys][\"path\"])\r\n",
        "        #dict_tables[keys][\"path\"] = structured_adls_path + keys\r\n",
        "        #source_df = spark.read.load(raw_adls_path+dict_tables[keys][\"source_path\"],format=\"delta\")\r\n",
        "        source_df = dict_tables[keys][\"source_df\"]\r\n",
        "        \r\n",
        "        print(\"target path::\", dict_tables[keys][\"path\"])\r\n",
        "        if file_exists(dict_tables[keys][\"path\"]):\r\n",
        "          print(dict_tables[keys],\"::table_exists\")\r\n",
        "          execute_merge(get_target_table_ref(dict_tables[keys][\"path\"]),source_df,dict_tables[keys][\"where_condition\"])\r\n",
        "        elif dict_tables[keys][\"partition\"]:\r\n",
        "          print(keys,\" table does not exist and is partitioned... going to create it...partion key::\",dict_tables[keys][\"partition\"])\r\n",
        "          source_df.write.mode(\"overwrite\").format(\"delta\").option(\"path\", dict_tables[keys][\"path\"])\\\r\n",
        "          .partitionBy(dict_tables[keys][\"partition\"])\\\r\n",
        "          .saveAsTable(dict_tables[keys][\"target_table\"])\r\n",
        "          print(keys,\"::table_created\" )\r\n",
        "        else:\r\n",
        "          print(keys,\" table does not exist, going to create it...\")\r\n",
        "          source_df.write.mode(\"overwrite\").format(\"delta\").option(\"path\", dict_tables[keys][\"path\"])\\\r\n",
        "          .saveAsTable(dict_tables[keys][\"target_table\"])\r\n",
        "          print(keys,\"::table_created\" )\r\n",
        "       \r\n",
        " \r\n",
        "    except Exception as e:\r\n",
        "      raise e  \r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}
