{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Change_date         revision_number     change_description                           author\r\n",
        "# 09/01/2023           1                   initial check-in                             Kranthi/Chad\r\n",
        "# 01/10/2024           2                   run optimize,vacuum for all folders          Kranthi\r\n",
        "# 01/15-22/2024        3                   Optimize code                                Chad\r\n",
        "# 04/29/2024           4                   Add bronze table                             Chad\r\n",
        "# 14Feb2025     KETTNECH Save output to bronze versus raw container\r\n",
        "# 2025-06-17 KETTNECH Add more folders/paths and trimmed text output drastically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import concurrent.futures\r\n",
        "from delta import *\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\r\n",
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\r\n",
        "spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "zorder_dict = {'AccountsReceivable':'documentdate'\r\n",
        ", 'InventoryAging':'calendarday'\r\n",
        ", 'InTransitsbyFicalPeriod': 'calendarday'\r\n",
        ", 'plmProjectsCurrent':'calendarday'\r\n",
        ", 'RetailInventoryMovements':'calendarday'\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Create empty dataframe to hold data\r\n",
        "schema = StructType([\r\n",
        "  StructField('id', StringType(), True),\r\n",
        "  StructField('location', StringType(), True),\r\n",
        "  StructField('table', StringType(), True),\r\n",
        "  StructField('createdAt', StringType(), True),\r\n",
        "  StructField('lastModified', StringType(), True),\r\n",
        "  StructField('numFiles', IntegerType(), True),\r\n",
        "  StructField('partitionedBy', StringType(), True),\r\n",
        "  StructField('sizeInBytes', LongType(), True)\r\n",
        "  ])\r\n",
        "deltas_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#For optimizing( optimize(reduce large number of small files and vacuum))\r\n",
        "def optimizeDeltaTable(delta_table_path, duration=24):\r\n",
        "  global deltas_df\r\n",
        "  output_str = ''\r\n",
        "  output_str2 = ''\r\n",
        "  if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
        "    output_str = delta_table_path\r\n",
        "  else:\r\n",
        "    #print(delta_table_path,'is not a delta table.\\n')\r\n",
        "    return\r\n",
        "  \r\n",
        "  try:\r\n",
        "    folder_name = delta_table_path.split('/')[-1]\r\n",
        "    p_zorder = zorder_dict[folder_name]\r\n",
        "  except:\r\n",
        "    p_zorder = ''\r\n",
        "  \r\n",
        "  try:\r\n",
        "    df = spark.sql(f'DESCRIBE DETAIL delta.`{delta_table_path}`')\r\n",
        "    if df.select('numFiles').first()[0] == 0:\r\n",
        "      output_str = output_str+' .. no files, no need to optimize.'\r\n",
        "    elif df.select('numFiles').first()[0] == 1:\r\n",
        "      output_str = output_str+' .. only 1 file, no need to optimize.'\r\n",
        "    else:\r\n",
        "      output_str2 = ' .. optimizing details:\\n'+df.select('partitionColumns','numFiles','sizeInBytes')._jdf.showString(20, 0, True)\r\n",
        "\r\n",
        "      if p_zorder:\r\n",
        "        optimize_cmd = f'optimize delta.`{delta_table_path}` ZORDER BY ({p_zorder})'\r\n",
        "      else:\r\n",
        "        optimize_cmd = f'optimize delta.`{delta_table_path}`'\r\n",
        "      if duration:\r\n",
        "        pass\r\n",
        "      else:\r\n",
        "        duration = 24\r\n",
        "    \r\n",
        "      after_optimze_stats = spark.sql(optimize_cmd)\r\n",
        "      if after_optimze_stats.select('metrics.numFilesAdded').first()[0] > 0:\r\n",
        "        output_str = output_str+output_str2+after_optimze_stats.select('metrics.numFilesAdded','metrics.numFilesRemoved',\r\n",
        "                                'metrics.partitionsOptimized','metrics.zOrderStats')._jdf.showString(20, 0, True)\r\n",
        "\r\n",
        "        df = spark.sql(f'DESCRIBE DETAIL delta.`{delta_table_path}`')\r\n",
        "        output_str = output_str+df.select('numFiles','sizeInBytes')._jdf.showString(20, 0, True)\r\n",
        "      else:\r\n",
        "        output_str = output_str+' .. no changes after optimization.'\r\n",
        "\r\n",
        "    spark.sql(f'VACUUM delta.`{delta_table_path}` RETAIN {duration} HOURS')\r\n",
        "    \r\n",
        "    print(output_str)\r\n",
        "  except Exception as e:\r\n",
        "    print('Other exception::',delta_table_path,str(e))\r\n",
        "    return\r\n",
        "\r\n",
        "  #Add latest details to deltas_df\r\n",
        "  newRow = spark.createDataFrame([(df.select('id').first()[0], df.select('location').first()[0], folder_name, str(df.select('createdAt').first()[0]), str(df.select('lastModified').first()[0]), df.select('numFiles').first()[0], df.select('partitionColumns').first()[0], df.select('sizeInBytes').first()[0])], schema)\r\n",
        "  deltas_df = deltas_df.union(newRow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "folder_path_list_gold = [folder.path for folder in\r\n",
        "        mssparkutils.fs.ls(f\"{gold_adls_path}\") + \r\n",
        "        mssparkutils.fs.ls(f\"{gold_adls_path}AS400\") +\r\n",
        "        mssparkutils.fs.ls(f\"{gold_adls_path}b2b2c\") +\r\n",
        "        mssparkutils.fs.ls(f\"{gold_adls_path}cco\") + \r\n",
        "        mssparkutils.fs.ls(f\"{gold_adls_path}GA4\") + \r\n",
        "        mssparkutils.fs.ls(f\"{gold_adls_path}mParticle\") + \r\n",
        "        mssparkutils.fs.ls(f\"{gold_adls_path}SAP/BW\") +\r\n",
        "        mssparkutils.fs.ls(f\"{gold_adls_path}TransportationInsights\") + \r\n",
        "        mssparkutils.fs.ls(f\"{gold_adls_path}vi3\")\r\n",
        "        if folder.size==0]\r\n",
        "#folder_path_list_gold.remove(f\"{gold_adls_path}NewStore\")\r\n",
        "folder_path_list_silver = [folder.path for folder in\r\n",
        "        mssparkutils.fs.ls(f\"{silver_adls_path}EMWBIS\") + \r\n",
        "        mssparkutils.fs.ls(f\"{silver_adls_path}mParticle\") +\r\n",
        "        mssparkutils.fs.ls(f\"{silver_adls_path}NewStore/merrell\") +\r\n",
        "        mssparkutils.fs.ls(f\"{silver_adls_path}NewStore/saucony\") +\r\n",
        "        mssparkutils.fs.ls(f\"{silver_adls_path}NewStore/sweatybetty\") +\r\n",
        "        mssparkutils.fs.ls(f\"{silver_adls_path}NewStore/wolverineusa\") +\r\n",
        "        mssparkutils.fs.ls(f\"{silver_adls_path}SAP/AFS\") +\r\n",
        "        mssparkutils.fs.ls(f\"{silver_adls_path}SAP/BW\") +\r\n",
        "        mssparkutils.fs.ls(f\"{silver_adls_path}StoreTech\")\r\n",
        "        if folder.size==0]\r\n",
        "folder_path_list_bronze = [folder.path for folder in\r\n",
        "        mssparkutils.fs.ls(f\"{bronze_adls_path}\") +\r\n",
        "        mssparkutils.fs.ls(f\"{bronze_adls_path}NewStore/merrell\") +\r\n",
        "        mssparkutils.fs.ls(f\"{bronze_adls_path}NewStore/saucony\") +\r\n",
        "        mssparkutils.fs.ls(f\"{bronze_adls_path}NewStore/sweatybetty\") +\r\n",
        "        mssparkutils.fs.ls(f\"{bronze_adls_path}NewStore/wolverineusa\") +\r\n",
        "        mssparkutils.fs.ls(f\"{bronze_adls_path}SAP/AFS\") +\r\n",
        "        mssparkutils.fs.ls(f\"{bronze_adls_path}SAP/BW\") +\r\n",
        "        mssparkutils.fs.ls(f\"{bronze_adls_path}SAP/Retail\") +\r\n",
        "        mssparkutils.fs.ls(f\"{bronze_adls_path}StoreTech\") +\r\n",
        "        mssparkutils.fs.ls(f\"{bronze_adls_path}TransportationInsights\") +\r\n",
        "        mssparkutils.fs.ls(f\"{bronze_adls_path}vi3\")\r\n",
        "        if folder.size==0]\r\n",
        "folder_path_list_raw = [folder.path for folder in\r\n",
        "        mssparkutils.fs.ls(f\"{raw_adls_path}SAP/BW/\")\r\n",
        "        if folder.size==0]\r\n",
        "\r\n",
        "folder_path_list = folder_path_list_gold + folder_path_list_silver + folder_path_list_bronze + folder_path_list_raw\r\n",
        "#print(folder_path_list_silver)\r\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\r\n",
        "        results = list(executor.map(optimizeDeltaTable, folder_path_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Save deltas_df to Delta\r\n",
        "deltas_df.write.option(\"overwriteSchema\", \"true\").mode(\"overwrite\").format(\"delta\").save(f'{bronze_adls_path}Synapse/DeltaInventory')"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}