{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import delta, pyspark.sql.functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /utils/environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "function = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def CompressZbpc_c12():\n",
        "    target_table = f'{raw_adls_path}/SAP/BW/ZBPC_C12'\n",
        "    print(target_table)\n",
        "    delta_df = delta.DeltaTable.forPath(spark, target_table).toDF().persist()\n",
        "    print(\"Pre count:\", delta_df.count())\n",
        "    delta_df.printSchema()\n",
        "\n",
        "    delta_df = delta_df.groupBy(['FISCPER3','FISCVARNT','FISCYEAR','FISCPER','CURKEY_GC','CURKEY_LC','UNIT','CURKEY_TC','SEM_CGCOMP','CO_AREA','FUNC_AREA','COUNTRY','MOVE_TYPE','ACQ_YEAR','ACQ_PER','SEM_ACOMP','BCS_APRCTR','CS_PLEVEL','BCS_DOCTY','BCS_CTFLG','BCS_INVCOM','BCS_INVCPC','PROFIT_CTR','PCOMP_CODE','SEM_CGPRCT','COMP_CODE','PART_PRCTR','VERSION','/BIC/ZCHR_ACT','/BIC/ZGL_ACCT','COMPANY','PCOMPANY','COSTCENTER','/BIC/ZAUDT_TRL','/BIC/ZELIM_FLG']).sum()\\\n",
        "        .withColumnsRenamed({'sum(CS_TRN_GC)':'CS_TRN_GC','sum(CS_TRN_LC)':'CS_TRN_LC','sum(CS_TRN_QTY)':'CS_TRN_QTY','sum(CS_TRN_TC)':'CS_TRN_TC','sum(/BIC/ZCS_GC_EU)':'/BIC/ZCS_GC_EU','sum(/BIC/ZCS_GC_GB)':'/BIC/ZCS_GC_GB'})\\\n",
        "        .withColumn('CS_TRN_GC',pyspark.sql.functions.col('CS_TRN_GC').cast('decimal(17,2)'))\\\n",
        "        .withColumn('CS_TRN_LC',pyspark.sql.functions.col('CS_TRN_LC').cast('decimal(17,2)'))\\\n",
        "        .withColumn('CS_TRN_QTY',pyspark.sql.functions.col('CS_TRN_QTY').cast('decimal(17,3)'))\\\n",
        "        .withColumn('CS_TRN_TC',pyspark.sql.functions.col('CS_TRN_TC').cast('decimal(17,2)'))\\\n",
        "        .withColumn('/BIC/ZCS_GC_EU',pyspark.sql.functions.col('/BIC/ZCS_GC_EU').cast('decimal(17,2)'))\\\n",
        "        .withColumn('/BIC/ZCS_GC_GB',pyspark.sql.functions.col('/BIC/ZCS_GC_GB').cast('decimal(17,2)'))\n",
        "    print(\"Post count:\", delta_df.count())\n",
        "    delta_df.printSchema()\n",
        "\n",
        "    delta_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").option(\"path\", target_table).save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "match function:\n",
        "    case 'ZBPC_C12':\n",
        "        CompressZbpc_c12()\n",
        "    case _:\n",
        "        notebookutils.mssparkutils.notebook.exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "notebookutils.mssparkutils.notebook.exit(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Repartition Delta Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "deltaTable = delta.DeltaTable.forPath(spark, f\"{raw_adls_path}/SAP/BW/0DELIV_NUMB$P\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "delta_df = deltaTable.toDF()\n",
        "delta_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Uncomment to use\n",
        "#delta_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\") \\\n",
        "#  .option(\"path\", f\"{raw_adls_path}/SAP/BW/0DELIV_NUMB_NEW$P\") \\\n",
        "#  .partitionBy(\"CREATEDON\").save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Selective Delete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "deltaTable = delta.DeltaTable.forPath(spark, f\"{raw_adls_path}/SAP/BW/ZBILLDOC\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "delta_detail_df = deltaTable.detail()\n",
        "delta_detail_df.drop('name','description','minReaderVersion','minWriterVersion').show(vertical=True,truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Uncomment to use\n",
        "#deltaTable.delete(\"FISCYEAR IN ('2024','2023')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Rollback version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "deltaTable = delta.DeltaTable.forPath(spark, f\"{raw_adls_path}/SAP/BW/ZBILLDOC\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "delta_history_df = deltaTable.history(3)\n",
        "delta_history_df.drop('userId','userName','job','notebook','clusterId','userMetadata').show(vertical=True,truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#In case of bad delete, reverse with above code based on version (get from history table above)\n",
        "#deltaTable.restoreToVersion(#version number)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Read raw CDC delta and push to raw/bronze table (~2025, hasn't been used in years, does this still work?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Read the staging CSV file\n",
        "df1= spark.read.format('csv').load(\"abfss://staging@azwwwprodprdadapadls.dfs.core.windows.net/ZPUR202320230917/*\",header='true',inferSchema='true')\n",
        "\n",
        "#Remove extra CDC columns that don't show up in the dataflow view\n",
        "df1=df1.drop('ODQ_CHANGEMODE','ODQ_ENTITYCNTR','_PACKAGEID','_SEQUENCENUMBER')\n",
        "\n",
        "#Optionally add a DELTA_ID column\n",
        "from datetime import datetime, timedelta, date\n",
        "from pytz import timezone\n",
        "now_utc = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "print(not_utc)\n",
        "#df1=df1.withColumn('DELTA_ID',F.lit(now_utc))\n",
        "\n",
        "#Create as Table for SQL Insert later (gets around format issues easily)\n",
        "df1.createOrReplaceTempView(\"TempTable1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Read the target raw delta table\n",
        "df=spark.read.format('delta').load(\"abfss://raw@azwwwprodprdadapadls.dfs.core.windows.net/SAP/BW/ZPURCHASE\")\n",
        "\n",
        "#Create as Table for SQL Insert later\n",
        "df.createOrReplaceTempView(\"TempTable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        }
      },
      "source": [
        "%%sql\n",
        "insert into TempTable \n",
        "select * from TempTable1\n",
        ""
      ]
    }
  ],
  "metadata": {
    "description": "Used for selective deletion, repartitioning, version rollback, and CDC work",
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}