{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Include the common_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run common_functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Test deduplicate_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Test usage including order_by_desc_column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {},
      "source": [
        "# usage with ordering column:   df_deduped = deduplicate_dataframe(df, key_columns=[\"id\", \"date\"], order_by_desc_column=\"updated_at\")\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
        "from datetime import datetime\n",
        "\n",
        "# Define schema\n",
        "schema = StructType([\n",
        "    StructField(\"CompanyName\", StringType(), True),\n",
        "    StructField(\"City\", StringType(), True),\n",
        "    StructField(\"LastUpdateTimeStamp\", TimestampType(), True)\n",
        "])\n",
        "\n",
        "# Sample data (10 unique, 2 duplicates)\n",
        "data = [\n",
        "    (\"Acme Corp\", \"New York\", datetime(2023, 7, 1, 12, 30)),\n",
        "    (\"Globex Inc\", \"Chicago\", datetime(2023, 7, 2, 9, 15)),\n",
        "    (\"Initech\", \"San Francisco\", datetime(2023, 7, 3, 14, 45)),\n",
        "    (\"Umbrella Corp\", \"Los Angeles\", datetime(2023, 7, 4, 16, 0)),\n",
        "    (\"Soylent Corp\", \"Boston\", datetime(2023, 7, 5, 8, 30)),\n",
        "    (\"Stark Industries\", \"Dallas\", datetime(2023, 7, 6, 10, 20)),\n",
        "    (\"Wayne Enterprises\", \"Gotham\", datetime(2023, 7, 7, 11, 10)),\n",
        "    (\"Wonka Industries\", \"Seattle\", datetime(2023, 7, 8, 13, 40)),\n",
        "    (\"Cyberdyne Systems\", \"Austin\", datetime(2023, 7, 9, 15, 25)),\n",
        "    (\"Tyrell Corporation\", \"Detroit\", datetime(2023, 7, 10, 17, 55)),\n",
        "    # Duplicate entries\n",
        "    (\"Acme Corp\", \"New York\", datetime(2023, 7, 11, 12, 0)),\n",
        "    (\"Globex Inc\", \"Chicago\", datetime(2023, 7, 12, 9, 0)),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show the data\n",
        "df.show(truncate=False)\n",
        "\n",
        "# Now deduplicate the data\n",
        "df_deduped = deduplicate_dataframe_by_key_columns(df, key_columns=[\"CompanyName\"], order_by_desc_column=\"LastUpdateTimeStamp\")\n",
        "df_deduped.show(truncate=False)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Test it without an order_by_desc_column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Use teh same df from above\n",
        "df_deduped2 = deduplicate_dataframe_by_key_columns(df, [\"CompanyName\"])\n",
        "df_deduped2.show() # Picks the first row available for key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Test sanitize_column_names(df, replacement_dict=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Creat a dataframe with funk column names.\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Define schema with problematic column names\n",
        "schema = StructType([\n",
        "    StructField(\"Company Name\", StringType(), True),\n",
        "    StructField(\"Revenue%\", IntegerType(), True),\n",
        "    StructField(\"Profit&Loss\", IntegerType(), True),\n",
        "    StructField(\"Address-Line#1\", StringType(), True),\n",
        "    StructField(\"@Region!\", StringType(), True),\n",
        "    StructField(\"1stColumn\", StringType(), True),\n",
        "    StructField(\"NormalColumn\", StringType(), True),\n",
        "])\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (\"Acme Corp\", 1000000, 100000, \"123 Elm St\", \"North\", \"Value1\", \"Valid\"),\n",
        "    (\"Globex Inc\", 2000000, 250000, \"456 Oak Ave\", \"South\", \"Value2\", \"StillValid\")\n",
        "]\n",
        "\n",
        "# Create the DataFrame\n",
        "df_test_column_name_fix = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show original schema\n",
        "print(\"Original Schema:\")\n",
        "df_test_column_name_fix.printSchema()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Test without dict of character sequence replacements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "df_names_fixed_no_replacement = sanitize_column_names(df_test_column_name_fix)\n",
        "df_names_fixed_no_replacement.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Test it with a dict of charecter sequence replacements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {},
      "source": [
        "replacement_dict = {\n",
        "    \"%\": \"Pct\",\n",
        "    \"&\": \"And\",\n",
        "    \"#\": \"Number\"\n",
        "}\n",
        "df_names_fixed = sanitize_column_names(df=df_test_column_name_fix, replacement_dict=replacement_dict)\n",
        "df_names_fixed.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Test the KeyValuePair getter and setter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "#def set_key_value(key_name: str, key_value: str):\n",
        "set_key_value(key_name='TestKey', key_value='TestKeyValue3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {},
      "source": [
        "#def get_key_value(key_name):\n",
        "key_value = get_key_value(key_name='TestKey')\n",
        "print(f\"key_value: {key_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "# The function defs.\n",
        "import inspect\n",
        "\n",
        "print(inspect.getsource(set_key_value))\n",
        "print(\"*\" * 100)\n",
        "print(inspect.getsource(get_key_value))"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}