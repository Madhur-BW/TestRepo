{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {},
      "source": [
        "account_name = raw_adls_path.split('@')[1].split('.')[0]\n",
        "account_fqdn  = account_name + \".dfs.core.windows.net\"\n",
        "print(account_name)\n",
        "print(account_fqdn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Container names\n",
        "gold_container = gold_adls_path.split('@')[0].split('//')[1]  # 'gold'\n",
        "bronze_container = bronze_adls_path.split('@')[0].split('//')[1]  # 'gold'\n",
        "raw_container = 'raw'\n",
        "\n",
        "# Parquet file paths for raw Centric views (Dev_Sample)\n",
        "ed_season_folder = '/Centric/ED_SEASON.parquet'\n",
        "ed_style_folder = '/Centric/ED_STYLE.parquet'\n",
        "ed_colorway_folder = '/Centric/ED_COLORWAY.parquet'\n",
        "ed_supplier_folder = '/Centric/ED_SUPPLIER.parquet'\n",
        "ed_factory_folder = '/Centric/ED_FACTORY.parquet'\n",
        "ed_product_source_folder = '/Centric/ED_PRODUCT_SOURCE.parquet'\n",
        "ed_user_folder = '/Centric/ED_USER.parquet'\n",
        "ed_product_size_folder = '/Centric/ED_PRODUCT_SIZE.parquet'\n",
        "ed_lookup_item_folder = '/Centric/ED_LOOKUP_ITEM.parquet'\n",
        "ed_apparel_bom_folder = '/Centric/ED_APPAREL_BOM.parquet'\n",
        "ed_apparel_bom_revision_folder = '/Centric/ED_APPAREL_BOM_REVISION.parquet'\n",
        "ed_sample_folder = '/Centric/ED_SAMPLE.parquet'\n",
        "\n",
        "\n",
        "#Additional Folders for Sales Sample\n",
        "\n",
        "ed_purchased_order_folder = \"/Centric/ED_PURCHASED_ORDER.parquet\"\n",
        "ed_shipment_folder = \"/Centric/ED_SHIPMENT.parquet\"\n",
        "ed_shipment_terms_folder = \"/Centric/ED_SHIPMENT_TERMS.parquet\"\n",
        "ed_lookup_item_folder = \"/Centric/ED_LOOKUP_ITEM.parquet\"\n",
        "ed_color_product_source_folder = \"/Centric/ED_COLOR_PRODUCT_SOURCE.parquet\"\n",
        "ed_order_folder = \"/Centric/ED_ORDER.parquet\"\n",
        "\n",
        "\n",
        "#additional folders for Colorway\n",
        "\n",
        "ed_color_specification_folder = \"/Centric/ED_COLOR_SPECIFICATION.parquet\"\n",
        "ed_category_1_folder          = \"/Centric/ED_CATEGORY_1.parquet\"\n",
        "ed_supplier_item_folder       = \"/Centric/ED_SUPPLIER_ITEM.parquet\"\n",
        "er_supplier_item_revision_folder = \"/Centric/ER_SUPPLIER_ITEM_REVISION.parquet\"\n",
        "ed_product_sales_region_folder = \"/Centric/ED_PRODUCT_SALES_REGION.parquet\"\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "# ED_SEASON\n",
        "ed_season_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_season_folder}\"\n",
        "print(f\"Reading data from: {ed_season_path}\")\n",
        "df_ed_season = spark.read.format(\"parquet\").load(ed_season_path)\n",
        "\n",
        "# ED_STYLE\n",
        "ed_style_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_style_folder}\"\n",
        "print(f\"Reading data from: {ed_style_path}\")\n",
        "df_ed_style = spark.read.format(\"parquet\").load(ed_style_path)\n",
        "\n",
        "# ED_COLORWAY\n",
        "ed_colorway_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_colorway_folder}\"\n",
        "print(f\"Reading data from: {ed_colorway_path}\")\n",
        "df_ed_colorway = spark.read.format(\"parquet\").load(ed_colorway_path)\n",
        "\n",
        "# ED_SUPPLIER\n",
        "ed_supplier_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_supplier_folder}\"\n",
        "print(f\"Reading data from: {ed_supplier_path}\")\n",
        "df_ed_supplier = spark.read.format(\"parquet\").load(ed_supplier_path)\n",
        "\n",
        "# ED_FACTORY\n",
        "ed_factory_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_factory_folder}\"\n",
        "print(f\"Reading data from: {ed_factory_path}\")\n",
        "df_ed_factory = spark.read.format(\"parquet\").load(ed_factory_path)\n",
        "\n",
        "# ED_PRODUCT_SOURCE\n",
        "ed_product_source_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_product_source_folder}\"\n",
        "print(f\"Reading data from: {ed_product_source_path}\")\n",
        "df_ed_product_source = spark.read.format(\"parquet\").load(ed_product_source_path)\n",
        "\n",
        "# ED_USER\n",
        "ed_user_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_user_folder}\"\n",
        "print(f\"Reading data from: {ed_user_path}\")\n",
        "df_ed_user = spark.read.format(\"parquet\").load(ed_user_path)\n",
        "\n",
        "# ED_PRODUCT_SIZE\n",
        "ed_product_size_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_product_size_folder}\"\n",
        "print(f\"Reading data from: {ed_product_size_path}\")\n",
        "df_ed_product_size = spark.read.format(\"parquet\").load(ed_product_size_path)\n",
        "\n",
        "# ED_LOOKUP_ITEM (Region & shipping lookup)\n",
        "ed_lookup_item_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_lookup_item_folder}\"\n",
        "print(f\"Reading data from: {ed_lookup_item_path}\")\n",
        "df_ed_lookup_item = spark.read.format(\"parquet\").load(ed_lookup_item_path)\n",
        "\n",
        "# ED_APPAREL_BOM\n",
        "ed_apparel_bom_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_apparel_bom_folder}\"\n",
        "print(f\"Reading data from: {ed_apparel_bom_path}\")\n",
        "df_ed_apparel_bom = spark.read.format(\"parquet\").load(ed_apparel_bom_path)\n",
        "\n",
        "# ED_APPAREL_BOM_REVISION\n",
        "ed_apparel_bom_revision_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_apparel_bom_revision_folder}\"\n",
        "print(f\"Reading data from: {ed_apparel_bom_revision_path}\")\n",
        "df_ed_apparel_bom_revision = spark.read.format(\"parquet\").load(ed_apparel_bom_revision_path)\n",
        "\n",
        "# ED_SAMPLE (used for Dev Samples)\n",
        "ed_sample_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_sample_folder}\"\n",
        "print(f\"Reading data from: {ed_sample_path}\")\n",
        "df_ed_sample = spark.read.format(\"parquet\").load(ed_sample_path)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "collapsed": false
      },
      "source": [
        "display(df_ed_lookup_item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "# ED_PURCHASED_ORDER\n",
        "ed_purchased_order_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_purchased_order_folder}\"\n",
        "print(f\"Reading data from: {ed_purchased_order_path}\")\n",
        "df_ed_purchased_order = spark.read.format(\"parquet\").load(ed_purchased_order_path)\n",
        "\n",
        "# ED_SHIPMENT\n",
        "ed_shipment_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_shipment_folder}\"\n",
        "print(f\"Reading data from: {ed_shipment_path}\")\n",
        "df_ed_shipment = spark.read.format(\"parquet\").load(ed_shipment_path)\n",
        "\n",
        "# ED_SHIPMENT_TERMS\n",
        "ed_shipment_terms_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_shipment_terms_folder}\"\n",
        "print(f\"Reading data from: {ed_shipment_terms_path}\")\n",
        "df_ed_shipment_terms = spark.read.format(\"parquet\").load(ed_shipment_terms_path)\n",
        "\n",
        "# ED_LOOKUP_ITEM\n",
        "'''\n",
        "ed_lookup_item_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_lookup_item_folder}\"\n",
        "print(f\"Reading data from: {ed_lookup_item_path}\")\n",
        "df_ed_lookup_item = spark.read.format(\"parquet\").load(ed_lookup_item_path)\n",
        "'''\n",
        "\n",
        "# ED_ORDER\n",
        "ed_order_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_order_folder}\"\n",
        "print(f\"Reading data from: {ed_order_path}\")\n",
        "df_ed_order = spark.read.format(\"parquet\").load(ed_order_path)\n",
        "\n",
        "\n",
        "\n",
        "# ED_COLOR_PRODUCT_SOURCE\n",
        "ed_color_product_source_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_color_product_source_folder}\"\n",
        "print(f\"Reading data from: {ed_color_product_source_path}\")\n",
        "df_ed_color_product_source = spark.read.format(\"parquet\").load(ed_color_product_source_path)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Additional Tables for COlorway"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "# ---------- ED_COLOR_SPECIFICATION ----------\n",
        "ed_color_specification_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_color_specification_folder}\"\n",
        "print(f\"Reading data from: {ed_color_specification_path}\")\n",
        "df_ed_color_specification = spark.read.format(\"parquet\").load(ed_color_specification_path)\n",
        "\n",
        "# ---------- ED_CATEGORY_1 ----------\n",
        "ed_category_1_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_category_1_folder}\"\n",
        "print(f\"Reading data from: {ed_category_1_path}\")\n",
        "df_ed_category_1 = spark.read.format(\"parquet\").load(ed_category_1_path)\n",
        "\n",
        "# ---------- ED_SUPPLIER_ITEM ----------\n",
        "ed_supplier_item_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_supplier_item_folder}\"\n",
        "print(f\"Reading data from: {ed_supplier_item_path}\")\n",
        "df_ed_supplier_item = spark.read.format(\"parquet\").load(ed_supplier_item_path)\n",
        "\n",
        "# ---------- ER_SUPPLIER_ITEM_REVISION ----------\n",
        "er_supplier_item_revision_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{er_supplier_item_revision_folder}\"\n",
        "print(f\"Reading data from: {er_supplier_item_revision_path}\")\n",
        "df_er_supplier_item_revision = spark.read.format(\"parquet\").load(er_supplier_item_revision_path)\n",
        "\n",
        "# ---------- ED_PRODUCT_SALES_REGION ----------\n",
        "ed_product_sales_region_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_product_sales_region_folder}\"\n",
        "print(f\"Reading data from: {ed_product_sales_region_path}\")\n",
        "df_ed_product_sales_region = spark.read.format(\"parquet\").load(ed_product_sales_region_path)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Region"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "# Region/Lookup Dimension (for shipping & region details)\n",
        "dim_region_df = df_ed_lookup_item.select(\n",
        "    F.col(\"ID\").alias(\"RegionId\"),\n",
        "    F.col(\"WVW_DESTINATION_SHIP_TO_ADDRESS\").alias(\"ShipToAddress\"),\n",
        "    F.col(\"WVW_NOTIFY_PARTYAND_CONSIGNEE\").alias(\"NotifyPartyConsignee\"),\n",
        "    F.col(\"WVW_DISTRIBUTOR_NAMEAND_CONTACT\").alias(\"DistributorContact\"),\n",
        "    F.col(\"WVW_SHIPPING_METHOD_COURIEROR_FF\").alias(\"ShippingMethod\"),\n",
        "    F.col(\"WVW_SHIPPING_DETAILS_COURIER_AC_NOOR_FF_DETAILS\").alias(\"ShippingDetails\"),\n",
        "    F.col(\"WVW_HALF_PAIR_INDICATOR\").alias(\"HalfPairIndicator\")\n",
        ").distinct()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Lookup Item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "# Region/Lookup Dimension (for shipping & region details)\n",
        "dim_lookup_item_df = df_ed_lookup_item.select(\n",
        "    F.col(\"ID\").alias(\"RegionId\"),\n",
        "    F.col(\"WVW_DESTINATION_SHIP_TO_ADDRESS\").alias(\"ShipToAddress\"),\n",
        "    F.col(\"WVW_NOTIFY_PARTYAND_CONSIGNEE\").alias(\"NotifyPartyConsignee\"),\n",
        "    F.col(\"WVW_DISTRIBUTOR_NAMEAND_CONTACT\").alias(\"DistributorContact\"),\n",
        "    F.col(\"WVW_SHIPPING_METHOD_COURIEROR_FF\").alias(\"ShippingMethod\"),\n",
        "    F.col(\"WVW_SHIPPING_DETAILS_COURIER_AC_NOOR_FF_DETAILS\").alias(\"ShippingDetails\"),\n",
        "    F.col(\"WVW_HALF_PAIR_INDICATOR\").alias(\"HalfPairIndicator\")\n",
        ").distinct()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Dimension Purchase Order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "dim_purchase_order_df = df_ed_purchased_order.select(\n",
        "    F.col(\"WVW_SEASON\").alias(\"Season\"),\n",
        "    F.col(\"PO\").alias(\"PO\"),  # unchanged\n",
        "    F.col(\"WVW_PO_SAMPLE_TYPE\").alias(\"PO_Sample_Type\"),\n",
        "    F.col(\"PO_SUPPLIER\").alias(\"Supplier\"),\n",
        "    F.col(\"PO_Factory\").alias(\"Factory\"),\n",
        "    F.col(\"Node_Name\").alias(\"Supplier_PO\"),\n",
        "    F.col(\"STATE_001\").alias(\"State\"),\n",
        "    F.col(\"STATE_CHANGE_TIME\").alias(\"State_Changed_On\"))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Cleanup - Define function\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "import re\n",
        "\n",
        "# Acronyms to preserve as UPPERCASE (customize as needed)\n",
        "ACRONYMS = {\"PO\", \"ETD\", \"ID\"}\n",
        "\n",
        "def remove_wvw(text: str, case_sensitive: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    Remove the substring 'WVW' from text.\n",
        "    By default, removal is case-insensitive.\n",
        "    \"\"\"\n",
        "    if case_sensitive:\n",
        "        return text.replace(\"WVW\", \"\")\n",
        "    else:\n",
        "        # Case-insensitive removal of 'WVW'\n",
        "        return re.sub(r\"(?i)WVW\", \"\", text)\n",
        "\n",
        "def to_pascal_after_cleanup(col_name: str) -> str:\n",
        "    \"\"\"\n",
        "    1) Remove 'WVW' substring.\n",
        "    2) Convert underscore-separated tokens to PascalCase.\n",
        "       Preserve acronyms in ACRONYMS (UPPERCASE).\n",
        "    \"\"\"\n",
        "    # Step 1: remove WVW\n",
        "    cleaned = remove_wvw(col_name, case_sensitive=False)\n",
        "\n",
        "    # Step 2: split by underscore and filter empty tokens\n",
        "    parts = [p for p in cleaned.split(\"_\") if p != \"\"]\n",
        "    converted = []\n",
        "    for p in parts:\n",
        "        token_upper = p.upper()\n",
        "        if token_upper in ACRONYMS:\n",
        "            converted.append(token_upper)            # preserve acronyms (PO, ETD, ID)\n",
        "        elif p.isupper() and len(p) <= 3:\n",
        "            converted.append(p)                      # preserve short all-caps tokens\n",
        "        else:\n",
        "            converted.append(p[:1].upper() + p[1:].lower())  # PascalCase\n",
        "    return \"\".join(converted)\n",
        "\n",
        "def rename_df_columns_remove_wvw(df):\n",
        "    \"\"\"\n",
        "    Rename all columns:\n",
        "      - remove 'WVW'\n",
        "      - convert to PascalCase\n",
        "      - avoid name collisions by appending numeric suffixes\n",
        "    \"\"\"\n",
        "    old_cols = df.columns\n",
        "    new_cols = []\n",
        "    used = set()\n",
        "    for c in old_cols:\n",
        "        base = to_pascal_after_cleanup(c)\n",
        "        new_name = base or c  # fallback if result becomes empty\n",
        "        idx = 2\n",
        "        while new_name in used:\n",
        "            new_name = f\"{base}{idx}\"\n",
        "            idx += 1\n",
        "        new_cols.append((c, new_name))\n",
        "        used.add(new_name)\n",
        "\n",
        "    for old, new in new_cols:\n",
        "        if old != new:\n",
        "            df = df.withColumnRenamed(old, new)\n",
        "    return df\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Clean up columns State and PO_Sample_Type\n",
        "We remove wvwText: column so that data looks clean in report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Columns to transform\n",
        "cols_to_clean = [\n",
        "    \"PO_Sample_Type\",\n",
        "    \"State\"\n",
        "    \n",
        "]\n",
        "\n",
        "# Apply substring_index to each column\n",
        "for col in cols_to_clean:\n",
        "    dim_purchase_order_df = dim_purchase_order_df.withColumn(col, F.substring_index(F.col(col), \":\", -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Dimension Shipment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "# Build the DataFrame with aliased columns\n",
        "dim_ed_shipment_df = df_ed_shipment.select(\n",
        "    F.col('WVW_PURCHASED_ORDER').alias('PO'), \n",
        "    F.col('WVW_SAMPLE_STATUS').alias('Shipment_Status'),\n",
        "    F.col('WVW_INITIAL_ORDER_QTY').alias('Initial_Shipment_Order_Qty'),\n",
        "    F.col('WVW_SMS_ORDER_QTY').alias('Shipment_Order_Qty'),\n",
        "    F.col('WVW_ETD').alias('ETD'),\n",
        "    F.col('WVW_SHIPPING_METHOD').alias('Shipping_Method'),\n",
        "    F.col('WVW_TRACKING_NUMBER').alias('Airway_Bill'),\n",
        "    F.col('WVW_DELAY_REASON_LIST').alias('Shipment_Delay_Reason'),\n",
        "    F.col('WVW_DELAY_REASON').alias('Shipment_Delay_Reason_Comment'),\n",
        "    F.col('THE_CREATED_AT').alias('Created_At'),\n",
        "    F.col('THE_PARENT_ID').alias('PARENT_ID'),\n",
        "    F.col('MODIFIED_AT').alias('MODIFIED_AT'),\n",
        "    F.col('MODIFIED_BY').alias('MODIFIED_BY'),\n",
        "    F.col('THE_CREATED_BY').alias('CREATED_BY'), \n",
        "    F.col('NODE_NAME').alias('Shipment'),\n",
        "    F.col('WVW_DELAY').alias('Shipment_Delay'),\n",
        "    F.col('ACTUAL_QUANTITY').alias('ACTUAL_QUANTITY'),\n",
        "    #\n",
        "\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Cleanup Shipment\n",
        "dim_ed_shipment_df clean up columns Shipment_Status and Shipment_Delay_Reason"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Columns to transform\n",
        "cols_to_clean = [\n",
        "    \"Shipment_Status\",\n",
        "    \"Shipment_Delay_Reason\"\n",
        "    \n",
        "]\n",
        "\n",
        "# Apply substring_index to each column\n",
        "for col in cols_to_clean:\n",
        "    dim_ed_shipment_df = dim_ed_shipment_df.withColumn(col, F.substring_index(F.col(col), \":\", -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Shipment Terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "outputs": [],
      "metadata": {},
      "source": [
        "dim_shipment_terms_df = df_ed_shipment_terms.select(\n",
        "    F.col('ID').alias('Shipment_Terms_ID'), \n",
        "    F.col('REQUESTED_SHIP_DATE').alias('REQUESTED_SHIP_DATE')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Join Shipment and Shipment Terms\n",
        "In Shipment We need to bring in one column from shipment terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "s = dim_ed_shipment_df.alias(\"s\")\n",
        "t = dim_shipment_terms_df.alias(\"t\")\n",
        "\n",
        "# Choose the join type you need: \"inner\", \"left\", \"right\", \"full\"\n",
        "# Using left join here so you keep all shipments even if no terms match\n",
        "joined_df = s.join(\n",
        "    t,\n",
        "    col(\"s.PARENT_ID\") == col(\"t.Shipment_Terms_ID\"),\n",
        "    \"left\"\n",
        ")\n",
        "\n",
        "# If you truly want *all* columns from both sides (including both key columns)\n",
        "joined_all_cols = joined_df.select(\"s.*\", \"t.*\")\n",
        "\n",
        "\n",
        "#display(joined_all_cols)\n",
        "\n",
        "\n",
        "dim_ed_shipment_df = joined_all_cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Color Product Source\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "outputs": [],
      "metadata": {},
      "source": [
        "dim_ed_color_product_source_df = df_ed_color_product_source.select('ID','COLORWAY','ACTIVE','WVW_UNIQUE_ID','NODE_NAME')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Order\n",
        "### ED_Order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "#display(df_ed_order)\n",
        "\n",
        "dim_ed_order_df = df_ed_order.select('ID',\n",
        "                                  'PO','WVW_PO_SUPPLIER_VALIDATION','PO_COLOR','PO_PRODUCT')\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Season"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# ---------------------------\n",
        "# DIMENSION TABLES (extended)\n",
        "# ---------------------------\n",
        "\n",
        "# Season Dimension (unchanged)\n",
        "dim_season_df = df_ed_season.select(\n",
        "    F.col(\"ID\").alias(\"SeasonId\"),\n",
        "    F.col(\"NODE_NAME\").alias(\"SeasonName\"),\n",
        "    F.col(\"WVW_SEASON_TYPE\").alias(\"SeasonType\"),\n",
        "    F.col(\"WVW_YEAR\").alias(\"Year\")\n",
        ").distinct()\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_ed_style.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Style Dimension (extended)\n",
        "dim_style_df = (\n",
        "    df_ed_style.select(\n",
        "        F.col(\"ID\").alias(\"StyleId\"),\n",
        "        F.col(\"PARENT_SEASON\").alias(\"SeasonId\"),\n",
        "\n",
        "        \n",
        "        F.col(\"WVW_BRAND\").alias(\"Brand_Composit\"),\n",
        "        F.col(\"WVW_SEASONAL_STYLE_CODE\").alias(\"SeasonalStyleCode\"),\n",
        "        F.col(\"WVW_STYLE_NAME\").alias(\"StyleName\"),\n",
        "        F.col(\"NODE_NAME\").alias(\"Style\"),\n",
        "        F.col(\"WVW_STYLE_NAME\").alias(\"Style_Working_Name\"),\n",
        "        F.col(\"WVW_STATUS\").alias(\"StyleStatus\"),\n",
        "        F.col(\"WVW_GENDER\").alias(\"Gender\"),\n",
        "        F.col(\"WVW_DIVISION_HIERARCHY\").alias(\"Division_Composit\"),\n",
        "        F.col(\"CATEGORY_1\").alias(\"Brand\"),\n",
        "        F.col(\"COLLECTION\").alias(\"Division\"),\n",
        "\n",
        "        \n",
        "        F.col(\"WVW_PATTERN_NAME\").alias(\"FinalPatternNameSeasonless\"),\n",
        "        F.col(\"ACTIVE\").alias(\"PAActive\"),\n",
        "        F.col(\"WVW_DESIGNER\").alias(\"Designer\"),\n",
        "        F.col(\"WVW_DEVELOPER\").alias(\"Developer\")\n",
        "    )\n",
        "    .distinct()\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# --- 1) Your existing selection: KEEP AS-IS ---\n",
        "existing_exprs = [\n",
        "    F.col(\"ID\").alias(\"StyleId\"),\n",
        "    F.col(\"PARENT_SEASON\").alias(\"SeasonId\"),\n",
        "\n",
        "    F.col(\"WVW_BRAND\").alias(\"Brand_Composit\"),\n",
        "    F.col(\"WVW_SEASONAL_STYLE_CODE\").alias(\"SeasonalStyleCode\"),\n",
        "    F.col(\"WVW_STYLE_NAME\").alias(\"StyleName\"),\n",
        "    F.col(\"NODE_NAME\").alias(\"Style\"),\n",
        "    F.col(\"WVW_STYLE_NAME\").alias(\"Style_Working_Name\"),\n",
        "    F.col(\"WVW_STATUS\").alias(\"StyleStatus\"),\n",
        "    F.col(\"WVW_GENDER\").alias(\"Gender\"),\n",
        "    F.col(\"WVW_DIVISION_HIERARCHY\").alias(\"Division_Composit\"),\n",
        "    F.col(\"CATEGORY_1\").alias(\"Brand\"),\n",
        "    F.col(\"COLLECTION\").alias(\"Division\"),\n",
        "\n",
        "    F.col(\"WVW_PATTERN_NAME\").alias(\"FinalPatternNameSeasonless\"),\n",
        "    F.col(\"ACTIVE\").alias(\"PAActive\"),\n",
        "    F.col(\"WVW_DESIGNER\").alias(\"Designer\"),\n",
        "    F.col(\"WVW_DEVELOPER\").alias(\"Developer\"),\n",
        "]\n",
        "\n",
        "# --- 2) Explicit list of alias names already present (parallel to existing_exprs) ---\n",
        "# This avoids calling methods on Column objects to retrieve alias names.\n",
        "existing_target_names = {\n",
        "    \"StyleId\",\n",
        "    \"SeasonId\",\n",
        "    \"Brand_Composit\",\n",
        "    \"SeasonalStyleCode\",\n",
        "    \"StyleName\",\n",
        "    \"Style\",\n",
        "    \"Style_Working_Name\",\n",
        "    \"StyleStatus\",\n",
        "    \"Gender\",\n",
        "    \"Division_Composit\",\n",
        "    \"Brand\",\n",
        "    \"Division\",\n",
        "    \"FinalPatternNameSeasonless\",\n",
        "    \"PAActive\",\n",
        "    \"Designer\",\n",
        "    \"Developer\",\n",
        "}\n",
        "\n",
        "# --- 3) Alias targets from Column_Aliases.csv (excluding \"NA\" and any you already have) ---\n",
        "# Mapping format: {target_alias : source_column_in_df_ed_style}\n",
        "to_add_map = {\n",
        "    # Identification / codes\n",
        "    \"CODE\": \"CODE\",\n",
        "\n",
        "    # Org & description\n",
        "    \"WVW_DEV_CENTER\": \"WVW_DEV_CENTER\",\n",
        "    \"DESCRIPTION\": \"DESCRIPTION\",\n",
        "\n",
        "    # Materials / weights / costing\n",
        "    \"WVW_BASIC_MATERIAL\": \"WVW_BASIC_MATERIAL\",\n",
        "    \"WVW_SAMPLE_PAIR_WEIGHT_KG\": \"WVW_SAMPLE_PAIR_WEIGHT_KG\",\n",
        "    \"WVW_COSTING_SIZE\": \"WVW_COSTING_SIZE\",\n",
        "    \"DEFAULT_COLOR\": \"DEFAULT_COLOR\",\n",
        "\n",
        "    # Planning / season meta\n",
        "    \"THEME\": \"THEME\",\n",
        "    \"EARLIEST_TARGET_DATE\": \"EARLIEST_TARGET_DATE\",\n",
        "    \"WVW_NEW_LAST\": \"WVW_NEW_LAST\",\n",
        "    \"WVW_LAST\": \"WVW_LAST\",\n",
        "    \"WVW_TOOL_SET\": \"WVW_TOOL_SET\",\n",
        "    \"WVW_ACCOUNTABLE\": \"WVW_ACCOUNTABLE\",\n",
        "    \"WVW_CONSTRUCTION\": \"WVW_CONSTRUCTION\",\n",
        "    \"WVW_COST_CONSTRUCTION\": \"WVW_COST_CONSTRUCTION\",\n",
        "    \"MARKETING_SEGMENT\": \"MARKETING_SEGMENT\",\n",
        "    \"WVW_PRODUCT_POSITIONING\": \"WVW_PRODUCT_POSITIONING\",\n",
        "    \"WVW_SEASONAL_PROGRAM\": \"WVW_SEASONAL_PROGRAM\",\n",
        "    \"WVW_SILHOUETTE\": \"WVW_SILHOUETTE\",\n",
        "    \"WVW_APPROVAL_NUMBER\": \"WVW_APPROVAL_NUMBER\",\n",
        "    \"WVW_TARGET_NUMBEROF_COLORS\": \"WVW_TARGET_NUMBEROF_COLORS\",\n",
        "    \"WVW_FLOCKED_OUTSOLE\": \"WVW_FLOCKED_OUTSOLE\",\n",
        "    \"WVW_NEW_KNIVES\": \"WVW_NEW_KNIVES\",\n",
        "    \"WVW_HEEL_HEIGHT\": \"WVW_HEEL_HEIGHT\",\n",
        "    \"WVW_SHAFT_HEIGHT\": \"WVW_SHAFT_HEIGHT\",\n",
        "    \"WVW_SHAFT_CIRCUMFERENCE\": \"WVW_SHAFT_CIRCUMFERENCE\",\n",
        "    \"WVW_DROP\": \"WVW_DROP\",\n",
        "    \"WVW_FOREFOOT_HEIGHT\": \"WVW_FOREFOOT_HEIGHT\",\n",
        "    \"WVW_PRODUCT_HIERARCHY\": \"WVW_PRODUCT_HIERARCHY\",\n",
        "    \"WVW_PLAN_REF_STYLE\": \"WVW_PLAN_REF_STYLE\",\n",
        "    \"WVW_DEV_REF_STYLE\": \"WVW_DEV_REF_STYLE\",\n",
        "\n",
        "    # Sample weights\n",
        "    \"WVW_SAMPLE_PAIR_WEIGHT_G\": \"WVW_SAMPLE_PAIR_WEIGHT_G\",\n",
        "    \"WVW_SAMPLE_PAIR_WEIGHTOZ\": \"WVW_SAMPLE_PAIR_WEIGHTOZ\",\n",
        "    \"WVW_SAMPLE_HALF_PAIR_WEIGHT_G\": \"WVW_SAMPLE_HALF_PAIR_WEIGHT_G\",\n",
        "    \"WVW_SAMPLE_HALF_PAIR_WEIGHT_KG\": \"WVW_SAMPLE_HALF_PAIR_WEIGHT_KG\",\n",
        "    \"WVW_SAMPLE_HALF_PAIR_WEIGHTOZ\": \"WVW_SAMPLE_HALF_PAIR_WEIGHTOZ\",\n",
        "\n",
        "    # Waterproofing / membranes\n",
        "    \"WVW_VIBRAM\": \"WVW_VIBRAM\",\n",
        "    \"WVW_WATERPROOF_TYPE\": \"WVW_WATERPROOF_TYPE\",\n",
        "    \"WVW_WATERPROOF\": \"WVW_WATERPROOF\",\n",
        "    \"WVW_MEMBRANE\": \"WVW_MEMBRANE\",\n",
        "    \"WVW_MEMBRANE_SUPPLIER\": \"WVW_MEMBRANE_SUPPLIER\",\n",
        "    \"WVW_MEMBRANE_DESCRIPTION\": \"WVW_MEMBRANE_DESCRIPTION\",\n",
        "    \"WVW_MEMBRANE_PACKAGE\": \"WVW_MEMBRANE_PACKAGE\",\n",
        "    \"WVW_BOOTIE\": \"WVW_BOOTIE\",\n",
        "    \"WVW_SEAM_SEALED_1\": \"WVW_SEAM_SEALED_1\",\n",
        "    \"WVW_DWR\": \"WVW_DWR\",\n",
        "\n",
        "    # Sorting / pricing\n",
        "    \"WVW_STYLE_SORT_ORDER\": \"WVW_STYLE_SORT_ORDER\",\n",
        "    \"WVW_MATERIAL_FREIGHT_GROUP\": \"WVW_MATERIAL_FREIGHT_GROUP\",\n",
        "    \"WVW_TARGET_PRICE_RETAIL\": \"WVW_TARGET_PRICE_RETAIL\",\n",
        "    \"WVW_TARGET_PRICE_WHSL\": \"WVW_TARGET_PRICE_WHSL\",\n",
        "    \"WVW_TARGET_PURCHASE_PRICE\": \"WVW_TARGET_PURCHASE_PRICE\",\n",
        "    \"WVW_ORIGINAL_TARGET_PURCHASE_PRICE\": \"WVW_ORIGINAL_TARGET_PURCHASE_PRICE\",\n",
        "    \"WVW_RETAIL_PRICE_MSRP\": \"WVW_RETAIL_PRICE_MSRP\",\n",
        "    \"WVW_WHOLESALE_PRICE\": \"WVW_WHOLESALE_PRICE\",\n",
        "\n",
        "    # Notes / BOM / testing\n",
        "    \"AUTHORITY_BOM\": \"AUTHORITY_BOM\",\n",
        "    \"WVW_SHOE_TESTING_STANDARD\": \"WVW_SHOE_TESTING_STANDARD\",\n",
        "    \"WVW_PROTO_DEV_NOTES\": \"WVW_PROTO_DEV_NOTES\",\n",
        "    \"WVW_ISR_DEV_NOTES\": \"WVW_ISR_DEV_NOTES\",\n",
        "    \"WVW_LDM_DEV_NOTES\": \"WVW_LDM_DEV_NOTES\",\n",
        "\n",
        "    # Sizes / activity\n",
        "    \"ACTUAL_SIZE_RANGE\": \"ACTUAL_SIZE_RANGE\",\n",
        "    \"DEFAULT_SIZE\": \"DEFAULT_SIZE\",\n",
        "    \"WVW_ACTIVITY_TYPE\": \"WVW_ACTIVITY_TYPE\",\n",
        "\n",
        "    # Duty / taxes\n",
        "    \"WVW_USA_TARGET_DUTY_FREE\": \"WVW_USA_TARGET_DUTY_FREE\",\n",
        "    \"WVW_USA_TARGET_DUTY_PERCENT\": \"WVW_USA_TARGET_DUTY_PERCENT\",\n",
        "    \"WVW_CANADA_TARGET_DUTY_PERCENTAGE\": \"WVW_CANADA_TARGET_DUTY_PERCENTAGE\",\n",
        "    \"WVW_EUROPE_TARGET_DUTY_PERCENTAGE\": \"WVW_EUROPE_TARGET_DUTY_PERCENTAGE\",\n",
        "    \"WVW_UK_TARGET_DUTY_PERCENTAGE\": \"WVW_UK_TARGET_DUTY_PERCENTAGE\",\n",
        "\n",
        "    # Audit / lineage\n",
        "    \"ORIGINAL_SEASON\": \"ORIGINAL_SEASON\",\n",
        "    \"THE_CREATED_AT\": \"THE_CREATED_AT\",\n",
        "    \"THE_CREATED_BY\": \"THE_CREATED_BY\",\n",
        "    \"MODIFIED_AT\": \"MODIFIED_AT\",\n",
        "    \"MODIFIED_BY\": \"MODIFIED_BY\",\n",
        "}\n",
        "\n",
        "# --- 4) Build additional expressions: add BrandID/BrandText + mapped targets ---\n",
        "additional_exprs = []\n",
        "\n",
        "# Brand pair (CSV indicates \"BrandID, BrandText\")\n",
        "if \"BrandText\" not in existing_target_names:\n",
        "    additional_exprs.append(F.col(\"CATEGORY_1\").alias(\"BrandText\"))\n",
        "if \"BrandID\" not in existing_target_names:\n",
        "    # TODO: replace NULL with actual BrandID via a lookup table when available\n",
        "    additional_exprs.append(F.lit(None).cast(\"string\").alias(\"BrandID\"))\n",
        "\n",
        "# Add remaining mapped targets (skip already present)\n",
        "src_cols = set(df_ed_style.columns)\n",
        "for tgt, src in to_add_map.items():\n",
        "    if tgt in existing_target_names:\n",
        "        continue\n",
        "    if src in src_cols:\n",
        "        additional_exprs.append(F.col(src).alias(tgt))\n",
        "    else:\n",
        "        # keep schema stable even if source is absent\n",
        "        additional_exprs.append(F.lit(None).cast(\"string\").alias(tgt))\n",
        "\n",
        "# --- 5) Final DataFrame ---\n",
        "dim_style_df = df_ed_style.select(*(existing_exprs + additional_exprs)).distinct()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "display(dim_style_df.limit(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "dim_style_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Colorway"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Colorway Dimension (extended)\n",
        "dim_colorway_df = (\n",
        "    df_ed_colorway.select(\n",
        "        F.col(\"ID\").alias(\"ColorwayId\"),\n",
        "        F.col(\"PARENT_SEASON\").alias(\"SeasonId\"),\n",
        "        F.col(\"STYLE_001\").alias(\"StyleId\"),\n",
        "\n",
        "        # existing fields\n",
        "        F.col(\"WVW_SEASONAL_COLORWAY_CODE\").alias(\"SeasonalColorwayCode\"),\n",
        "        F.col(\"WVW_STOCK_NUMBER\").alias(\"StockNumber\"),\n",
        "        F.col(\"WVW_COLORWAY_WORKING_NAME\").alias(\"DevelopmentColorwayName\"),\n",
        "        F.col(\"WVW_STATUS\").alias(\"ColorwayStatus\"),\n",
        "        F.coalesce(F.col(\"WVW_ACTIVE_USER_SELECT\"), F.col(\"ACTIVE\")).alias(\"ActiveFlag\"),\n",
        "\n",
        "        # new fields\n",
        "        F.col(\"WVW_PRODUCT_TYPE_CLASS\").alias(\"ProductTypeClassification\"),\n",
        "        F.col(\"WVW_PRODUCT_LINE\").alias(\"ProductLineSeasonless\"),\n",
        "\n",
        "        # Fields for Sales Sample\n",
        "        F.col(\"WVW_DESIGNER\").alias(\"Designer\"),\n",
        "        F.col(\"WVW_DEVELOPER\").alias(\"Developer\"),\n",
        "        F.col(\"NODE_NAME\").alias(\"Development_Colorway_Name\"),\n",
        "        F.col(\"WVW_SEASONAL_COLORWAY_CODE\").alias(\"Seasonal_Colorway_Code\"),\n",
        "        F.col(\"WVW_PRODUCT_TYPE_CLASS\").alias(\"Product_Type_classification\"),\n",
        "        \n",
        "        F.col(\"WVW_PRODUCT_LINE\").alias(\"Product_Line_Seasonless\"),\n",
        "        #ACTIVE\n",
        "        F.col(\"ACTIVE\").alias(\"CW_Active\"),\n",
        "\n",
        "\n",
        "    )\n",
        "    .distinct()\n",
        ")\n",
        "#wvwColorProductSourcesActive not found\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Size Dimension (new)\n",
        "dim_size_df = (\n",
        "    df_ed_product_size.select(\n",
        "        F.col(\"ID\").alias(\"SizeId\"),\n",
        "        F.col(\"SIZE_CODE\").alias(\"SizeCode\"),\n",
        "        F.col(\"US_LABEL\").alias(\"USLabel\"),\n",
        "        F.col(\"WVW_WIDTH_ALPHA\").alias(\"WidthAlpha\"),\n",
        "        F.col(\"DIMENSION_1_SIZE\").alias(\"Dimension1\"),\n",
        "        F.col(\"DIMENSION_2_SIZE\").alias(\"Dimension2\")\n",
        "    )\n",
        "    .distinct()\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Supplier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "dim_supplier_df = (\n",
        "    df_ed_supplier.select(\n",
        "        F.col(\"ID\").alias(\"SupplierId\"),\n",
        "        F.col(\"WVW_C_8_SUPPLIER_ID\").alias(\"SupplierCode\"),\n",
        "        F.col(\"NODE_NAME\").alias(\"SupplierName\"),\n",
        "        F.col(\"COUNTRY\").alias(\"CountryId\"),\n",
        "        F.col(\"ADDRESS\").alias(\"SupplierAddress\")\n",
        "    )\n",
        "    .distinct()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "dim_factory_df = (\n",
        "    df_ed_factory.select(\n",
        "        F.col(\"ID\").alias(\"FactoryId\"),\n",
        "        F.col(\"WVW_C_8_FACTORY_ID\").alias(\"FactoryCode\"),\n",
        "        F.col(\"NODE_NAME\").alias(\"FactoryName\"),\n",
        "        F.col(\"COUNTRY\").alias(\"CountryId\"),\n",
        "        F.col(\"ADDRESS\").alias(\"FactoryAddress\"),\n",
        "        #Add column for dev sample\n",
        "        F.col(\"SUPPLIER_NUMBER\").alias(\"Factory_ID\")\n",
        "        #\n",
        "    )\n",
        "    .distinct()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Product Source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "outputs": [],
      "metadata": {},
      "source": [
        "dim_product_source_df = (\n",
        "    df_ed_product_source.select(\n",
        "        F.col(\"ID\").alias(\"ProductSourceId\"),\n",
        "        F.col(\"THE_PARENT_ID\").alias(\"StyleId\"),\n",
        "        F.col(\"SUPPLIER\").alias(\"SupplierId\"),\n",
        "        F.col(\"SUPPLIER_FACTORY\").alias(\"FactoryId\"),\n",
        "        F.col(\"WVW_SOURCING_DEVELOPER\").alias(\"SourcingDeveloperId\"),\n",
        "        F.col(\"ACTIVE\").alias(\"ActiveFlag\")\n",
        "    )\n",
        "    .distinct()\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "outputs": [],
      "metadata": {},
      "source": [
        "dim_sample_df = (\n",
        "    df_ed_sample.select(\n",
        "        F.col(\"ID\").alias(\"SampleId\"),\n",
        "        F.col(\"THE_PARENT_ID\").alias(\"StyleId\"),\n",
        "        F.col(\"NODE_NAME\").alias(\"SampleName\"),\n",
        "        F.col(\"WVW_SAMPLE_STAGE\").alias(\"SampleStage\"),\n",
        "        F.col(\"ITERATION\").alias(\"Iteration\"),\n",
        "        F.col(\"SAMPLE_STATUS\").alias(\"SampleStatus\"),\n",
        "        F.col(\"SAMPLE_TYPE\").alias(\"SampleType\"),\n",
        "\n",
        "        F.col(\"SAMPLE_RECEIVED_DATE\").alias(\"SampleReceivedDate\"),\n",
        "        F.col(\"SAMPLE_REVIEW_DATE\").alias(\"SampleReviewDate\"),\n",
        "        F.col(\"SAMPLE_NOTES\").alias(\"SampleNotes\"),\n",
        "\n",
        "        F.col(\"WVW_TECH_PACK_RECEIVED\").alias(\"TechPackReceivedDate\"),\n",
        "        F.col(\"WVW_TRACKING_NUMBER\").alias(\"SampleTrackingNumber\"),\n",
        "        F.col(\"WVW_ADDRESS\").alias(\"SampleAddress\"),\n",
        "        F.col(\"WVW_SELECTED_BOM\").alias(\"SampleBOM\"),\n",
        "        F.col(\"PRODUCT_COLOR\").alias(\"ProductColor\"),\n",
        "        F.col(\"wvw_ETD\").alias(\"ETD\")\n",
        "    )\n",
        "    .distinct()\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "outputs": [],
      "metadata": {},
      "source": [
        "dim_user_df = (\n",
        "    df_ed_user.select(\n",
        "        F.col(\"ID\").alias(\"UserId\"),\n",
        "        F.col(\"NODE_NAME\").alias(\"UserName\")\n",
        "    )\n",
        "    .distinct()\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Column Clean up for Dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Style Dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "\n",
        "from pyspark.sql.functions import split, col\n",
        "\n",
        "# Split the Brand column by ':'\n",
        "split_col = split(col(\"Brand_Composit\"), \":\")\n",
        "\n",
        "# Transform the DataFrame\n",
        "dim_style_df_transformed = (\n",
        "    dim_style_df\n",
        "    .withColumn(\"BrandID\", split_col.getItem(1))       # Extract the second element (index 1)\n",
        "    .withColumn(\"BrandText\", split_col.getItem(2))     # Extract the third element (index 2)\n",
        "    .drop(\"Brand_Composit\")         \n",
        ")\n",
        "dim_style_df_transformed = dim_style_df_transformed.drop(\"Brand\")\n",
        "\n",
        "# Split the Division\n",
        "\n",
        "dim_style_df_transformed = (\n",
        "    dim_style_df_transformed\n",
        "    .withColumn(\"DivisionCode\", F.element_at(F.split(F.col(\"Division_Composit\"), \":\"), 2))\n",
        "    .withColumn(\"DivisionText\", F.element_at(F.split(F.col(\"Division_Composit\"), \":\"), 3))\n",
        "    .drop(\"Division_Composit\")  # remove the raw column if you only want the derived fields\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "dim_style_df_transformed = (\n",
        "    dim_style_df_transformed\n",
        "        .withColumn(\"StyleStatus\", F.substring_index(F.col(\"StyleStatus\"), \":\", -1))\n",
        "        \n",
        ")\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Columns to exclude from cleanup\n",
        "exclude_cols = {\"Brand_Composit\", \"Division_Composit\"}\n",
        "\n",
        "# Build transformation dynamically\n",
        "dim_style_df_cleaned = dim_style_df_transformed\n",
        "for c in dim_style_df_cleaned.columns:\n",
        "    if c not in exclude_cols:\n",
        "        dim_style_df_cleaned = dim_style_df_cleaned.withColumn(c, F.substring_index(F.col(c), \":\", -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "rename_df_columns_remove_wvw(dim_style_df_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "collapsed": false
      },
      "source": [
        "display(dim_style_df_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "num_columns = len(dim_style_df_cleaned.columns)\n",
        "print(f\"Number of columns in dim_style_df_cleaned: {num_columns}\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Cleanup Color Way Dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Columns to transform\n",
        "cols_to_clean = [\n",
        "    \"ProductTypeClassification\",\n",
        "    \"ProductLineSeasonless\",\n",
        "    \"ColorwayStatus\"\n",
        "]\n",
        "\n",
        "# Apply substring_index to each column\n",
        "for col in cols_to_clean:\n",
        "    dim_colorway_df = dim_colorway_df.withColumn(col, F.substring_index(F.col(col), \":\", -1))\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Clean up Season Dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Columns to transform\n",
        "cols_to_clean = [\n",
        "    \"SeasonType\",\n",
        "    \"Year\"\n",
        "]\n",
        "\n",
        "# Apply substring_index to each column (take the part after the last \":\")\n",
        "for col in cols_to_clean:\n",
        "    dim_season_df = dim_season_df.withColumn(col, F.trim(F.substring_index(F.col(col), \":\", -1)))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "outputs": [],
      "metadata": {},
      "source": [
        "user_designer_df  = dim_user_df.alias(\"ud\")\n",
        "user_developer_df = dim_user_df.alias(\"uv\")\n",
        "user_sourcing_df  = dim_user_df.alias(\"us\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Clean up Sample Dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Columns to clean\n",
        "cols_to_clean = [\n",
        "    \"SampleStage\",\n",
        "    \"SampleStatus\",\n",
        "    \"SampleType\"\n",
        "]\n",
        "\n",
        "# Apply substring_index to each column (take the part after the last \":\")\n",
        "for col in cols_to_clean:\n",
        "    dim_sample_df = dim_sample_df.withColumn(col, F.trim(F.substring_index(F.col(col), \":\", -1)))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Combine Dimension - PO, order, shipment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "# Left join dim_ed_order_df on PO\n",
        "combined_PO_order_shipment_df = (\n",
        "    dim_purchase_order_df\n",
        "        .join(dim_ed_order_df, on=\"PO\", how=\"left\")\n",
        "        .join(dim_ed_shipment_df, on=\"PO\", how=\"left\")\n",
        ")\n",
        "\n",
        "# Verify PO only appears once\n",
        "print(combined_PO_order_shipment_df.columns)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "outputs": [],
      "metadata": {},
      "source": [
        "combined_PO_order_shipment_df = combined_PO_order_shipment_df.distinct()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Clean Up Combined PO, Order and Sales\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "# Apply to your combined dataframe\n",
        "combined_clean_df = rename_df_columns_remove_wvw(combined_PO_order_shipment_df)\n",
        "\n",
        "# Verify\n",
        "print(combined_clean_df.columns)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "combined_PO_order_shipment_df = combined_clean_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Write Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Declare variables to store paths of the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# gold_adls_path is assumed to be already defined, e.g.\n",
        "# gold_adls_path = \"abfss://gold@youraccount.dfs.core.windows.net/gold\"\n",
        "\n",
        "gold_container = gold_adls_path.split('@')[0].split('//')[1]   # e.g. 'gold'\n",
        "\n",
        "CDM_Root_Folder = \"/Centric/CDM\"\n",
        "\n",
        "# One folder per dimension / fact\n",
        "dim_season_folder          = CDM_Root_Folder + \"/dim_season\"\n",
        "dim_style_folder           = CDM_Root_Folder + \"/dim_style\"\n",
        "dim_colorway_folder        = CDM_Root_Folder + \"/dim_colorway\"\n",
        "dim_supplier_folder        = CDM_Root_Folder + \"/dim_supplier\"\n",
        "dim_factory_folder         = CDM_Root_Folder + \"/dim_factory\"\n",
        "dim_product_source_folder  = CDM_Root_Folder + \"/dim_product_source\"\n",
        "dim_size_folder            = CDM_Root_Folder + \"/dim_size\"\n",
        "dim_sample_folder          = CDM_Root_Folder + \"/dim_sample\"\n",
        "dim_user_folder            = CDM_Root_Folder + \"/dim_user\"\n",
        "\n",
        "dim_purchase_order_folder = CDM_Root_Folder + \"/dim_purchase_order\"\n",
        "dim_shipment_folder =       CDM_Root_Folder + \"/dim_shipment\"\n",
        "dim_color_product_source_folder = CDM_Root_Folder + \"/dim_color_product_source\"\n",
        "\n",
        "dim_order_folder = CDM_Root_Folder + \"/dim_order\"\n",
        "dim_lookup_item_folder = CDM_Root_Folder + \"/dim_lookup_item\"\n",
        "\n",
        "#Combined PO, Order, Shipment\n",
        "dim_combined_PO_Order_Shipment_Folder = CDM_Root_Folder + \"/dim_combined_PO_Order_Shipment\"\n",
        "\n",
        "\n",
        "fact_dev_samples_folder    = CDM_Root_Folder + \"/fact_dev_samples\"\n",
        "\n",
        "def cdm_path(folder: str) -> str:\n",
        "    \"\"\"\n",
        "    Build full ABFSS path under the gold container for a given folder.\n",
        "    \"\"\"\n",
        "    return f\"abfss://{gold_container}@{account_fqdn}{folder}\"\n",
        "\n",
        "\n",
        "#Colorway Folders\n",
        "\n",
        "\n",
        "dim_color_specification_folder   = CDM_Root_Folder + \"/dim_color_specification\"\n",
        "dim_category_1_folder            = CDM_Root_Folder + \"/dim_category_1\"\n",
        "dim_supplier_item_folder         = CDM_Root_Folder + \"/dim_supplier_item\"\n",
        "dim_supplier_item_revision_folder = CDM_Root_Folder + \"/dim_supplier_item_revision\"\n",
        "dim_product_sales_region_folder  = CDM_Root_Folder + \"/dim_product_sales_region\"\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write Combined PO Order Shipment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [],
      "metadata": {},
      "source": [
        "(combined_PO_order_shipment_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_combined_PO_Order_Shipment_Folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write Season"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_season_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_season_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write Style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_style_df_transformed\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .option(\"mergeSchema\", \"true\")\n",
        "    .save(cdm_path(dim_style_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write Colorway"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_colorway_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_colorway_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write Supplier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_supplier_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_supplier_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_factory_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_factory_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write Product Source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "(dim_product_source_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_product_source_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write SIze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_size_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_size_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_sample_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_sample_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_user_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_user_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write Purchase order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_purchase_order_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_purchase_order_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write Shipment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "(dim_ed_shipment_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .option(\"mergeSchema\", \"true\") \n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_shipment_folder))\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write Color Product Source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_ed_color_product_source_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_color_product_source_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write Order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_ed_order_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_order_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write Lookup Item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_lookup_item_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_lookup_item_folder)))"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}