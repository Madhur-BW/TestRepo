{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Revision History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Change_date         revision_number     change_description                           author\r\n",
        "# 03/13/2024          1                   initial check-in                             Kranthi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import concurrent.futures\r\n",
        "from delta import *\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType,IntegerType, DoubleType, MapType, ArrayType\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from functools import reduce\r\n",
        "from pyspark.sql.dataframe import DataFrame\r\n",
        "import pyspark.sql.functions as F\r\n",
        "import json\r\n",
        "import base64\r\n",
        "from datetime import datetime,timedelta\r\n",
        "from time import sleep\r\n",
        "from pytz import timezone\r\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"DYNAMIC\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# from notebookutils import mssparkutils\r\n",
        "\r\n",
        "# import sys  \r\n",
        "# from pyspark.sql import SparkSession  \r\n",
        "  \r\n",
        "# sc = SparkSession.builder.getOrCreate()  \r\n",
        "# token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary  \r\n",
        "# #token_library.getSecret(\"kv-name\", \"secret-name\", \"linked-service\")  \r\n",
        "# jdbcPassword = token_library.getSecret(kv_name, \"SqlAdmin\", \"ls_kv_adap\")  \r\n",
        "# print(jdbcPassword)\r\n",
        "\r\n",
        "# #jdbcHostname = 'az-www-datahub-nonprod-dev-adap-sql.database.windows.net'\r\n",
        "# #\"azwwwnonproddevadapsyn01.sql.azuresynapse.net\"\r\n",
        "# #jdbc:sqlserver://azwwwnonproddevadapsyn01.sql.azuresynapse.net:1433;database=syndw01;user=sqlAdmin@azwwwnonproddevadapsyn01;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\r\n",
        "# jdbcPort = 1433\r\n",
        "\r\n",
        "# #az-www-datahub-prod-prd-adap-sql.database.windows.net\r\n",
        "\r\n",
        "# jdbcDatabase = \"dw\"\r\n",
        "\r\n",
        "# jdbcUsername = \"sqlAdmin\"\r\n",
        "\r\n",
        "# #jdbcPassword = mssparkutils.credentials.getSecret('az-www-dev-adap-kv', 'SqlAdmin')\r\n",
        "\r\n",
        "# jdbcDriver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
        "\r\n",
        "# #url = s\"jdbc:sqlserver://${database_host}:${database_port}/${database_name}\"\r\n",
        "\r\n",
        "gua_table = \"stg.gua_weekly_stats\"\r\n",
        "\r\n",
        "date_dim_table = 'report.DateDim'\r\n",
        "\r\n",
        "# jdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};databaseName={jdbcDatabase}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "pip install apiclient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "pip install --upgrade google-api-python-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# define the schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "gua_schema = StructType([\r\n",
        "StructField('brand',StringType(), True)\r\n",
        ",StructField('device_category',StringType(), True)\r\n",
        "#,StructField('region',StringType(), True)\r\n",
        "#,StructField('country',StringType(), True)\r\n",
        ",StructField('sessions',IntegerType(), True)\r\n",
        ",StructField('adds_to_cart',IntegerType(), True)\r\n",
        ",StructField('transactions',IntegerType(), True)\r\n",
        ",StructField('quantity',IntegerType(), True)\r\n",
        ",StructField('revenue',DoubleType(), True)\r\n",
        ",StructField('shipping',DoubleType(), True)\r\n",
        ",StructField('tax',DoubleType(), True)])\r\n",
        "#,StructField('item_sales',DoubleType(), True)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# get the date range from Azure SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "query = \"\"\"\r\n",
        "    SELECT DISTINCT dd.fiscalweek, dd.weekbegindate, dd.weekenddate\r\n",
        "    FROM report.DateDim dd\r\n",
        "    JOIN (\r\n",
        "        SELECT fiscalyear, fiscalweek, fiscalweek - 1 AS previousFiscalWeek\r\n",
        "        FROM report.DateDim\r\n",
        "        WHERE DayDate = cast(getdate() as date)\r\n",
        "    ) t1\r\n",
        "    ON dd.fiscalweek = t1.previousfiscalweek AND dd.fiscalyear = t1.fiscalyear\r\n",
        "\"\"\"\r\n",
        "df_date = spark.read.format(\"jdbc\")\\\r\n",
        ".option(\"driver\", jdbcDriver)\\\r\n",
        ".option(\"url\", jdbcUrl)\\\r\n",
        ".option(\"query\",query)\\\r\n",
        ".option(\"user\", jdbcUsername)\\\r\n",
        ".option(\"password\", jdbcPassword)\\\r\n",
        ".load()\r\n",
        "gua_dates_range = df_date.collect()\r\n",
        "fiscal_week = gua_dates_range[0][0]\r\n",
        "week_begin_date = gua_dates_range[0][1]\r\n",
        "week_end_date = gua_dates_range[0][2]\r\n",
        "print(\"fiscal_week::\",fiscal_week,\"week_begin_date::\",week_begin_date,\"week_end_date::\",week_end_date) \r\n",
        "display(df_date)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# pull the GUA data using the date range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "from google.oauth2 import service_account\r\n",
        "from google.oauth2.credentials import Credentials\r\n",
        "from googleapiclient.discovery import build\r\n",
        "#from oauth2client.service_account import ServiceAccountCredentials\r\n",
        "\r\n",
        "file_text = spark.read.text(f\"{raw_adls_path}GA4/credentials.json\", wholetext=True)\r\n",
        "credential_data = file_text.collect()[0][0]\r\n",
        "#print(json.loads(credential_data))\r\n",
        "#credentials = Credentials.from_authorized_user_info(json.loads(decoded_bytes))\r\n",
        "credentials = service_account.Credentials.from_service_account_info(json.loads(credential_data))\r\n",
        "\r\n",
        "\r\n",
        "SCOPES = ['https://www.googleapis.com/auth/analytics.readonly']\r\n",
        "#KEY_FILE_LOCATION = '<REPLACE_WITH_JSON_FILE>' -- using client_id/secret \r\n",
        "all_list = []\r\n",
        "view_id_dict = {\r\n",
        "'206477765':'Bates US'\r\n",
        ",'206427693':'Cat US'\r\n",
        ",'206444558':'Chacos US'\r\n",
        ",'206456872':'Harley US'\r\n",
        ",'206410753':'Hush_Puppies US'\r\n",
        ",'206448897':'Hytest US'\r\n",
        ",'206374006':'Keds US'\r\n",
        ",'206382057':'Merrell US'\r\n",
        ",'206447765':'OLS US' ##OnLine Shoes\r\n",
        ",'206458988':'Prokeds US'\r\n",
        ",'206412752':'Saucony US'\r\n",
        ",'206417996':'Sperry US'\r\n",
        ",'197529557':'Wolverine US'\r\n",
        ",'206416996':'Cat CA'\r\n",
        ",'206417904':'Hush_Puppies CA'\r\n",
        ",'206387296':'Keds CA'\r\n",
        ",'206374184':'Merrell CA'\r\n",
        ",'206408305':'Saucony CA'\r\n",
        ",'206387297':'Sperry CA'\r\n",
        ",'206404077':'Wolverine CA'\r\n",
        ",'206385282':'Cat UK'\r\n",
        ",'206423967':'Merrell BE'\r\n",
        ",'206387013':'Merrell DE'\r\n",
        ",'209263784':'Merrell ES'\r\n",
        ",'206408612':'Merrell FR'\r\n",
        ",'206399949':'Merrell NL'\r\n",
        ",'206386116':'Merrell SE'\r\n",
        ",'206404969':'Merrell UK'\r\n",
        ",'206429101':'Saucony AT'\r\n",
        ",'206372175':'Saucony BE'\r\n",
        ",'206412758':'Saucony DE'\r\n",
        ",'206372789':'Saucony FR'\r\n",
        ",'206383771':'Saucony IT'\r\n",
        ",'206405484':'Saucony NL'\r\n",
        ",'206386112':'Saucony UK'\r\n",
        ",'231134802':'Cat DE'\r\n",
        ",'237948899':'Saucony ES'\r\n",
        ",'240003466':'Saucony EU'\r\n",
        ",'241661587':'Merrell EU'\r\n",
        ",'242992506':'Cat EU'\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "def initialize_analyticsreporting():\r\n",
        "  \"\"\"Initializes an Analytics Reporting API V4 service object.\r\n",
        "\r\n",
        "  Returns:\r\n",
        "    An authorized Analytics Reporting API V4 service object.\r\n",
        "  \"\"\"\r\n",
        "  #credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES)\r\n",
        "  file_text = spark.read.text(f\"{raw_adls_path}GA4/credentials.json\", wholetext=True)\r\n",
        "  credential_data = file_text.collect()[0][0]\r\n",
        "  #print(json.loads(credential_data))\r\n",
        "  #credentials = Credentials.from_authorized_user_info(json.loads(decoded_bytes))\r\n",
        "  credentials = service_account.Credentials.from_service_account_info(json.loads(credential_data))\r\n",
        "  # Build the service object.\r\n",
        "  analytics = build('analyticsreporting', 'v4', credentials=credentials)\r\n",
        "\r\n",
        "  return analytics\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "def get_report(analytics,p_view_id):\r\n",
        "  \"\"\"Queries the Analytics Reporting API V4.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    analytics: An authorized Analytics Reporting API V4 service object.\r\n",
        "  Returns:\r\n",
        "    The Analytics Reporting API V4 response.\r\n",
        "  \"\"\"\r\n",
        "#   return analytics.reports().batchGet(\r\n",
        "#       body={\r\n",
        "#         'reportRequests': [\r\n",
        "#         {\r\n",
        "#           'viewId': VIEW_ID,\r\n",
        "#           'dateRanges': [{'startDate': '2024-03-03', 'endDate': '2024-03-09'}],\r\n",
        "#           'metrics': [{'expression': 'ga:sessions'},],\r\n",
        "#           'dimensions': [{'name': 'ga:deviceCategory'}]\r\n",
        "#         }]\r\n",
        "#       }\r\n",
        "#   ).execute()\r\n",
        "  query = {\r\n",
        "    'viewId': p_view_id,\r\n",
        "    'dateRanges': [{'startDate': f'{week_begin_date}', 'endDate': f'{week_end_date}'}],\r\n",
        "    'dimensions': [{'name': 'ga:deviceCategory'}],\r\n",
        "    'metrics': [\r\n",
        "        {'expression': 'ga:sessions'},\r\n",
        "        {'expression': 'ga:productAddsToCart'},\r\n",
        "        {'expression': 'ga:transactions'},\r\n",
        "        {'expression': 'ga:itemQuantity'},\r\n",
        "        {'expression': 'ga:transactionRevenue'},\r\n",
        "        {'expression': 'ga:transactionShipping'},\r\n",
        "        {'expression': 'ga:transactionTax'}\r\n",
        "        #{'expression': 'ga:itemRevenue'},\r\n",
        "                ]\r\n",
        "      }\r\n",
        "\r\n",
        "# Execute the query\r\n",
        "  return analytics.reports().batchGet(body={'reportRequests': [query]}).execute()\r\n",
        "\r\n",
        "\r\n",
        "  \r\n",
        " \r\n",
        "   \r\n",
        "analytics = initialize_analyticsreporting()\r\n",
        "df = ''\r\n",
        "for i,j in view_id_dict.items():\r\n",
        "\r\n",
        "  response = get_report(analytics,i)\r\n",
        "  for report in response.get('reports', []):\r\n",
        "    for row in report.get('data', {}).get('rows', []):\r\n",
        "        device_category = row.get('dimensions', [])[0]\r\n",
        "        sessions = int(row.get('metrics', [])[0]['values'][0])\r\n",
        "        adds_to_cart = int(row.get('metrics', [])[0]['values'][1])\r\n",
        "        transactions = int(row.get('metrics', [])[0]['values'][2])\r\n",
        "        quantity = int(row.get('metrics', [])[0]['values'][3])\r\n",
        "        revenue = float(row.get('metrics', [])[0]['values'][4])\r\n",
        "        shipping = float(row.get('metrics', [])[0]['values'][5])\r\n",
        "        tax = float(row.get('metrics', [])[0]['values'][6])\r\n",
        "        #item_sales = round(lit(revenue-shipping-tax),2)\r\n",
        "        #item_sales = round(lit(item_sales),2)\r\n",
        "        #item_revenue = row.get('metrics', [])[0]['values'][7]\r\n",
        "        # print(\"device_category::\",device_category,\"sessions::\",sessions,\"adds_to_cart::\",adds_to_cart,\r\n",
        "        # \"transactions::\",transactions,\"quantity::\",quantity,\"revenue::\",revenue,\"shipping::\",shipping\r\n",
        "        # ,\"tax::\",tax)\r\n",
        "        all_list.append((j,device_category,sessions,adds_to_cart,transactions,quantity,revenue,shipping,tax))\r\n",
        "\r\n",
        "df = spark.createDataFrame(data = all_list,schema= gua_schema)\r\n",
        "df = df.withColumn('item_sales',F.round(F.lit(df.revenue-df.shipping-df.tax),2))\r\n",
        "df = df.groupBy('brand').pivot('device_category').sum('sessions','adds_to_cart','transactions','quantity','item_sales')\r\n",
        "df = df.selectExpr('brand'\r\n",
        ",\"split(brand,' ')[1] as country\" \r\n",
        ",\"case when split(brand,' ')[1] = 'US' then 'US' when split(brand,' ')[1] = 'CA' then 'CA' else 'EU' end as region\"\r\n",
        ",'`desktop_sum(sessions)` as desktop_visits'\r\n",
        ",'`mobile_sum(sessions)` as phone_visits'\r\n",
        ",'`tablet_sum(sessions)` as tablet_visits'\r\n",
        ",'`desktop_sum(adds_to_cart)` as desktop_carts'\r\n",
        ",'`mobile_sum(adds_to_cart)` as phone_carts'\r\n",
        ",'`tablet_sum(adds_to_cart)` as tablet_carts'\r\n",
        ",'`desktop_sum(transactions)` as desktop_orders'\r\n",
        ",'`mobile_sum(transactions)` as phone_orders'\r\n",
        ",'`tablet_sum(transactions)` as tablet_orders'\r\n",
        ",'`desktop_sum(quantity)` as desktop_units'\r\n",
        ",'`mobile_sum(quantity)` as phone_units'\r\n",
        ",'`tablet_sum(quantity)` as tablet_units'\r\n",
        ",'`desktop_sum(item_sales)` as desktop_web_sales'\r\n",
        ",'`mobile_sum(item_sales)` as phone_web_sales'\r\n",
        ",'`tablet_sum(item_sales)` as tablet_web_sales'\r\n",
        ")\r\n",
        "df = df.withColumn('week_end',lit(week_end_date))\r\n",
        "#df = df.withColumn('country',split(df.brand,' ')[-1])\r\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# write the data to Azure SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "\r\n",
        "df = df.write.format(\"jdbc\")\\\r\n",
        ".option(\"driver\", jdbcDriver)\\\r\n",
        ".option(\"url\", jdbcUrl)\\\r\n",
        ".option(\"dbtable\", gua_table)\\\r\n",
        ".option(\"user\", jdbcUsername)\\\r\n",
        ".option(\"password\", jdbcPassword)\\\r\n",
        ".mode('overwrite')\\\r\n",
        ".save()\r\n",
        "\r\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print(all_list)"
      ]
    }
  ],
  "metadata": {
    "description": "ingest gua data using reporting api",
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}