{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyspark.sql.functions as F\r\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"DYNAMIC\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\r\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\",\"true\")\r\n",
        "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\",\"true\")\r\n",
        "spark.conf.set(\"spark.databricks.adaptive.autoOptimizeShuffle.enabled\",\"true\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "select distinct stream_name from delta.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/GA4/events_feb05_nov08/events`\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## get dates for Full load - comment after the first full load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "full_load_dates = \"\"\"\r\n",
        "     SELECT DISTINCT  dd.fiscalweek as fiscalweek, dd.weekbegindate as weekbegindate, dd.weekenddate as weekenddate\r\n",
        "    FROM report.DateDim dd\r\n",
        "   where daydate between '2023-02-05' and '2023-11-11'\r\n",
        "   \r\n",
        "\"\"\"\r\n",
        "df_date = spark.read.format(\"jdbc\")\\\r\n",
        ".option(\"driver\", jdbcDriver)\\\r\n",
        ".option(\"url\", jdbcUrl)\\\r\n",
        ".option(\"query\",full_load_dates)\\\r\n",
        ".option(\"user\", jdbcUsername)\\\r\n",
        ".option(\"password\", jdbcPassword)\\\r\n",
        ".load()\r\n",
        "gua_dates_range = df_date.collect()\r\n",
        "fiscal_week = gua_dates_range[0][0]\r\n",
        "week_begin_date = gua_dates_range[0][1]\r\n",
        "week_end_date = gua_dates_range[0][2]\r\n",
        "print(\"fiscal_week::\",fiscal_week,\"week_begin_date::\",week_begin_date,\"week_end_date::\",week_end_date) \r\n",
        "display(df_date)\r\n",
        "df_date2 = df_date.createOrReplaceTempView('DateDim')\r\n",
        "df_date.printSchema()\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create aggregated carts table for all weekends between Feb 05 2023 - Nov 08 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "create table lakedb_gold.ga4_backfill_aggregate_events_carts \r\n",
        "using delta \r\n",
        "partitioned by (weekenddate)\r\n",
        "location 'abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/GA4/events_feb05_nov08/aggregate_events_carts'\r\n",
        "select d.weekbegindate\r\n",
        ", d.weekenddate\r\n",
        ", device_category\r\n",
        ", stream_name\r\n",
        ",'carts' as metric \r\n",
        ", round(sum(event_count),3) as carts\r\n",
        " from delta.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/GA4/events_feb05_nov08/events/` a\r\n",
        " inner join datedim d on a.date between d.weekbegindate and d.weekenddate\r\n",
        "where event_name IN ('add_to_cart')\r\n",
        "group by  d.weekbegindate\r\n",
        ", d.weekenddate\r\n",
        ", device_category\r\n",
        ", stream_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "select * from lakedb_gold.ga4_backfill_aggregate_events_carts; "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create aggregated orders table for all weekends between Feb 05 2023 - Nov 08 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "create table lakedb_gold.ga4_backfill_aggregate_events_orders \r\n",
        "using delta \r\n",
        "partitioned by (weekenddate)\r\n",
        "location 'abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/GA4/events_feb05_nov08/aggregate_events_orders'\r\n",
        "select d.weekbegindate\r\n",
        ", d.weekenddate\r\n",
        ", device_category\r\n",
        ", stream_name\r\n",
        ",'orders' as metric \r\n",
        ", round(sum(event_count),3) as orders\r\n",
        " from delta.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/GA4/events_feb05_nov08/events/` a\r\n",
        " inner join datedim d on a.date between d.weekbegindate and d.weekenddate\r\n",
        "where event_name IN ('purchase', 'transaction')\r\n",
        "group by  d.weekbegindate\r\n",
        ", d.weekenddate\r\n",
        ", device_category\r\n",
        ", stream_name\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "select * from lakedb_gold.ga4_backfill_aggregate_events_orders;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create aggregated sessions table for all weekends between Feb 05 2023 - Nov 08 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "create table lakedb_gold.ga4_backfill_aggregate_sessions\r\n",
        "using delta \r\n",
        "partitioned by (weekenddate)\r\n",
        "location 'abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/GA4/events_feb05_nov08/aggregate_sessions'\r\n",
        "as \r\n",
        "select  d.weekbegindate\r\n",
        ", d.weekenddate\r\n",
        ", device_category\r\n",
        ", stream_name\r\n",
        ",'sessions' as metric\r\n",
        ", sum(sessions) as sessions\r\n",
        " from delta.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/GA4/events_feb05_nov08/acquisition_sessions/` a\r\n",
        " inner join datedim d on a.date between d.weekbegindate and d.weekenddate\r\n",
        "group by d.weekbegindate\r\n",
        ", d.weekenddate\r\n",
        ", device_category\r\n",
        ", stream_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "select * from lakedb_gold.ga4_backfill_aggregate_sessions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create aggregated qty table for all weekends between Feb 05 2023 - Nov 08 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "create table lakedb_gold.ga4_backfill_aggregate_qty\r\n",
        "using delta \r\n",
        "partitioned by (weekenddate)\r\n",
        "location 'abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/GA4/events_feb05_nov08/aggregate_qty'\r\n",
        "as \r\n",
        "select   d.weekbegindate\r\n",
        ", d.weekenddate\r\n",
        ", device_category\r\n",
        ", stream_name\r\n",
        ", 'quantity' as metric\r\n",
        ", sum(items_purchased) as qty\r\n",
        " from delta.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/GA4/events_feb05_nov08/transaction_qty/` a\r\n",
        "inner join datedim d on a.date between d.weekbegindate and d.weekenddate\r\n",
        "group by d.weekbegindate\r\n",
        ", d.weekenddate\r\n",
        ", device_category\r\n",
        ", stream_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "select * from lakedb_gold.ga4_backfill_aggregate_qty;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create aggregated sales table for all weekends between Feb 05 2023 - Nov 08 2023\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "create table lakedb_gold.ga4_backfill_aggregate_sales\r\n",
        "using delta \r\n",
        "partitioned by (weekenddate)\r\n",
        "location 'abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/GA4/events_feb05_nov08/aggregate_sales'\r\n",
        "as \r\n",
        "select d.weekbegindate\r\n",
        ", d.weekenddate\r\n",
        ", device_category\r\n",
        ", stream_name\r\n",
        ", 'sales' as metric\r\n",
        ", round(sum(total_revenue-tax_amount-shipping_amount),3) as sales\r\n",
        " from delta.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/GA4/events_feb05_nov08/transactions/` a\r\n",
        " inner join datedim d on a.date between d.weekbegindate and d.weekenddate\r\n",
        " group by d.weekbegindate\r\n",
        ", d.weekenddate\r\n",
        ", device_category\r\n",
        ", stream_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "select * from lakedb_gold.ga4_backfill_aggregate_sales\r\n",
        "-- select distinct stream_name from lakedb_gold.ga4_backfill_aggregate_sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        " df_all_tabs = spark.sql(\"\"\" with all_dates as (select weekbegindate, weekenddate from datedim)\r\n",
        " ,t_category as (select 'mobile' as device_category \r\n",
        "   union all\r\n",
        "   select 'tablet'\r\n",
        "   union all\r\n",
        "   select 'desktop')\r\n",
        " ,t_brands as (select distinct stream_name from lakedb_gold.ga4_backfill_aggregate_sales )  \r\n",
        " ,all_data as (select * from all_dates, t_category, t_brands)\r\n",
        " ,t4 as (select * from lakedb_gold.ga4_backfill_aggregate_events_carts\r\n",
        "union all\r\n",
        "select * from lakedb_gold.ga4_backfill_aggregate_events_orders\r\n",
        "union all\r\n",
        "select * from lakedb_gold.ga4_backfill_aggregate_sessions\r\n",
        "union all\r\n",
        "select * from lakedb_gold.ga4_backfill_aggregate_qty\r\n",
        "union all\r\n",
        "select * from lakedb_gold.ga4_backfill_aggregate_sales)\r\n",
        "  SELECT property_brand, brand_country,concat(device_category,'_',metric) as metric,nvl(cnt,0) cnt\r\n",
        ",weekbegindate,weekenddate FROM t4\"\"\")\r\n",
        "\r\n",
        "df_stats = df_stats.groupBy('property_brand', 'brand_country','weekbegindate','weekenddate')\\\r\n",
        ".pivot('metric')\\\r\n",
        ".agg(F.first('cnt').alias('cnt'))\r\n",
        "\r\n",
        "\r\n",
        "#df_stats.createOrReplaceTempView('ga4_weekly_stats')\r\n",
        "display(df_stats)\r\n",
        "df_stats=  df_stats.repartition('weekenddate','property_brand','brand_country')\\\r\n",
        "    .write.format(\"delta\")\\\r\n",
        "    .mode(\"overwrite\")\\\r\n",
        "    .option(\"path\",f\"{gold_adls_path}GA4/weekly_stats\")\\\r\n",
        "    .option(\"mergeSchema\", \"true\")\\\r\n",
        "    .partitionBy('weekenddate','property_brand','brand_country')\\\r\n",
        "    .saveAsTable('lakedb_gold.ga4_weekly_stats')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Load the data using dates in DateDim table created above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# display(spark.sql(\"select * from datedim\"))\r\n",
        "# df_stats = spark.sql(\"\"\"\r\n",
        "# with t_streams as (select 'mobile' as device_category \r\n",
        "#   union all\r\n",
        "#   select 'tablet'\r\n",
        "#   union all\r\n",
        "#   select 'desktop')\r\n",
        "# , all_dates as (select * from datedim)\r\n",
        "# ,all_data as (select * from t_streams,all_dates)\r\n",
        "# , t1_carts as (select date\r\n",
        "# , device_category\r\n",
        "# , stream_name\r\n",
        "# , round(sum(event_count),3) as carts\r\n",
        "#  from delta.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/GA4/events_feb05_nov08/events/`\r\n",
        "# where event_name IN ('add_to_cart')\r\n",
        "# group by date\r\n",
        "# , device_category\r\n",
        "# , stream_name)\r\n",
        "# ,t2_orders as (select date\r\n",
        "# , device_category\r\n",
        "# , stream_name\r\n",
        "# , round(sum(event_count),3) as orders\r\n",
        "#  from delta.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/GA4/events_feb05_nov08/events/`\r\n",
        "# where event_name IN ('purchase', 'transaction')\r\n",
        "# group by date\r\n",
        "# , device_category\r\n",
        "# , stream_name )\r\n",
        "# , t3_sales as (select date\r\n",
        "# , device_category\r\n",
        "# , stream_name\r\n",
        "# , round(sum(total_revenue-tax_amount-shipping_amount),3) as sales\r\n",
        "#  from delta.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/GA4/events_feb05_nov08/transactions/`\r\n",
        "#  group by date\r\n",
        "# , device_category\r\n",
        "# , stream_name)\r\n",
        "# , t4_sessions as\r\n",
        "# (select  date\r\n",
        "# , device_category\r\n",
        "# , stream_name\r\n",
        "# , sum(sessions) as sessions\r\n",
        "#  from delta.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/GA4/events_feb05_nov08/acquisition_sessions/`\r\n",
        "# group by date\r\n",
        "# , device_category\r\n",
        "# , stream_name)\r\n",
        "# ,t5_qty as (select  date\r\n",
        "# , device_category\r\n",
        "# , stream_name\r\n",
        "# , sum(items_purchased) as qty\r\n",
        "#  from delta.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/GA4/events_feb05_nov08/transaction_qty/`\r\n",
        "# group by date\r\n",
        "# , device_category\r\n",
        "# , stream_name)\r\n",
        "# select all_data.date\r\n",
        "# , all_data.device_category\r\n",
        "# , t1.stream_name\r\n",
        "# , t1.carts\r\n",
        "# , t2.orders\r\n",
        "# , t3.sales\r\n",
        "# , t4.sessions\r\n",
        "# , t5.qty\r\n",
        "# from all_data left join t1_carts t1 on all_data.date = t1.date and all_data.device_category = t1.device_category\r\n",
        "#  left join t2_orders t2 on t1.date = t2.date and t1.device_category = t2.device_category\r\n",
        "# and t1.stream_name = t2.stream_name\r\n",
        "# left join t3_sales t3 on t1.date = t3.date and t1.device_category = t3.device_category\r\n",
        "# and t1.stream_name = t3.stream_name\r\n",
        "# left join t4_sessions t4 on t1.date = t4.date and t1.device_category = t4.device_category\r\n",
        "# and t1.stream_name = t4.stream_name\r\n",
        "# left join t5_qty t5 on t1.date = t5.date and t1.device_category = t5.device_category\r\n",
        "# and t1.stream_name = t5.stream_name\r\n",
        "# where all_data.date = '2023-11-08'\"\"\")\r\n",
        "\r\n",
        "# df_stats = df_stats.groupBy('property_brand', 'brand_country','weekbegindate','weekenddate')\\\r\n",
        "# .pivot('metric')\\\r\n",
        "# .agg(F.first('cnt').alias('cnt'))\r\n",
        "\r\n",
        "\r\n",
        "# #df_stats.createOrReplaceTempView('ga4_weekly_stats')\r\n",
        "# display(df_stats)\r\n",
        "# df_stats=  df_stats.repartition('weekenddate','property_brand','brand_country')\\\r\n",
        "#     .write.format(\"delta\")\\\r\n",
        "#     .mode(\"overwrite\")\\\r\n",
        "#     .option(\"path\",f\"{gold_adls_path}GA4/weekly_stats\")\\\r\n",
        "#     .option(\"mergeSchema\", \"true\")\\\r\n",
        "#     .partitionBy('weekenddate','property_brand','brand_country')\\\r\n",
        "#     .saveAsTable('lakedb_gold.ga4_weekly_stats')\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "select all_data.date\r\n",
        ", all_data.device_category\r\n",
        ", t1.stream_name\r\n",
        ", t1.carts\r\n",
        ", t2.orders\r\n",
        ", t3.sales\r\n",
        ", t4.sessions\r\n",
        ", t5.qty\r\n",
        "from all_data left join t1_carts t1 on all_data.date = t1.date and all_data.device_category = t1.device_category\r\n",
        " left join t2_orders t2 on t1.date = t2.date and t1.device_category = t2.device_category\r\n",
        "and t1.stream_name = t2.stream_name\r\n",
        "left join t3_sales t3 on t1.date = t3.date and t1.device_category = t3.device_category\r\n",
        "and t1.stream_name = t3.stream_name\r\n",
        "left join t4_sessions t4 on t1.date = t4.date and t1.device_category = t4.device_category\r\n",
        "and t1.stream_name = t4.stream_name\r\n",
        "left join t5_qty t5 on t1.date = t5.date and t1.device_category = t5.device_category\r\n",
        "and t1.stream_name = t5.stream_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "select * from lakedb_gold.ga4_weekly_stats\r\n",
        "where 1=1\r\n",
        "        and weekenddate= 20240601 order by property_brand,brand_country\r\n",
        "   \r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "select weekenddate,count(*)\r\n",
        " from lakedb_gold.ga4_weekly_stats\r\n",
        " group by weekenddate\r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}