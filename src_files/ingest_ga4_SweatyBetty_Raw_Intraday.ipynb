{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# GA4 Sweaty Betty Load Intraday - Raw Data\r\n",
        "\r\n",
        "\r\n",
        "**Revision History**<br>\r\n",
        "Created 2/27/2025 Vish<br>\r\n",
        "This notebook ingests raw data from google Analytics for Sweaty Betty\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "import concurrent.futures\r\n",
        "from delta import *\r\n",
        "from pyspark.sql.types import StructType, StructField, ArrayType, StringType, LongType, DoubleType, BooleanType, MapType,IntegerType\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from functools import reduce\r\n",
        "from pyspark.sql.dataframe import DataFrame\r\n",
        "import pyspark.sql.functions as F\r\n",
        "import json\r\n",
        "import base64\r\n",
        "from datetime import datetime,timedelta\r\n",
        "from time import sleep\r\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"DYNAMIC\")\r\n",
        "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\r\n",
        "from azure.storage.blob import BlobServiceClient\r\n",
        "from pyspark.sql.functions import max as spark_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "!pip install google-cloud-bigquery\r\n",
        "!pip install google-auth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "from google.cloud import bigquery\r\n",
        "from google.oauth2 import service_account"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Run the common functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Retrieve Google Big Query Credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "project = \"sb-dw-ga-bigquery-link\"\r\n",
        "\r\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary  \r\n",
        "ga4_credentials = token_library.getSecret(kv_name, \"GA4-SweatyBetty\", \"ls_kv_adap\")  \r\n",
        "print(ga4_credentials)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Decode the base64-encoded string\r\n",
        "decoded_key_file_content = base64.b64decode(ga4_credentials).decode('utf-8')\r\n",
        "key_file_dict = json.loads(decoded_key_file_content)\r\n",
        "credentials = service_account.Credentials.from_service_account_info(key_file_dict)\r\n",
        "print(\"Trying to authenticate\")\r\n",
        "# Create BigQuery client with explicit authentication\r\n",
        "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\r\n",
        "print(\"Authenticated\")\r\n",
        "print(\"Project ID:\", client.project)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "# Dictionary to store datasets and their tables\r\n",
        "bq_data = {}\r\n",
        "\r\n",
        "print(\"Trying to list datasets\")\r\n",
        "datasets = list(client.list_datasets())\r\n",
        "\r\n",
        "if datasets:\r\n",
        "    print(\"Datasets in the project:\")\r\n",
        "    for dataset in datasets:\r\n",
        "        dataset_id = dataset.dataset_id\r\n",
        "        print(f\"- {dataset_id}\")\r\n",
        "\r\n",
        "        # Get all tables for the dataset\r\n",
        "        tables = list(client.list_tables(dataset_id))\r\n",
        "        table_names = [table.table_id for table in tables] if tables else []\r\n",
        "\r\n",
        "        # Store in dictionary\r\n",
        "        bq_data[dataset_id] = table_names\r\n",
        "else:\r\n",
        "    print(\"No datasets found in the project.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get All tables in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "# Dictionary to store datasets and their tables\r\n",
        "bq_data = {}\r\n",
        "\r\n",
        "print(\"Trying to list datasets\")\r\n",
        "datasets = list(client.list_datasets())\r\n",
        "\r\n",
        "table_count = 0\r\n",
        "\r\n",
        "if datasets:\r\n",
        "    print(\"Datasets in the project:\")\r\n",
        "    for dataset in datasets:\r\n",
        "        dataset_id = dataset.dataset_id\r\n",
        "        print(f\"- {dataset_id}\")\r\n",
        "\r\n",
        "        # Get all tables for the dataset\r\n",
        "        tables = list(client.list_tables(dataset_id))\r\n",
        "        table_names = [table.table_id for table in tables] if tables else []\r\n",
        "\r\n",
        "        # Store in dictionary\r\n",
        "        bq_data[dataset_id] = table_names\r\n",
        "\r\n",
        "        # Print tables in the format catalog.tablename\r\n",
        "        for table_name in table_names:\r\n",
        "            print(f\"{dataset_id}.{table_name}\")\r\n",
        "            table_count += 1\r\n",
        "\r\n",
        "    print(f\"Total number of tables: {table_count}\")\r\n",
        "else:\r\n",
        "    print(\"No datasets found in the project.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Filter Tables for today"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "from datetime import datetime\r\n",
        "\r\n",
        "# Get today's date in YYYYMMDD format\r\n",
        "today_date = datetime.utcnow().strftime(\"%Y%m%d\")\r\n",
        "\r\n",
        "# Dictionary to store filtered datasets\r\n",
        "filtered_bq_data = {}\r\n",
        "\r\n",
        "# Iterate through the original bq_data\r\n",
        "for dataset_id, table_list in bq_data.items():\r\n",
        "    # Filter tables that contain today's date\r\n",
        "    filtered_tables = [table for table in table_list if today_date in table]\r\n",
        "\r\n",
        "    # Only add datasets that have matching tables\r\n",
        "    if filtered_tables:\r\n",
        "        filtered_bq_data[dataset_id] = filtered_tables\r\n",
        "\r\n",
        "# Print filtered results\r\n",
        "print(\"Filtered datasets and tables with today's date:\")\r\n",
        "for dataset, tables in filtered_bq_data.items():\r\n",
        "    for table in tables:\r\n",
        "        print(f\"{dataset}.{table}\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Filter Tables for a Date Range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "'''\r\n",
        "#This section can be uncommented and used to pull data for a specific range.\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "# Define your date range (in YYYYMMDD format)\r\n",
        "start_date = \"20250401\"\r\n",
        "end_date = \"20250417\"\r\n",
        "\r\n",
        "# Convert strings to datetime objects for comparison\r\n",
        "start_dt = datetime.strptime(start_date, \"%Y%m%d\")\r\n",
        "end_dt = datetime.strptime(end_date, \"%Y%m%d\")\r\n",
        "\r\n",
        "# Dictionary to store filtered datasets\r\n",
        "filtered_bq_data = {}\r\n",
        "\r\n",
        "# Iterate through the original bq_data\r\n",
        "for dataset_id, table_list in bq_data.items():\r\n",
        "    filtered_tables = []\r\n",
        "\r\n",
        "    for table in table_list:\r\n",
        "        # Extract date from table name using 8-digit sequence\r\n",
        "        for i in range(len(table) - 7):\r\n",
        "            substr = table[i:i+8]\r\n",
        "            if substr.isdigit():\r\n",
        "                try:\r\n",
        "                    table_date = datetime.strptime(substr, \"%Y%m%d\")\r\n",
        "                    if start_dt <= table_date <= end_dt:\r\n",
        "                        filtered_tables.append(table)\r\n",
        "                        break  # Only add once per match\r\n",
        "                except ValueError:\r\n",
        "                    continue\r\n",
        "\r\n",
        "    if filtered_tables:\r\n",
        "        filtered_bq_data[dataset_id] = filtered_tables\r\n",
        "\r\n",
        "# Print filtered results\r\n",
        "print(f\"Filtered datasets and tables between {start_date} and {end_date}:\")\r\n",
        "for dataset, tables in filtered_bq_data.items():\r\n",
        "    for table in tables:\r\n",
        "        print(f\"{dataset}.{table}\")\r\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {},
      "source": [
        "json_blob_path =f\"{raw_adls_path}/GA4_SweatyBetty/bigquery_datasets_tables.json\"\r\n",
        "base_folder = \"GA4_SweatyBetty\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "def process_bigquery_tables(filtered_bq_data, raw_adls_path, base_folder, ga4_credentials):\r\n",
        "    \"\"\"\r\n",
        "    Reads all tables from BigQuery as specified in the filtered_bq_data dictionary, \r\n",
        "    loads them into Spark DataFrames, and saves them as Parquet in ADLS.\r\n",
        "\r\n",
        "    Parameters:\r\n",
        "    filtered_bq_data (dict): Dictionary containing datasets and tables to process.\r\n",
        "    raw_adls_path (str): Base Azure Data Lake Storage (ADLS) path.\r\n",
        "    base_folder (str): Folder in ADLS where data will be saved.\r\n",
        "    ga4_credentials (str): Google Analytics 4 credentials for BigQuery access.\r\n",
        "    \"\"\"\r\n",
        "    spark = SparkSession.builder \\\r\n",
        "        .appName(\"BigQueryToParquet\") \\\r\n",
        "        .getOrCreate()\r\n",
        "\r\n",
        "    parent_project = \"sb-dw-ga-bigquery-link\"  # Update this to your actual parent project\r\n",
        "\r\n",
        "    for dataset, tables in filtered_bq_data.items():\r\n",
        "        for table_name in tables:\r\n",
        "            try:\r\n",
        "                print(f\"Reading table: {dataset}.{table_name}\")\r\n",
        "\r\n",
        "                # Read from BigQuery\r\n",
        "                df = spark.read.format(\"bigquery\") \\\r\n",
        "                    .option(\"credentials\", ga4_credentials) \\\r\n",
        "                    .option(\"parentProject\", parent_project) \\\r\n",
        "                    .option(\"dataset\", dataset) \\\r\n",
        "                    .option(\"table\", table_name) \\\r\n",
        "                    .load()\r\n",
        "\r\n",
        "                # Dataset is the project name (e.g., analytics_455108528)\r\n",
        "                project_name = dataset\r\n",
        "\r\n",
        "                # Extract dataset_name from table name (everything before last underscore and date)\r\n",
        "                dataset_name = \"_\".join(table_name.split(\"_\")[:-1])  # e.g., events from events_20250325\r\n",
        "\r\n",
        "                output_folder = f\"{raw_adls_path}{base_folder}/{project_name}/{dataset_name}/{table_name}\"\r\n",
        "                print(f\"Saving table {dataset}.{table_name} to {output_folder}\")\r\n",
        "\r\n",
        "                df.write.format(\"parquet\").mode(\"overwrite\").save(output_folder)\r\n",
        "\r\n",
        "                print(f\"Successfully saved {dataset}.{table_name} to {output_folder}\")\r\n",
        "\r\n",
        "            except Exception as e:\r\n",
        "                print(f\"Error processing {dataset}.{table_name}: {str(e)}\")\r\n",
        "\r\n",
        "    print(\"All tables processed successfully!\")\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "process_bigquery_tables(filtered_bq_data, raw_adls_path, base_folder, ga4_credentials)"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}