{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Peter Daniels - 2025 August\n",
        "\n",
        "Apparently, the old 8x8 \"Post Call Survery\" API will be switched off on 2025-08-20, so we need to use the new API which is documentted here:\n",
        "\n",
        "https://developer.8x8.com/analytics/docs/customer-experience-post-call-survey\n",
        "\n",
        "The base URL is region specific, based on the location of your Contact Center tenant.\n",
        "\n",
        "- United States: https://api.8x8.com/analytics/cc/{version}/historical-metrics/\n",
        "- Europe: https://api.8x8.com/eu/analytics/cc/{version}/historical-metrics/\n",
        "- Asia-Pacific: https://api.8x8.com/au/analytics/cc/{version}/historical-metrics/\n",
        "- Canada: https://api.8x8.com/ca/analytics/cc/{version}/historical-metrics/\n",
        "- {version} to be replaced by current Version. As of June 2023 this is 7 resulting in /v7/\n",
        "\n",
        "So, our base URL for historical metrics is: https://api.8x8.com/analytics/cc/v7/historical-metrics/\n",
        "and we use a \"type\" of \"detailed-reports-survey\" in the create-report API call.\n",
        "\n",
        "I am using this notebook to replace the old \"pcs\" subject_area code in the existing contact_center_ops_api_processing notebook rather than try to munge the new API calls into that maze of code.\n",
        "\n",
        "This code gets most of the data elements for the existing raw pcs delta table from eth new API, then joins to the raw delats table we built from the old-code's detailed-reports-interaction-details call to get the durations. The idea is to append to the same raw delta table the old API code produced so that down stream processing and consumption (e.g. 8x8_perform_transformations notebook) remains the same.\n",
        "\n",
        "The /historical_analytics/incremental/20250828/ (for example) data has interactionId and transactionId, which are always single valued. To handle new PCS API interactionIds and transacitonIds where there might be multiple Ids in a list, we are taking the first element of the list.  That allows us to join to the /historical_analytics/incremental delta table to get durations and times.\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Questions and Answers\n",
        "\n",
        "1. Is there any filtering we should be doing?  The old code's API had a region parameter - so we sent a call to get each region's PCS data: 'regions':[69,70,301,304,303,321,305]. These were only US regions.  It had a commented out line for UK regions. 'regions': [301,304,322,303,321,305,324].\n",
        "\n",
        "    **Answer: No, no filtering. report filters are being handled downstream.**\n",
        "\n",
        "2. In order to fit the raw schema that the old PCS API code created for downstream processing into gold, we would need \"hits\" and \"QuestionLabel\" columns populated. We don't have hits, so considering just populating with 0.  QuestionLabel might be populated by some logic that looks at the survey type - TBD. Not sure how relliant the downstream xform code is on this being accurate. However, I did see that \"hits\" was not in the gold table/view (gold.CcoPcs), so unless there are reports based on raw (bad idea), we should be OK with 0 for hits.\n",
        "    \n",
        "    **Answer: this is one for Ethan Boelkins.  He says these are not used, so we are OK.**\n",
        "3. I have not deeply reviewed the raw-to-gold processing in 8x8_perform_transformations notebook or how the final gold view (gold.CcoPcs) is consumed in order to get a solid sense of how important various data elements are.\n",
        "\n",
        "\n",
        "    ** Answer: I would defer to the answer on two for this. In the next call, lets figure out which exact fields **\n",
        "    **are mismatched in the new API vs the old one to determine any impact. Verified with Ethan that we have the data we need.**\n",
        "\n",
        "4. The old PCS API died 2025-08-21. Do we need some capability to go back in time and reload days since then using the new API? Right now, like the old code, it just gets the previous day's data.\n",
        "\n",
        "    **Answer: We added manual date range setting in the code to do a historical load**\n",
        "\n",
        "5. The new API uses transactionIds and interactionIds (plural) our code currently picks the first Id in those columns to join to the histaorical analytics data to get our durations/times/etc. Is this OK? Since the duraton data may not be of import, this may not be an issue, but worth noting here again.\n",
        "\n",
        "    **Answer: I think yes, as here is the behavior that I have observed so far: 1. All interaction IDs coming from the Interaction Details API, they are all unique. 2. The interaction duration is not relevant to survey reporting. It could be at some point, so it would probably be best to have continuity here, and to me that appears to be taking the first queue and transaction ID from the survey records to tie that to. This again is likely something that Ethan can confirm. Confimred.  Agreed.**\n",
        "\n",
        "6.  Do we want 0s and 0.0s for ALL duration and times when we fail to lookup interaction details via first interactionId and first transactionId? \n",
        "\n",
        "    **Answer: We are now setting them to \"0\" when NULL (JOIN to interaction deails failed)**\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from base64 import b64encode\n",
        "from datetime import date, timedelta, datetime, timezone\n",
        "import re\n",
        "import json\n",
        "from pyspark.sql import Row, functions as F, types as T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Includes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## API Connection Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Token URL\n",
        "access_token_url = \"https://api.8x8.com/oauth/v2/token\"\n",
        "\n",
        "# Get a token (how?)\n",
        "#  Need the Admin Console clientId and secret to get an access token.\n",
        "# \n",
        "# curl #location --request POST 'https://api.8x8.com/oauth/v2/token' \\\n",
        "# --header 'Content-Type: application/x-www-form-urlencoded' \\\n",
        "# --header 'Authorization: Basic base64encode({clientId}:{secret})' \n",
        "# --data-urlencode 'grant_type=client_credentials'\n",
        "\n",
        "# Client ID and client secret are used to build an auth string to an access token\n",
        "# We get the client secret from AKV\n",
        "client_id = \"YonXyRNfVMGFcwOOe2Dna8GTHEcGHsfy\"\n",
        "client_secret = mssparkutils.credentials.getSecret(kv_name, \"cco-8x8-client-secret\", \"ls_kv_adap\")\n",
        "print(f\"Retrieved cco-8x8-client-secret from AKV for getting an access token (partial output here): {client_secret[:3]}...{client_secret[-3:]}\")\n",
        "\n",
        "auth_string = f'{client_id}:{client_secret}'\n",
        "auth_header = b64encode(auth_string.encode()).decode()\n",
        "#print(f\"auth_header: {auth_header}\")\n",
        "\n",
        "headers = {'Authorization': f'Basic {auth_header}',\n",
        "           'Content-Type': 'application/x-www-form-urlencoded'}\n",
        "\n",
        "data = { 'grant_type': 'client_credentials'}\n",
        "\n",
        "resp = requests.post(access_token_url, headers=headers, data=data, timeout=30)\n",
        "resp.raise_for_status()          # raises on 4xx/5xx\n",
        "response_json = resp.json()      # now safely parse JSON\n",
        "\n",
        "access_token = response_json['access_token'] # We use this access token in all subsequent API calls\n",
        "#print(access_token)\n",
        "print(\"Access token generated for subsequent API calls\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Set our datetime range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Normal code - yesterday\n",
        "# Get yesterday's start and end datetimes (ISO 8601 format - yyyy-MM-ddThh:mm:ss.###Z)\n",
        "print(\"Building our start and end datetime params for the report creation API call\")\n",
        "yesterday = date.today() - timedelta(days=1)\n",
        "# TODO: ARE We supposed to use UTC? When generating \"yesterday\"? The old code did not, so I am leaving it as-is for now.\n",
        "# Get yesterday's date in UTC?\n",
        "# utc_today = datetime.now(timezone.utc).date()\n",
        "# yesterday = utc_today - timedelta(days=1)\n",
        "\n",
        "# Start of yesterday (midnight)\n",
        "yesterday_start_str = datetime.combine(yesterday, datetime.min.time()).strftime('%Y-%m-%dT%H:%M:%S.000Z')\n",
        "# End of yesterday (23:59:59)\n",
        "yesterday_end_str = datetime.combine(yesterday, datetime.max.time().replace(microsecond=0)).strftime('%Y-%m-%dT%H:%M:%S.999Z')\n",
        "\n",
        "print(\"yesterday_start_str:\", yesterday_start_str)\n",
        "print(\"yesterday_end_str:\", yesterday_end_str)\n",
        "\n",
        "# Use a generic start and end datetime str variable\n",
        "start_datetime_str = yesterday_start_str\n",
        "end_datetime_str = yesterday_end_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Manual Code to set a particular datetime range.\n",
        "# IMPORTANT! This should be commented out for normal execution.\n",
        "# IMPORTANT! You still need to run the code cell above in order to compare our dates to yesterday further down.\n",
        "# The last successful loadDate for Prod with the old API was 2025-08-20 for 2025-08-19 call data, \n",
        "# so we should start with the 20th's data for that historical load.\n",
        "#start_datetime_str = \"2025-08-20T00:00:00.000Z\"\n",
        "# I think just use yesterday for the end.  You can put whatever you want here anyhoo.\n",
        "#end_datetime_str = yesterday_end_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(f\"Using start_datetime_str: {start_datetime_str} in our API call dateRange param\")\n",
        "print(f\"Using end_datetime_str: {end_datetime_str} in our API call dateRange param\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Make API calls to create the detailed-reports-survey report and access it - page by page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "report_request_headers = {'Authorization':f'Bearer {access_token}'}\n",
        "report_request_url = 'https://api.8x8.com/analytics/cc/v7/historical-metrics/detailed-reports'\n",
        "\n",
        "# We are using the timezone element in the body that the previous code used.\n",
        "# Build our API body for creating the report. We are using yesterday's dates.\n",
        "body = {'type':'detailed-reports-survey',\n",
        "        'title': 'interactions',\n",
        "        'timezone': 'America/New_York',\n",
        "        'dateRange':\n",
        "            {'start': start_datetime_str,\n",
        "            'end': end_datetime_str\n",
        "             },\n",
        "        }\n",
        "print(body)\n",
        "\n",
        "# -------------------------------Create the report -----------------------------\n",
        "print(\"Making the 'create report' API call...\")\n",
        "report_request = requests.post(headers=report_request_headers, url=report_request_url, json=body)\n",
        "report_request.raise_for_status()\n",
        "report_request_data = report_request.json()\n",
        "print(report_request_data)\n",
        "\n",
        "# From the create report API call, we get a link/URL to the report\n",
        "report_link = report_request.headers.get('Link')\n",
        "if not report_link:\n",
        "    raise RuntimeError(\"No 'Link' header returned from report creation.\")\n",
        "#print(report_request)\n",
        "print(report_link)\n",
        "\n",
        "match = re.search(r'<(.+?)>; rel=\"data\"', report_link) #regex expression that drops the unneeded details from the report link.\n",
        "report_data_url = None # Use this variable to access the report.\n",
        "if match:\n",
        "    report_data_url = match.group(1)\n",
        "    report_data_url = report_data_url #assigns the needed link back to the variable outside the If statement for easier usage.\n",
        "\n",
        "#---------------------------------------Access the report created in the prior call----------------------------------------------------\n",
        "\n",
        "all_data = []\n",
        "report_access_headers = {'Authorization':f'Bearer {access_token}', 'Accept': 'application/json'}\n",
        "report_access = requests.get(report_data_url, headers = report_access_headers)\n",
        "report_access.raise_for_status()\n",
        "\n",
        "while True:\n",
        "    data = report_access.json() # Grabs the data from the first page response.\n",
        "    all_data.append(data) #appends the data to all_data which will be eventually cleaned.\n",
        "\n",
        "    head = report_access.headers.get('Link')\n",
        "    if not head:\n",
        "        break\n",
        "\n",
        "    match = re.search(r'<(.+?)>; rel=\"next\"', head)\n",
        "    if not match:\n",
        "        break\n",
        "\n",
        "    next_url = match.group(1)\n",
        "    print(f\"next_url: {next_url}\")\n",
        "    report_access = requests.get(next_url, headers=report_access_headers) #If the header contains a URL to access the next page, call it.\n",
        "    report_access.raise_for_status()\n",
        "\n",
        "print(\"Retrieved all response data from our report.\")\n",
        "\n",
        "#all_data_pretty = json.dumps(all_data, indent=4)\n",
        "#print(all_data_pretty)\n",
        "\n",
        "#-------------------------------- Handling the Response-------------------------------------------\n",
        "\n",
        "# TODO: Dump the truly raw data into the datalake before \"cleaning\"/flattening?\n",
        "\n",
        "print(\"Cleaning the data into a nice format\")\n",
        "cleaned_data = []\n",
        "for outer in all_data:\n",
        "    # Each page can be a list; each list element can be a dict with 'items'\n",
        "    for record in outer if isinstance(outer, list) else []:\n",
        "        if not isinstance(record, dict) or 'items' not in record:\n",
        "            continue\n",
        "        row = {}\n",
        "        for item in record['items']:\n",
        "            key = item.get('key')\n",
        "            value = item.get('value')\n",
        "            if key is None:\n",
        "                continue\n",
        "\n",
        "            if isinstance(value, list):\n",
        "                if all(isinstance(v, dict) for v in value):\n",
        "                    # List of dicts, e.g. results -> results_1_answerDigit, etc.\n",
        "                    for i, obj in enumerate(value, start=1):\n",
        "                        for sub_k, sub_v in obj.items():\n",
        "                            row[f\"{key}_{i}_{sub_k}\"] = sub_v\n",
        "                else:\n",
        "                    # List of scalars -> comma-join\n",
        "                    row[key] = \", \".join(map(str, value))\n",
        "            else:\n",
        "                row[key] = value\n",
        "        cleaned_data.append(row)\n",
        "\n",
        "if not cleaned_data:\n",
        "    raise RuntimeError(\"No rows returned from API (cleaned_data is empty).\")\n",
        "\n",
        "print(\"Finished cleaning our reponse/report data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Convert cleaned data to Spark dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Convert to spark dataframe, etc.\n",
        "\n",
        "# *****************************************************************\n",
        "# Let's get a spark dataframe for ALL columns  (as strings) for figuring out addl columns needed for filtering.\n",
        "# 1) Collect all possible keys across all dicts\n",
        "all_keys = sorted({k for r in cleaned_data if isinstance(r, dict) for k in r.keys()})\n",
        "\n",
        "# 2) Normalize rows (missing keys → None)\n",
        "rows = [{k: r.get(k) for k in all_keys} for r in cleaned_data if isinstance(r, dict)]\n",
        "\n",
        "# 3) Build stable string-typed schema\n",
        "schema = T.StructType([T.StructField(k, T.StringType(), True) for k in all_keys])\n",
        "\n",
        "# 4) Create DataFrame\n",
        "df_spark_all = spark.createDataFrame([Row(**row) for row in rows], schema=schema)\n",
        "\n",
        "#df_spark_all.printSchema()\n",
        "# root\n",
        "#  |-- achievableScore: string (nullable = true)\n",
        "#  |-- actualScore: string (nullable = true)\n",
        "#  |-- agentGroupIds: string (nullable = true)\n",
        "#  |-- agentGroupNames: string (nullable = true)\n",
        "#  |-- agentIds: string (nullable = true)\n",
        "#  |-- agentNames: string (nullable = true)\n",
        "#  |-- callerName: string (nullable = true)\n",
        "#  |-- callerNumber: string (nullable = true)\n",
        "#  |-- channelId: string (nullable = true)\n",
        "#  |-- endTime: string (nullable = true)\n",
        "#  |-- interactionIds: string (nullable = true) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Key column\n",
        "#  |-- queueIds: string (nullable = true)\n",
        "#  |-- queueNames: string (nullable = true)\n",
        "#  |-- results_1_answerDigit: string (nullable = true)\n",
        "#  |-- results_1_answerType: string (nullable = true)\n",
        "#  |-- results_1_questionId: string (nullable = true)\n",
        "#  |-- results_1_questionTitle: string (nullable = true)\n",
        "#  |-- results_1_questionType: string (nullable = true)\n",
        "#  |-- results_1_scaleMax: string (nullable = true)\n",
        "#  |-- results_1_scaleMin: string (nullable = true)\n",
        "#  |-- results_1_voiceRecordingUuid: string (nullable = true)\n",
        "#  |-- results_2_answerDigit: string (nullable = true)\n",
        "#  |-- results_2_answerType: string (nullable = true)\n",
        "#  |-- results_2_questionId: string (nullable = true)\n",
        "#  |-- results_2_questionTitle: string (nullable = true)\n",
        "#  |-- results_2_questionType: string (nullable = true)\n",
        "#  |-- results_2_scaleMax: string (nullable = true)\n",
        "#  |-- results_2_scaleMin: string (nullable = true)\n",
        "#  |-- results_2_voiceRecordingUuid: string (nullable = true)\n",
        "#  |-- startTime: string (nullable = true)\n",
        "#  |-- surveyDuration: string (nullable = true) ??????????????????????????????\n",
        "#  |-- surveyId: string (nullable = true)\n",
        "#  |-- surveyIsDeleted: string (nullable = true)\n",
        "#  |-- surveyName: string (nullable = true) ??????????????????????????????????\n",
        "#  |-- surveyScorePercentage: string (nullable = true)\n",
        "#  |-- surveyType: string (nullable = true) ??????????????????????????????????\n",
        "#  |-- transactionIds: string (nullable = true) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Key Column\n",
        "\n",
        "#display(df_spark_all)\n",
        "\n",
        "# *****************************************************************\n",
        "\n",
        "# We only need certain columns fro proceed - keep it lean.\n",
        "wanted = [\n",
        "    \"interactionIds\",\"startTime\",\"callerName\",\"callerNumber\",\n",
        "    \"results_1_answerDigit\",\"results_2_answerDigit\",\n",
        "    \"actualScore\",\"agentIds\",\"queueIds\",\"transactionIds\",\n",
        "]\n",
        "\n",
        "# 1) Filter at the source (missing keys become None)\n",
        "rows = [{k: r.get(k) for k in wanted} for r in cleaned_data if isinstance(r, dict)]\n",
        "\n",
        "# 2) Give Spark a stable schema (all strings first is very robust)\n",
        "schema = T.StructType([T.StructField(k, T.StringType(), True) for k in wanted])\n",
        "df_spark = spark.createDataFrame([Row(**row) for row in rows], schema=schema)\n",
        "\n",
        "print(\"Converted cleaned_data to a spark dataframe, df_spark, with schema:\")\n",
        "df_spark.printSchema()\n",
        "# root\n",
        "#  |-- interactionIds: string (nullable = true)\n",
        "#  |-- startTime: string (nullable = true)\n",
        "#  |-- callerName: string (nullable = true)\n",
        "#  |-- callerNumber: string (nullable = true)\n",
        "#  |-- results_1_answerDigit: string (nullable = true)\n",
        "#  |-- results_2_answerDigit: string (nullable = true)\n",
        "#  |-- actualScore: string (nullable = true)\n",
        "#  |-- agentIds: string (nullable = true)\n",
        "#  |-- queueIds: string (nullable = true)\n",
        "#  |-- transactionIds: string (nullable = true)\n",
        "\n",
        "#display(df_spark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "#display(df_spark.limit(100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Join to historical_analytics data we captured in legacy notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Get our dates to load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [],
      "metadata": {},
      "source": [
        "# What dates of historical_analytics do we need?\n",
        "# Get multiple paths if our start_datetime_str is not the same as our yesterday_start_str (hist load)\n",
        "# Otherwise, normal processing - get today's date\n",
        "if start_datetime_str != yesterday_start_str:\n",
        "    print(f\"start_datetime_str ({start_datetime_str}) is not yesterday_start_str ({yesterday_start_str})\")\n",
        "    print(\"Building a list of dates to get our historical_analytics data to join to.\")\n",
        "\n",
        "    # Parse strings (format is always: 2025-08-20T00:00:00.000Z)\n",
        "    start_dt = datetime.strptime(start_datetime_str, \"%Y-%m-%dT%H:%M:%S.%fZ\").replace(tzinfo=timezone.utc)\n",
        "    end_dt   = datetime.strptime(end_datetime_str,   \"%Y-%m-%dT%H:%M:%S.%fZ\").replace(tzinfo=timezone.utc)\n",
        "\n",
        "    # Build list of dates (inclusive)\n",
        "    dates_to_get = []\n",
        "    current_date = start_dt.date()\n",
        "    end_date     = end_dt.date()\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        dates_to_get.append(current_date.strftime(\"%Y%m%d\"))\n",
        "        current_date += timedelta(days=1)\n",
        "else:\n",
        "    today_str = datetime.now(timezone.utc).strftime(\"%Y%m%d\")\n",
        "    print(today_str)\n",
        "    dates_to_get = [today_str]\n",
        "\n",
        "print(\"dates we will load for 'historical_analytics/incremental/yyyyMMdd/':\")\n",
        "print(dates_to_get)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Load historical_analytics incremental data for our dates (usually just today's)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [],
      "metadata": {},
      "source": [
        "# We have to get today's historical_analytics load OR a range of dates if we're loding hostory\n",
        "# Example delta path:\n",
        "#abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/historical_analytics/incremental/20250827\n",
        "print(\"Loading incremental dates for historical_analytics.\")\n",
        "\n",
        "df_hist_analytics = None\n",
        "\n",
        "for date_str in dates_to_get:\n",
        "    hist_analytics_delta_path = f\"{raw_adls_path}historical_analytics/incremental/{date_str}\"\n",
        "    try:\n",
        "        df_temp = spark.read.format(\"delta\").load(hist_analytics_delta_path)\n",
        "        if df_hist_analytics is None:\n",
        "            df_hist_analytics = df_temp\n",
        "        else:\n",
        "            df_hist_analytics = df_hist_analytics.unionByName(df_temp)\n",
        "        print(f\"✅ Loaded {hist_analytics_delta_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Skipping {hist_analytics_delta_path}: {e}\")\n",
        "\n",
        "if df_hist_analytics is None:\n",
        "    raise RuntimeError(\"❌ No dataframes were loaded — be sure to run the main contact_center_ops_api_proicessing notebook befor this so we have the interaction details for our date(s).\")\n",
        "\n",
        "print(\"Done loading incremental dates for historical_analytics.\")\n",
        "#df_hist_analytics.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "#display(df_hist_analytics.limit(20))\n",
        "# df_hist_analytics.printSchema()\n",
        "# root\n",
        "#  |-- participants: string (nullable = true)\n",
        "#  |-- time: string (nullable = true)\n",
        "#  |-- agentNotes: string (nullable = true)\n",
        "#  |-- blindTransferToAgent: string (nullable = true)\n",
        "#  |-- blindTransferToQueue: string (nullable = true)\n",
        "#  |-- campaignId: string (nullable = true)\n",
        "#  |-- campaignName: string (nullable = true)\n",
        "#  |-- caseFollowUp: string (nullable = true)\n",
        "#  |-- caseNumber: string (nullable = true)\n",
        "#  |-- channelId: string (nullable = true)\n",
        "#  |-- chatType: string (nullable = true)\n",
        "#  |-- conferencesEstablished: string (nullable = true)\n",
        "#  |-- consultationsEstablished: string (nullable = true)\n",
        "#  |-- creationTime: string (nullable = true)\n",
        "#  |-- customerName: string (nullable = true)\n",
        "#  |-- destination: string (nullable = true)\n",
        "#  |-- direction: string (nullable = true)\n",
        "#  |-- dispositionAction: string (nullable = true)\n",
        "#  |-- externalTransactionData: string (nullable = true)\n",
        "#  |-- finishedTime: string (nullable = true)\n",
        "#  |-- interactionId: string (nullable = true)\n",
        "#  |-- interactionLabels: string (nullable = true)\n",
        "#  |-- interactionType: string (nullable = true)\n",
        "#  |-- ivrTreatmentDuration: string (nullable = true)\n",
        "#  |-- mediaType: string (nullable = true)\n",
        "#  |-- originalInteractionId: string (nullable = true)\n",
        "#  |-- originalTransactionId: string (nullable = true)\n",
        "#  |-- origination: string (nullable = true)\n",
        "#  |-- outboundPhoneCode: string (nullable = true)\n",
        "#  |-- outboundPhoneCodeId: string (nullable = true)\n",
        "#  |-- outboundPhoneCodeList: string (nullable = true)\n",
        "#  |-- outboundPhoneCodeListId: string (nullable = true)\n",
        "#  |-- outboundPhoneCodeText: string (nullable = true)\n",
        "#  |-- outboundPhoneShortCode: string (nullable = true)\n",
        "#  |-- participantAssignNumber: string (nullable = true)\n",
        "#  |-- participantBusyDuration: string (nullable = true)\n",
        "#  |-- participantHandlingDuration: string (nullable = true)\n",
        "#  |-- participantHandlingEndTime: string (nullable = true)\n",
        "#  |-- participantHold: string (nullable = true)\n",
        "#  |-- participantHoldDuration: string (nullable = true)\n",
        "#  |-- participantId: string (nullable = true)\n",
        "#  |-- participantLongestHoldDuration: string (nullable = true)\n",
        "#  |-- participantName: string (nullable = true)\n",
        "#  |-- participantOfferAction: string (nullable = true)\n",
        "#  |-- participantOfferActionTime: string (nullable = true)\n",
        "#  |-- participantOfferDuration: string (nullable = true)\n",
        "#  |-- participantOfferTime: string (nullable = true)\n",
        "#  |-- participantProcessingDuration: string (nullable = true)\n",
        "#  |-- participantType: string (nullable = true)\n",
        "#  |-- participantWrapUpDuration: string (nullable = true)\n",
        "#  |-- participantWrapUpEndTime: string (nullable = true)\n",
        "#  |-- queueId: string (nullable = true)\n",
        "#  |-- queueName: string (nullable = true)\n",
        "#  |-- queueTime: string (nullable = true)\n",
        "#  |-- queueWaitDuration: string (nullable = true)\n",
        "#  |-- recordId: string (nullable = true)\n",
        "#  |-- transactionId: string (nullable = true)\n",
        "#  |-- warmTransfersCompleted: string (nullable = true)\n",
        "#  |-- wrapUpCode: string (nullable = true)\n",
        "#  |-- wrapUpCodeId: string (nullable = true)\n",
        "#  |-- wrapUpCodeList: string (nullable = true)\n",
        "#  |-- wrapUpCodeListId: string (nullable = true)\n",
        "#  |-- wrapUpCodeText: string (nullable = true)\n",
        "#  |-- wrapUpShortCode: string (nullable = true)\n",
        "#  |-- loadDateTime: timestamp (nullable = true)\n",
        "#  |-- participantMute: string (nullable = true)\n",
        "#  |-- participantMuteDuration: string (nullable = true)\n",
        "#  |-- participantLongestMuteDuration: string (nullable = true)\n",
        "#  |-- scheduleHours: string (nullable = true)\n",
        "#  |-- TimeToAbandon: string (nullable = true)\n",
        "#  |-- Transfers: string (nullable = true)\n",
        "#  |-- interactionDuration: string (nullable = true)\n",
        "#  |-- loadDate: timestamp (nullable = true)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### The actual JOIN processing - a bit complex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Join new survey data (df_spark) with historical analytics (df_hist_analytics - interactions)\n",
        "# Important: We handle the case when the join columns in pcs (df_spark) have a list of Ids for interactionIds of transactionIds\n",
        "# by picking the first one in the list\n",
        "# Note that the lists of these Ids are comma separated, but also have a space.  For example: \"765345, 263547\"\n",
        "\n",
        "print(\"Building df_final to add to our pcs/pcs delta table...\")\n",
        "\n",
        "# --- 1) Parse duration-like JSON strings in interactions -> string ms values ---\n",
        "# Example duration column value: \"{'value': 26990, 'ongoing': False}\"\n",
        "dur_schema = T.StructType([\n",
        "    T.StructField(\"value\", T.LongType(), True),\n",
        "    T.StructField(\"ongoing\", T.BooleanType(), True),\n",
        "])\n",
        "\n",
        "duration_cols = {\n",
        "    \"participantHandlingDuration\": \"agentCallHandlingDuration\",\n",
        "    \"participantHoldDuration\": \"holdDuration\",\n",
        "    \"participantMuteDuration\": \"muteDuration\",\n",
        "    \"ivrTreatmentDuration\": \"timeInIVR\",\n",
        "    \"queueWaitDuration\": \"waitTime\",\n",
        "    \"interactionDuration\": \"callDuration\",\n",
        "}\n",
        "\n",
        "df_hist_norm = df_hist_analytics\n",
        "for src_col, out_name in duration_cols.items():\n",
        "    src_str = F.col(src_col).cast(\"string\")\n",
        "\n",
        "    # Clean Python-like dict text into valid JSON (handles None/True/False and single quotes)\n",
        "    cleaned = F.regexp_replace(src_str, r\"'\", '\"')\n",
        "    cleaned = F.regexp_replace(cleaned, r\"\\bTrue\\b\", \"true\")\n",
        "    cleaned = F.regexp_replace(cleaned, r\"\\bFalse\\b\", \"false\")\n",
        "    cleaned = F.regexp_replace(cleaned, r\"\\bNone\\b\", \"null\")\n",
        "    cleaned = F.when(F.length(cleaned) == 0, F.lit(None)).otherwise(cleaned)\n",
        "\n",
        "    # Parse JSON; if it parses, take .value; else fall back to numeric cast of original string\n",
        "    parsed_value = F.from_json(cleaned, dur_schema).getField(\"value\")\n",
        "    value_long = F.when(parsed_value.isNotNull(), parsed_value) \\\n",
        "                  .otherwise(src_str.cast(\"long\"))\n",
        "\n",
        "    # Final as STRING (ms) to match legacy schema\n",
        "    df_hist_norm = df_hist_norm.withColumn(out_name, value_long.cast(\"string\"))\n",
        "\n",
        "df_hist_pick = df_hist_norm.select(\n",
        "    F.col(\"interactionId\").cast(\"string\").alias(\"interactionId_key\"),\n",
        "    F.col(\"transactionId\").cast(\"string\").alias(\"transactionId_key\"),\n",
        "    *[F.col(v) for v in duration_cols.values()]\n",
        ")\n",
        "\n",
        "#df_hist_pick.printSchema()\n",
        "#display(df_hist_pick.limit(100))\n",
        "\n",
        "# THIS CODE WORKS\n",
        "\n",
        "# --- 2) Helper to extract the first ID from a comma-separated / bracketed / quoted string ---\n",
        "def first_from_csv_col(col):\n",
        "    # Remove wrapping brackets and quotes, then split on commas, then take first token\n",
        "    cleaned = F.regexp_replace(col, r'^\\[|\\]$|\\\"', '')            # strip [ ], \"\n",
        "    parts   = F.split(cleaned, r'\\s*,\\s*')                        # split on commas with optional spaces\n",
        "    return F.element_at(parts, 1)                                 # 1-based index; null-safe if empty\n",
        "\n",
        "# Normalize df_spark keys for join\n",
        "df_s_norm = (\n",
        "    df_spark\n",
        "    .withColumn(\"interactionId_key\", first_from_csv_col(F.col(\"interactionIds\")))\n",
        "    .withColumn(\"transactionId_key\", first_from_csv_col(F.col(\"transactionIds\")))\n",
        ")\n",
        "\n",
        "#display(df_s_norm.limit(100))\n",
        "\n",
        "# --- 3) Join and select with correct legacy names & types ---\n",
        "# NOTE: New API startTime is formatted like: \"2025-08-28T19:37:05.696-04:00\"\n",
        "#   We need it to be formatted like the old API: \"08/29/2024 17:17:49\" for down stream processing.\n",
        "df_joined = (\n",
        "    df_s_norm.alias(\"s\")\n",
        "    .join(df_hist_pick.alias(\"h\"),\n",
        "          on=F.col(\"s.interactionId_key\") == F.col(\"h.interactionId_key\"),\n",
        "          how=\"left\")\n",
        "    .select(\n",
        "        # Legacy names\n",
        "        F.col(\"s.interactionId_key\").alias(\"callId\"),\n",
        "        # OLD CODE - Keeps raw UTC timezone\n",
        "        # F.date_format(\n",
        "        #     F.to_timestamp(\"s.startTime\", \"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\"),\n",
        "        #     \"MM/dd/yyyy HH:mm:ss\"\n",
        "        # ).alias(\"callDate\"),\n",
        "        # NEW CODE converts incoming raw UTC to EST\n",
        "        F.date_format(\n",
        "            F.from_utc_timestamp(\n",
        "                F.to_timestamp(\"s.startTime\", \"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\"),\n",
        "                \"America/New_York\"\n",
        "            ),\n",
        "            \"MM/dd/yyyy HH:mm:ss\"\n",
        "        ).alias(\"callDate\"),\n",
        "        F.col(\"s.callerName\").cast(\"string\").alias(\"callerName\"),\n",
        "        F.col(\"s.callerNumber\").cast(\"string\").alias(\"callerNumber\"),\n",
        "\n",
        "        # Defaults for NULL/None\n",
        "        F.coalesce(F.col(\"s.results_1_answerDigit\").cast(\"string\"), F.lit(\"NA\")).alias(\"question1\"),\n",
        "        F.coalesce(F.col(\"s.results_2_answerDigit\").cast(\"string\"), F.lit(\"NA\")).alias(\"question2\"),\n",
        "\n",
        "        F.col(\"s.actualScore\").cast(\"int\").alias(\"totalScore\"),\n",
        "        F.col(\"s.agentIds\").cast(\"string\").alias(\"agentList\"),\n",
        "        F.col(\"s.queueIds\").cast(\"string\").alias(\"queueList\"),\n",
        "        F.col(\"s.transactionId_key\").alias(\"transactionId\"),\n",
        "\n",
        "        # Durations with defaults\n",
        "        F.coalesce(F.col(\"h.agentCallHandlingDuration\").cast(\"string\"), F.lit(\"0\")).alias(\"agentCallHandlingDuration\"),\n",
        "        F.coalesce(F.col(\"h.holdDuration\").cast(\"string\"), F.lit(\"0.0\")).alias(\"holdDuration\"),\n",
        "        F.coalesce(F.col(\"h.muteDuration\").cast(\"int\"), F.lit(0)).alias(\"muteDuration\"),\n",
        "        F.coalesce(F.col(\"h.timeInIVR\").cast(\"string\"), F.lit(\"0\")).alias(\"timeInIVR\"),\n",
        "        F.coalesce(F.col(\"h.waitTime\").cast(\"string\"), F.lit(\"0\")).alias(\"waitTime\"),\n",
        "        F.coalesce(F.col(\"h.callDuration\").cast(\"string\"), F.lit(\"0\")).alias(\"callDuration\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Join with survey df and rename to fit old API call data element names:\n",
        "# AttributeName → MAPPED TO NEW\n",
        "# ----------------------------------------\n",
        "# callId                    → interactionIds\n",
        "# callDate                  → startTime\n",
        "# callerName                → callerName\n",
        "# callerPhoneNumber         → callerNumber\n",
        "# question1                 → results_1_answerDigit\n",
        "# question2                 → results_2_answerDigit\n",
        "# totalScore                → actualScore\n",
        "# agentList                 → agentIds\n",
        "# queueList                 → queueIds\n",
        "# transactionId             → transactionIds\n",
        "# agentCallHandlingDuration → Get from interaction details column w same name\n",
        "# holdDuration              → Get from interaction details column w same name\n",
        "# muteDuration              → Get from interaction details column w same name\n",
        "# timeInIVR                 → Get from interaction details column w same name\n",
        "# waitTime                  → Get from interaction details column w same name\n",
        "# callDuration              → Get from interaction details column w same name\n",
        "\n",
        "# Transform this into the same dataframe structure we have in the old raw pcs/pcs delta table\n",
        "# {hits, data, QuestionLabel, loadDate, loadDateTime}\n",
        "# Gold delta table does NOT include hits, so we MAY not need to figure out how to calculate that.\n",
        "# Gold delta table DOES include QuestionLable, so we will want to figure out some logic for it.\n",
        "# data is our essential payload - need to convert the dataframe column values to a JSON/dict element:\n",
        "#\n",
        "# --{\n",
        "# --    \"callId\": \"int-18fdf8df0d3-aYkEI6o0fvyMwMiP1eGiNTV1S-phone-03-wolverineworldwid01\",\n",
        "# --    \"callDate\": \"06/03/2024 15:22:52\",\n",
        "# --    \"callerName\": \"WIRELESS CALLER\",\n",
        "# --    \"callerPhoneNumber\": \"2222222222\",\n",
        "# --    \"question1\": \"5\",\n",
        "# --    \"question2\": \"5\",\n",
        "# --    \"totalScore\": 10,\n",
        "# --    \"agentList\": \"[agkOrv32ujRCuUOQ3sYjW_Ag]\",\n",
        "# --    \"queueList\": \"[276]\",\n",
        "# --    \"transactionId\": \"178519\",\n",
        "# --    \"agentCallHandlingDuration\": \"312815.0\",\n",
        "# --    \"holdDuration\": \"0.0\",\n",
        "# --    \"muteDuration\": 0,\n",
        "# --    \"timeInIVR\": \"105709.0\",\n",
        "# --    \"waitTime\": \"41592.0\",\n",
        "# --    \"callDuration\": \"461309.0\"\n",
        "# --}\n",
        "\n",
        "df_final = (\n",
        "    df_joined.select(\n",
        "        F.lit(0).cast(\"int\").alias(\"hits\"),\n",
        "        F.struct(\n",
        "            F.col(\"callId\").cast(\"string\").alias(\"callId\"),\n",
        "            F.col(\"callDate\").cast(\"string\").alias(\"callDate\"),\n",
        "            F.col(\"callerName\").cast(\"string\").alias(\"callerName\"),\n",
        "            F.col(\"callerNumber\").cast(\"string\").alias(\"callerPhoneNumber\"),  # <-- fix name\n",
        "            F.col(\"question1\").cast(\"string\").alias(\"question1\"),\n",
        "            F.col(\"question2\").cast(\"string\").alias(\"question2\"),\n",
        "            F.col(\"totalScore\").cast(\"int\").alias(\"totalScore\"),\n",
        "            F.col(\"agentList\").cast(\"string\").alias(\"agentList\"),\n",
        "            F.col(\"queueList\").cast(\"string\").alias(\"queueList\"),\n",
        "            F.col(\"transactionId\").cast(\"string\").alias(\"transactionId\"),\n",
        "            F.col(\"agentCallHandlingDuration\").cast(\"string\").alias(\"agentCallHandlingDuration\"),\n",
        "            F.col(\"holdDuration\").cast(\"string\").alias(\"holdDuration\"),\n",
        "            F.col(\"muteDuration\").cast(\"int\").alias(\"muteDuration\"),\n",
        "            F.col(\"timeInIVR\").cast(\"string\").alias(\"timeInIVR\"),\n",
        "            F.col(\"waitTime\").cast(\"string\").alias(\"waitTime\"),\n",
        "            F.col(\"callDuration\").cast(\"string\").alias(\"callDuration\"),\n",
        "        ).alias(\"data\"),\n",
        "        F.lit(\"{Q1=Satisfaction, Q2=Resolution}\").alias(\"QuestionLabel\"),\n",
        "        F.current_date().alias(\"loadDate\"),\n",
        "        F.current_timestamp().alias(\"loadDateTime\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Done Building df_final to add to our pcs/pcs delta table.\")\n",
        "\n",
        "#df_final.printSchema()\n",
        "# root\n",
        "#  |-- hits: integer (nullable = false)\n",
        "#  |-- data: struct (nullable = false)\n",
        "#  |    |-- callId: string (nullable = true)\n",
        "#  |    |-- callDate: string (nullable = true)\n",
        "#  |    |-- callerName: string (nullable = true)\n",
        "#  |    |-- callerPhoneNumber: string (nullable = true)\n",
        "#  |    |-- question1: string (nullable = false)\n",
        "#  |    |-- question2: string (nullable = false)\n",
        "#  |    |-- totalScore: integer (nullable = true)\n",
        "#  |    |-- agentList: string (nullable = true)\n",
        "#  |    |-- queueList: string (nullable = true)\n",
        "#  |    |-- transactionId: string (nullable = true)\n",
        "#  |    |-- agentCallHandlingDuration: string (nullable = false)\n",
        "#  |    |-- holdDuration: string (nullable = false)\n",
        "#  |    |-- muteDuration: integer (nullable = false)\n",
        "#  |    |-- timeInIVR: string (nullable = false)\n",
        "#  |    |-- waitTime: string (nullable = false)\n",
        "#  |    |-- callDuration: string (nullable = false)\n",
        "#  |-- QuestionLabel: string (nullable = false)\n",
        "#  |-- loadDate: date (nullable = false)\n",
        "#  |-- loadDateTime: timestamp (nullable = false)\n",
        "\n",
        "#display(df_final.limit(100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "#display(df_joined.limit(20))\n",
        "\n",
        "# Min and MAx callDate:\n",
        "# df_joined.select(\n",
        "#     F.min(F.to_timestamp(\"callDate\", \"MM/dd/yyyy HH:mm:ss\")).alias(\"min_callDate\"),\n",
        "#     F.max(F.to_timestamp(\"callDate\", \"MM/dd/yyyy HH:mm:ss\")).alias(\"max_callDate\")\n",
        "# ).show(truncate=False)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Append to the raw pcs/pcs delta table.  Consider duplicate loadDate handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Confirm our existing delta table schema - ah - data is not a string!  It's a struct!\n",
        "# pcs_raw_delta_path = f\"{raw_adls_path}/pcs/pcs\"\n",
        "# df_pcs = (\n",
        "#     spark\n",
        "#     .read\n",
        "#     .format(\"delta\")\n",
        "#     .load(pcs_raw_delta_path)\n",
        "# )\n",
        "# df_pcs.printSchema()\n",
        "# root\n",
        "#  |-- hits: integer (nullable = true)\n",
        "#  |-- data: struct (nullable = true)\n",
        "#  |    |-- callId: string (nullable = true)\n",
        "#  |    |-- callDate: string (nullable = true)\n",
        "#  |    |-- callerName: string (nullable = true)\n",
        "#  |    |-- callerPhoneNumber: string (nullable = true)\n",
        "#  |    |-- question1: string (nullable = true)\n",
        "#  |    |-- question2: string (nullable = true)\n",
        "#  |    |-- totalScore: integer (nullable = true)\n",
        "#  |    |-- agentList: string (nullable = true)\n",
        "#  |    |-- queueList: string (nullable = true)\n",
        "#  |    |-- transactionId: string (nullable = true)\n",
        "#  |    |-- agentCallHandlingDuration: string (nullable = true)\n",
        "#  |    |-- holdDuration: string (nullable = true)\n",
        "#  |    |-- muteDuration: integer (nullable = true)\n",
        "#  |    |-- timeInIVR: string (nullable = true)\n",
        "#  |    |-- waitTime: string (nullable = true)\n",
        "#  |    |-- callDuration: string (nullable = true)\n",
        "#  |-- QuestionLabel: string (nullable = true)\n",
        "#  |-- loadDate: date (nullable = true)\n",
        "#  |-- loadDateTime: timestamp (nullable = true)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "outputs": [],
      "metadata": {},
      "source": [
        "pcs_raw_delta_path = f\"{raw_adls_path}/pcs/pcs\"\n",
        "\n",
        "# TODO: What if we've already loaded this date?\n",
        "#   We have some manual code below.  Leaving it out of here for now to make regular daily runs more efficient.\n",
        "\n",
        "# Append df_final to the pcs/pcs delta table\n",
        "print(\"Appending df_final to the raw pcs/pcs delta table...\")\n",
        "(\n",
        "    df_final\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"append\")\n",
        "    .save(pcs_raw_delta_path)\n",
        ")\n",
        "print(\"Done Appending df_final to the raw pcs/pcs delta table.\")\n",
        "print(\"DONE.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# EXIT normal processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Exit normal processing\n",
        "mssparkutils.notebook.exit(\"0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Manual Code - consider adding to daily load code above.  For now, leave it manual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Delete raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Delete todays data (or any date's data) so we don't duplicate data\n",
        "# NOTE: Will need to activate PIM to do this when executing as your user account.\n",
        "# loadDate = \"yyyy-MM-dd\"\n",
        "today_str = date.today().strftime(\"%Y-%m-%d\")\n",
        "print(f\"today_str {today_str}\")\n",
        "\n",
        "# Change this as needed  # TMP: 9/3, 8/29\n",
        "dateToDelete = \"2025-09-11\"\n",
        "#dateToDelete = today_str\n",
        "print(f\"dateToDelete: {dateToDelete}\")\n",
        "\n",
        "pcs_raw_delta_path = f\"{raw_adls_path}/pcs/pcs\"\n",
        "\n",
        "spark.sql(f\"\"\"\n",
        "DELETE FROM delta.`{pcs_raw_delta_path}`\n",
        "WHERE loadDate = '{dateToDelete}'\n",
        "\"\"\")\n",
        "print(f\"Deleted raw pcs/pcs data for loadDate '{dateToDelete}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Delete gold data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Manual code to delete our recent historical load from 8/20-current\n",
        "# # GOLD\n",
        "# so we can reload it.\n",
        "\n",
        "# Change this as needed  # TMP: 9/3, 8/29\n",
        "dateToDelete = \"2025-09-11\"\n",
        "#dateToDelete = today_str\n",
        "print(f\"dateToDelete: {dateToDelete}\")\n",
        "\n",
        "pcs_gold_delta_path = f\"{gold_adls_path}cco/pcs\"\n",
        "\n",
        "spark.sql(f\"\"\"\n",
        "DELETE FROM delta.`{pcs_gold_delta_path}`\n",
        "WHERE loadDate = '{dateToDelete}'\n",
        "\"\"\")\n",
        "print(f\"Deleted raw pcs/pcs data for loadDate '{dateToDelete}'\")\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Clean up the small file problem in raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "pcs_raw_delta_path = f\"{raw_adls_path}/pcs/pcs\"\n",
        "print(f\"Cleaning up the many small file issue in raw.pcs {pcs_raw_delta_path}\")\n",
        "# Read current table\n",
        "df = spark.read.format(\"delta\").load(pcs_raw_delta_path)\n",
        "\n",
        "# Pick a small, sane number of output files for ~690k rows (e.g., 8–16)\n",
        "df.coalesce(12) \\\n",
        "  .write.format(\"delta\") \\\n",
        "  .mode(\"overwrite\") \\\n",
        "  .option(\"overwriteSchema\", \"true\") \\\n",
        "  .save(pcs_raw_delta_path)\n",
        "\n",
        "print(f\"Done - Cleaning up the many small file issue in raw.pcs {pcs_raw_delta_path}\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Vacuum the old files after coalescing to 12 (or whatever) files for raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from delta.tables import DeltaTable\n",
        "print(f\"Vaccuming {pcs_raw_delta_path}\")\n",
        "DeltaTable.forPath(spark, pcs_raw_delta_path).vacuum()   # default retention (~7 days)\n",
        "print(f\"Done - Vaccuming {pcs_raw_delta_path}\")"
      ]
    }
  ],
  "metadata": {
    "description": "In August of 2025, 8x8 required us to migrate to a new API.  This notebook ingests data from that API.",
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}