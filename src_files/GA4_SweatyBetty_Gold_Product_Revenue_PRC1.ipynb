{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# GA4 Sweaty Betty Gold Layer Processing <br>\n",
        "## Product Revenue<br>\n",
        "This notebook reads raw data and processes it for Product Revenue Table\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import concurrent.futures\n",
        "from delta import *\n",
        "from pyspark.sql.types import StructType, StructField, ArrayType, StringType, LongType, DoubleType, BooleanType, MapType,IntegerType\n",
        "from pyspark.sql.functions import *\n",
        "from functools import reduce\n",
        "from pyspark.sql.dataframe import DataFrame\n",
        "import pyspark.sql.functions as F\n",
        "import json\n",
        "import base64\n",
        "from datetime import datetime,timedelta\n",
        "from time import sleep\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"DYNAMIC\")\n",
        "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from pyspark.sql.functions import max as spark_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "account_name = raw_adls_path.split('@')[1].split('.')[0]\n",
        "json_blob_path =f\"{raw_adls_path}/GA4_SweatyBetty/bigquery_datasets_tables.json\"\n",
        "#base_folder = \"GA4_SweatyBetty\"\n",
        "base_folder = \"GA4_SweatyBetty/analytics_292120381/events\"\n",
        "gold_container = 'gold'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Product Revenue\n",
        "product_revenue_target_folder = '/GA4-SweatyBetty/product_revenue/'\n",
        "product_revenue_delta_table_path = f\"abfss://{gold_container}@{account_name}.dfs.core.windows.net/{product_revenue_target_folder}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Product Revenue\n",
        "print(f\"Product Revenue Target Folder: {product_revenue_target_folder}\")\n",
        "print(f\"Product Revenue Delta Table Path: {product_revenue_delta_table_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Variables for Currency Exchange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "silver_container = 'silver'\n",
        "currency_exchange_source_folder = '/SAP/BW/FxRates/'\n",
        "currency_exchange_delta_table_path = f\"abfss://{silver_container}@{account_name}.dfs.core.windows.net/{currency_exchange_source_folder}\"\n",
        "print(currency_exchange_delta_table_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Read the Currency Delta table from the specified path\n",
        "df_currency_exchange = spark.read.format(\"delta\").load(currency_exchange_delta_table_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Step 1: Truncate CalendarDay to month\n",
        "df_monthly_avg = (\n",
        "    df_currency_exchange\n",
        "    .withColumn(\"month_start\", trunc(\"CalendarDay\", \"month\"))\n",
        "    .groupBy(\"FromCurrency\", \"ToCurrency\", \"month_start\")\n",
        "    .agg(avg(\"FxRate\").alias(\"exchange_rate_amount\"))\n",
        ")\n",
        "\n",
        "# Step 2: Add valid_from and valid_to\n",
        "transformed_exchange_df = (\n",
        "    df_monthly_avg\n",
        "    .withColumn(\"valid_from\", col(\"month_start\"))\n",
        "    .withColumn(\"valid_to\", add_months(col(\"month_start\"), 1))\n",
        "    .withColumnRenamed(\"FromCurrency\", \"base_currency_code\")\n",
        "    .withColumnRenamed(\"ToCurrency\", \"target_currency_code\")\n",
        "    .select(\n",
        "        \"base_currency_code\",\n",
        "        \"target_currency_code\",\n",
        "        \"exchange_rate_amount\",\n",
        "        \"valid_from\",\n",
        "        \"valid_to\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Optional: filter for GBP only\n",
        "transformed_exchange_df = transformed_exchange_df.filter(col(\"target_currency_code\") == \"GBP\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "exchange_rate_df = transformed_exchange_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import col, md5, concat_ws, coalesce, lit, from_unixtime, to_date, explode_outer, row_number, min as min_, when\n",
        "from pyspark.sql.window import Window\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# 1. Add session_hkey\n",
        "def add_session_hkey(df):\n",
        "    return df.withColumn(\n",
        "        \"session_hkey\",\n",
        "        md5(concat_ws(\"-\",\n",
        "            coalesce(col(\"user_pseudo_id\").cast(\"string\"), lit(\"\")),\n",
        "            coalesce(col(\"event_params.ga_session_id.int_value\").cast(\"string\"), lit(\"\")),\n",
        "            coalesce(col(\"event_params.ga_session_number.int_value\").cast(\"string\"), lit(\"\"))\n",
        "        ))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# 2. Explode items array\n",
        "\n",
        "def explode_items(df):\n",
        "    return df.withColumn(\"item\", explode_outer(\"items\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#This function avoids duplicates, but product_index does not match\n",
        "#3 compute_product_metrics with Unique\n",
        "\n",
        "from pyspark.sql.functions import col, from_unixtime, min as min_, row_number, date_format, sha2, concat_ws\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "def compute_product_metrics(df):\n",
        "    # Filter out records where product_sku (item.item_id) is null\n",
        "    df = df.filter(col(\"item.item_id\").isNotNull())\n",
        "\n",
        "    # Add event_timestamp_ts\n",
        "    df = df.withColumn(\"event_timestamp_ts\", from_unixtime(col(\"event_timestamp\") / 1000000).cast(\"timestamp\"))\n",
        "\n",
        "    # Add visit_key\n",
        "    df = df.withColumn(\"visit_key\", col(\"event_params.ga_session_id.int_value\"))\n",
        "\n",
        "    # Compute visit_start_time\n",
        "    window_vs = Window.partitionBy(\"visit_key\")\n",
        "    df = df.withColumn(\"visit_start_time\", min_(\"event_timestamp_ts\").over(window_vs))\n",
        "    df = df.withColumn(\"visit_start_time\", date_format(col(\"visit_start_time\"), \"yyyy-MM-dd HH:mm:ss.SSS\").cast(\"timestamp\"))\n",
        "\n",
        "    # Compute hit_number\n",
        "    window_hit = Window.partitionBy(\"user_pseudo_id\", \"visit_key\").orderBy(\"event_timestamp\")\n",
        "    df = df.withColumn(\"hit_number\", row_number().over(window_hit))\n",
        "\n",
        "    # New product_index logic\n",
        "    window_index = Window.partitionBy(\"event_timestamp_ts\") \\\n",
        "                         .orderBy(col(\"event_timestamp_ts\").asc(),\n",
        "                                  col(\"item.item_id\").asc(),\n",
        "                                  col(\"event_params.engagement_time_msec.int_value\").cast(\"int\").asc())\n",
        "    df = df.withColumn(\"product_index\", row_number().over(window_index))\n",
        "\n",
        "    # Generate event_product_unique_key and event_unique_key\n",
        "    df = df.withColumn(\"event_product_unique_key\", concat_ws(\"_\", col(\"event_name\"), col(\"item.item_id\")))\n",
        "    df = df.withColumn(\"event_unique_key\", col(\"event_params.event_id.string_value\"))\n",
        "\n",
        "    # Generate unique_key (equivalent to dbt_utils.generate_surrogate_key)\n",
        "    df = df.withColumn(\"unique_key\", sha2(\n",
        "        concat_ws(\"||\",\n",
        "            col(\"session_hkey\"),\n",
        "            col(\"user_pseudo_id\"),\n",
        "            col(\"visit_start_time\").cast(\"string\"),\n",
        "            col(\"event_product_unique_key\"),\n",
        "            col(\"event_unique_key\")\n",
        "        ), 256\n",
        "    ))\n",
        "\n",
        "    # Select required columns\n",
        "    return df.select(\n",
        "        \"event_timestamp\",\n",
        "        \"user_pseudo_id\",\n",
        "        \"session_hkey\",\n",
        "        \"user_id\",\n",
        "        \"visit_key\",\n",
        "        \"visit_start_time\",\n",
        "        \"hit_number\",\n",
        "        col(\"event_name\").alias(\"ecommerceaction_actiontype\"),\n",
        "        col(\"ecommerce.transaction_id\").alias(\"transaction_id\"),\n",
        "        col(\"geo.country\").alias(\"country\"),\n",
        "        \"product_index\",\n",
        "        col(\"item.item_id\").alias(\"product_sku\"),\n",
        "        col(\"item.quantity\").alias(\"product_quantity\"),\n",
        "        col(\"event_params.currency.string_value\").alias(\"transaction_currency\"),\n",
        "        col(\"item.item_revenue\").alias(\"local_product_revenue\"),\n",
        "        col(\"item.item_revenue_in_usd\").alias(\"product_revenue_usd\"),\n",
        "        col(\"event_timestamp_ts\"),\n",
        "        \"unique_key\"\n",
        "    )\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# 4. Join with exchange rate to compute product_revenue_gbp\n",
        "def join_exchange_rates(df, exchange_rate_df):\n",
        "    df = df.join(\n",
        "        exchange_rate_df,\n",
        "        (df.transaction_currency == exchange_rate_df.base_currency_code) &\n",
        "        (df.visit_start_time.between(exchange_rate_df.valid_from, exchange_rate_df.valid_to)) &\n",
        "        (exchange_rate_df.target_currency_code == \"GBP\"),\n",
        "        how=\"left\"\n",
        "    ).withColumn(\n",
        "        \"product_revenue_gbp\",\n",
        "        when(col(\"transaction_currency\") == \"GBP\", col(\"local_product_revenue\"))\n",
        "        .otherwise(col(\"local_product_revenue\") * coalesce(col(\"exchange_rate_amount\"), lit(1)))\n",
        "\n",
        "    )\n",
        "    #df.printSchema()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "from pyspark.sql.functions import md5, concat_ws, coalesce, col, lit\n",
        "\n",
        "def compute_unique_key(df):\n",
        "    return df.withColumn(\n",
        "        \"unique_key\",\n",
        "        md5(concat_ws(\"-\",\n",
        "            coalesce(col(\"session_hkey\").cast(\"string\"), lit(\"\")),\n",
        "            coalesce(col(\"visit_key\").cast(\"string\"), lit(\"\")),\n",
        "            coalesce(col(\"ecommerceaction_actiontype\").cast(\"string\"), lit(\"\")),\n",
        "            coalesce(col(\"event_timestamp_ts\").cast(\"string\"), lit(\"\")),\n",
        "            coalesce(col(\"hit_number\").cast(\"string\"), lit(\"\"))\n",
        "        ))\n",
        "    )\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# 6. Apply incremental filter if needed\n",
        "def filter_incremental(df, last_loaded_datetime):\n",
        "    return df.filter(col(\"load_datetime\") > lit(last_loaded_datetime))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# 7. Full transformation pipeline\n",
        "def transform_product_revenue(df_events, exchange_rate_df):\n",
        "    df = add_session_hkey(df_events)\n",
        "    df = explode_items(df)\n",
        "    df = compute_product_metrics(df)\n",
        "    df = join_exchange_rates(df, exchange_rate_df)\n",
        "    df = compute_unique_key(df)\n",
        "    #df.printSchema()\n",
        "    return df\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Function to Save to Delta\n",
        "Hit number is non deterministic, this code can give duplicate records later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Save and Merge Function - New Logic\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "def save_to_delta_merge(df, spark, delta_table_path: str):\n",
        "    \"\"\"\n",
        "    Save the given DataFrame to a Delta table using merge on 'unique_key'.\n",
        "    If the table exists, update matching rows and insert new ones.\n",
        "    If it doesn't exist, create it.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame to save\n",
        "    - spark: SparkSession\n",
        "    - delta_table_path: path to the Delta table\n",
        "    \"\"\"\n",
        "    if DeltaTable.isDeltaTable(spark, delta_table_path):\n",
        "        delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
        "\n",
        "        # Merge using unique_key\n",
        "        merge_condition = \"target.unique_key = source.unique_key\"\n",
        "\n",
        "        delta_table.alias(\"target\").merge(\n",
        "            source=df.alias(\"source\"),\n",
        "            condition=merge_condition\n",
        "        ).whenMatchedUpdateAll() \\\n",
        "         .whenNotMatchedInsertAll() \\\n",
        "         .execute()\n",
        "    else:\n",
        "        # If the table doesn't exist, write the full DataFrame\n",
        "        df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Process a Date Range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Process data for Day before Yesterday or a Date Range\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def process_and_save_product_revenue_date_range(\n",
        "    raw_adls_path: str,\n",
        "    base_folder: str,\n",
        "    delta_base_path: str,\n",
        "    exchange_rate_df,\n",
        "    spark,\n",
        "    start_date: str = None,\n",
        "    end_date: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Process and save product revenue tables over a date range.\n",
        "\n",
        "    Parameters:\n",
        "    - raw_adls_path: Base path to raw data in ADLS\n",
        "    - base_folder: Subfolder for event tables\n",
        "    - delta_base_path: Path to write delta tables\n",
        "    - exchange_rate_df: Pre-loaded DataFrame with exchange rates\n",
        "    - spark: SparkSession\n",
        "    - start_date: Start of date range (format: YYYYMMDD)\n",
        "    - end_date: End of date range (format: YYYYMMDD)\n",
        "    \"\"\"\n",
        "\n",
        "    if not start_date or not end_date:\n",
        "        # Compute day before yesterday in UTC\n",
        "        day_before_yesterday = (datetime.utcnow() - timedelta(days=2)).strftime(\"%Y%m%d\")\n",
        "        start_date = end_date = day_before_yesterday\n",
        "\n",
        "    start = datetime.strptime(start_date, \"%Y%m%d\")\n",
        "    end = datetime.strptime(end_date, \"%Y%m%d\")\n",
        "\n",
        "    current = start\n",
        "    while current <= end:\n",
        "        date_str = current.strftime(\"%Y%m%d\")\n",
        "        table_name = f\"events_{date_str}\"\n",
        "        print(f\"Processing table: {table_name}\")\n",
        "\n",
        "        try:\n",
        "            full_path = f\"{raw_adls_path}{base_folder}/{table_name}\"\n",
        "            df_events = spark.read.parquet(full_path)\n",
        "\n",
        "            df_processed = transform_product_revenue(df_events, exchange_rate_df)\n",
        "            # df_processed = df_processed.dropDuplicates()\n",
        "\n",
        "            delta_table_path = f\"{delta_base_path}\"\n",
        "\n",
        "            save_to_delta_merge(df_processed, spark, delta_table_path)\n",
        "            print(f\"Saved to {delta_table_path}\")\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {table_name}: {e}\")\n",
        "        \n",
        "        current += timedelta(days=1)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Call Product Revenue for Day before yesterday "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "process_and_save_product_revenue_date_range(\n",
        "    raw_adls_path=raw_adls_path,\n",
        "    base_folder=base_folder,\n",
        "    delta_base_path=product_revenue_delta_table_path,\n",
        "    exchange_rate_df=exchange_rate_df,\n",
        "    spark=spark\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#The following code can be uncommented to process data for a specific date range\n",
        "'''\n",
        "process_and_save_product_revenue_date_range(\n",
        "    raw_adls_path=raw_adls_path,\n",
        "    base_folder=base_folder,\n",
        "    delta_base_path=product_revenue_delta_table_path,\n",
        "    exchange_rate_df=exchange_rate_df,\n",
        "    spark=spark,\n",
        "    start_date=\"20250525\",\n",
        "    end_date=\"20250525\"\n",
        ")'''\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}