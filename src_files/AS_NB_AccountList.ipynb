{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Read new traffic data via API, store in raw, process as Delta table\r\n",
        "import delta #from delta import *\r\n",
        "import pyspark.sql.functions  #from pyspark.sql.functions import *\r\n",
        "import pyspark.sql.types #from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType, LongType\r\n",
        "import datetime\r\n",
        "import requests, json\r\n",
        "import azure.storage.blob #from azure.storage.blob import BlobServiceClient\r\n",
        "import notebookutils\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from py4j.java_gateway import JavaObject"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Includes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run /utils/environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Bronze - Conform to Delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Read the csv into a data frame\r\n",
        "file_path = f'{raw_adls_path}/ExternalData/GL/accountlist1.csv'\r\n",
        "print(file_path)\r\n",
        "\r\n",
        "df = spark.read.option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").option(\"header\", \"true\").csv(file_path,\r\n",
        "    header=True,\r\n",
        "    inferSchema=True\r\n",
        ")\r\n",
        "\r\n",
        "df.printSchema()\r\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Cleaning Column Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "import re\r\n",
        "\r\n",
        "# Get the current column names\r\n",
        "old_columns = df.columns\r\n",
        "\r\n",
        "# Create a list for the new column names\r\n",
        "new_columns = []\r\n",
        "\r\n",
        "# Process each column name\r\n",
        "for column in old_columns:\r\n",
        "    # Replace spaces with underscores and remove special characters (keep letters, numbers, and underscores)\r\n",
        "    clean_name = re.sub(r'[^\\w]', '', column.replace(' ', '_'))\r\n",
        "    \r\n",
        "    # Ensure the name doesn't start with a number (add prefix if needed)\r\n",
        "    if clean_name and clean_name[0].isdigit():\r\n",
        "        clean_name = 'col_' + clean_name\r\n",
        "    \r\n",
        "    # Handle empty string case\r\n",
        "    if not clean_name:\r\n",
        "        clean_name = 'column_' + str(len(new_columns))\r\n",
        "        \r\n",
        "    new_columns.append(clean_name)\r\n",
        "\r\n",
        "# Rename the columns in the dataframe\r\n",
        "df = df.toDF(*new_columns)\r\n",
        "\r\n",
        "# Print the before and after column names for verification\r\n",
        "print(\"Original columns:\", old_columns)\r\n",
        "print(\"Cleaned columns:\", new_columns)\r\n",
        "\r\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Writing data to delta table and saving in Bronze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Write dataframe to bronze\r\n",
        "bronze_delta_table_path = f'{bronze_adls_path}/ExternalData/GL/AccountList'\r\n",
        "print(bronze_delta_table_path)\r\n",
        "\r\n",
        "df.write.option(\"overwriteSchema\", \"true\").mode(\"overwrite\").format(\"delta\").save(bronze_delta_table_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Silver Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Read the existing Delta table from bronze layer\r\n",
        "bronze_path = f\"{bronze_adls_path}/ExternalData/GL/AccountList\"\r\n",
        "bronze_df = spark.read.format(\"delta\").load(bronze_path)\r\n",
        "\r\n",
        "# Display the schema to confirm current data types\r\n",
        "print(\"Bronze schema:\")\r\n",
        "bronze_df.printSchema()\r\n",
        "\r\n",
        "# Import necessary functions\r\n",
        "from pyspark.sql.functions import col\r\n",
        "\r\n",
        "# Transform data - convert specific columns from string to integer\r\n",
        "silver_df = bronze_df # bronze_df.drop(\"_c8\")\r\n",
        "\r\n",
        "# Display the new schema to confirm the type conversions\r\n",
        "print(\"Silver schema:\")\r\n",
        "silver_df.printSchema()\r\n",
        "\r\n",
        "# Define the silver layer path\r\n",
        "silver_path = f\"{silver_adls_path}/ExternalData/GL/AccountList\"\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Writing to Silver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {},
      "source": [
        "\r\n",
        "# Write the transformed data to the silver layer as a Delta table\r\n",
        "silver_df.write.option(\"overwriteSchema\", \"true\") \\\r\n",
        "    .format(\"delta\") \\\r\n",
        "    .mode(\"overwrite\") \\\r\n",
        "    .save(silver_path)\r\n",
        "\r\n",
        "print(f\"Silver Delta table successfully written to: {silver_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {},
      "source": [
        "notebookutils.mssparkutils.notebook.exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# schema = pyspark.sql.types.StructType([\r\n",
        "#     pyspark.sql.types.StructField('Key', pyspark.sql.types.StringType(), True),\r\n",
        "#     pyspark.sql.types.StructField('Global_GL_Account', pyspark.sql.types.StringType(), True),\r\n",
        "#     pyspark.sql.types.StructField('WWW_P&L', pyspark.sql.types.StringType(), True),\r\n",
        "#     pyspark.sql.types.StructField('CorporateSortOrder', pyspark.sql.types.StringType(), True),\r\n",
        "#     pyspark.sql.types.StructField('CorporateFormat_String', pyspark.sql.types.LongType(), True),\r\n",
        "#     pyspark.sql.types.StructField('Retail_P&L', pyspark.sql.types.LongType(), True),\r\n",
        "#     pyspark.sql.types.StructField('RetailFormatString', pyspark.sql.types.StringType(), True),\r\n",
        "#     pyspark.sql.types.StructField('RetailSortOrder', pyspark.sql.types.LongType(), True),\r\n",
        "#     pyspark.sql.types.StructField('Sweaty_Betty_P&L', pyspark.sql.types.StringType(), True),\r\n",
        "#     pyspark.sql.types.StructField('SweatyBettyFormat', pyspark.sql.types.StringType(), True),\r\n",
        "#     pyspark.sql.types.StructField('SweatyBettyOrder', pyspark.sql.types.LongType(), True),\r\n",
        "#     pyspark.sql.types.StructField('Other_SB_P&L', pyspark.sql.types.LongType(), True),\r\n",
        "#     pyspark.sql.types.StructField('OtherSPFormat', pyspark.sql.types.StringType(), True),\r\n",
        "#     pyspark.sql.types.StructField('OtherSPOrder', pyspark.sql.types.LongType(), True)\r\n",
        "# ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import col\r\n",
        "\r\n",
        "# Filter the rows based on specific values in column 1 and select columns 1, 4, and 8\r\n",
        "silver_df.filter(col(df.columns[0]).isin('0000700276_B', '0000700276_D', '0000700276_V', '0000700276_W')) \\\r\n",
        "  .select(df.columns[0], df.columns[3], df.columns[7]) \\\r\n",
        "  .show()\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Check for folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Function to tell us if a folder exists\r\n",
        "def folder_exists(abfss_path):\r\n",
        "    from notebookutils import mssparkutils\r\n",
        "    try:\r\n",
        "        mssparkutils.fs.ls(abfss_path)\r\n",
        "        return True\r\n",
        "    except:\r\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Function to create a folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Creating a folder:\r\n",
        "mssparkutils.fs.mkdirs(f'{raw_adls_path}/ExternalData/GL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Creating remaining folders for medallion architecture for Dev:\r\n",
        "mssparkutils.fs.mkdirs(f'{bronze_adls_path}/ExternalData/GL')\r\n",
        "mssparkutils.fs.mkdirs(f'{silver_adls_path}/ExternalData/GL')\r\n",
        "mssparkutils.fs.mkdirs(f'{gold_adls_path}/ExternalData/GL')\r\n",
        "#silver and gold do not have ExternalData folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Creating raw GL folder in ExternalData for Test\r\n",
        "mssparkutils.fs.mkdirs('abfss://raw@azwwwnonprodtestadapadls.dfs.core.windows.net/ExternalData/GL')\r\n",
        "mssparkutils.fs.mkdirs('abfss://bronze@azwwwnonprodtestadapadls.dfs.core.windows.net/ExternalData/GL')\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Creating raw and bronze GL folders in ExternalData for Prod\r\n",
        "mssparkutils.fs.mkdirs('abfss://raw@azwwwprodprdadapadls.dfs.core.windows.net/ExternalData/GL')\r\n",
        "mssparkutils.fs.mkdirs('abfss://bronze@azwwwprodprdadapadls.dfs.core.windows.net/ExternalData/GL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Query of Silver Delta Table\r\n",
        "df = spark.read.format(\"delta\").load(\"abfss://silver@azwwwnonproddevadapadls.dfs.core.windows.net/ExternalData/GL/AccountList\")\r\n",
        "df.createOrReplaceTempView(\"delta_table\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import col\r\n",
        "\r\n",
        "# Filter the rows based on specific values in column 1 and select columns 1, 4, and 8\r\n",
        "bronze_df.show()\r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}