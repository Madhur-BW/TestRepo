{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# GA4 Sweaty Betty Process from Raw to Gold<br>\r\n",
        "## Table 1 & Table 2\r\n",
        "\r\n",
        "\r\n",
        "**Revision History**<br>\r\n",
        "Created 2/27/2025 Vish<br>\r\n",
        "Adding Gold layer 03/25/2025 Vish\r\n",
        "\r\n",
        "This notebook processes two tables from Raw to Gold.\r\n",
        "These tables are Session Hits and Sessions Tables\r\n",
        "\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "import concurrent.futures\r\n",
        "from delta import *\r\n",
        "from pyspark.sql.types import StructType, StructField, ArrayType, StringType, LongType, DoubleType, BooleanType, MapType,IntegerType\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from functools import reduce\r\n",
        "from pyspark.sql.dataframe import DataFrame\r\n",
        "import pyspark.sql.functions as F\r\n",
        "import json\r\n",
        "import base64\r\n",
        "from datetime import datetime,timedelta\r\n",
        "from time import sleep\r\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"DYNAMIC\")\r\n",
        "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\r\n",
        "from azure.storage.blob import BlobServiceClient\r\n",
        "from pyspark.sql.functions import max as spark_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "!pip install google-cloud-bigquery\r\n",
        "!pip install google-auth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "from google.cloud import bigquery\r\n",
        "from google.oauth2 import service_account"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Run the common functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {},
      "source": [
        "account_name = raw_adls_path.split('@')[1].split('.')[0]\r\n",
        "json_blob_path =f\"{raw_adls_path}/GA4_SweatyBetty/bigquery_datasets_tables.json\"\r\n",
        "#base_folder = \"GA4_SweatyBetty\"\r\n",
        "base_folder = \"GA4_SweatyBetty/analytics_292120381/events\"\r\n",
        "events_fresh_base_folder = \"GA4_SweatyBetty/analytics_292120381/events_fresh\"\r\n",
        "events_intraday_base_folder = \"GA4_SweatyBetty/analytics_292120381/events_intraday\"\r\n",
        "gold_container = 'gold'\r\n",
        "\r\n",
        "#Table Specific Variables\r\n",
        "# Session Hits\r\n",
        "session_hits_target_folder = '/GA4-SweatyBetty/session_hits/'\r\n",
        "session_hits_delta_table_path = f\"abfss://{gold_container}@{account_name}.dfs.core.windows.net/{session_hits_target_folder}\"\r\n",
        "\r\n",
        "# Session\r\n",
        "\r\n",
        "session_target_folder = '/GA4-SweatyBetty/session/'\r\n",
        "session_delta_table_path = f\"abfss://{gold_container}@{account_name}.dfs.core.windows.net/{session_target_folder}\"\r\n",
        "\r\n",
        "# Product Revenue\r\n",
        "\r\n",
        "product_revenue_target_folder = '/GA4-SweatyBetty/product_revenue/'\r\n",
        "product_revenue_delta_table_path = f\"abfss://{gold_container}@{account_name}.dfs.core.windows.net/{product_revenue_target_folder}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Print Variable Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(f\"Account Name: {account_name}\")\r\n",
        "print(f\"JSON Blob Path: {json_blob_path}\")\r\n",
        "print(f\"Base Folder: {base_folder}\")\r\n",
        "print(f\"Gold Container: {gold_container}\")\r\n",
        "\r\n",
        "# Table Specific Variables\r\n",
        "# Session Hits\r\n",
        "print(f\"Session Hits Target Folder: {session_hits_target_folder}\")\r\n",
        "print(f\"Session Hits Delta Table Path: {session_hits_delta_table_path}\")\r\n",
        "\r\n",
        "# Session\r\n",
        "print(f\"Session Target Folder: {session_target_folder}\")\r\n",
        "print(f\"Session Delta Table Path: {session_delta_table_path}\")\r\n",
        "\r\n",
        "# Product Revenue\r\n",
        "print(f\"Product Revenue Target Folder: {product_revenue_target_folder}\")\r\n",
        "print(f\"Product Revenue Delta Table Path: {product_revenue_delta_table_path}\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Event Session Hits Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define Function to process events for Session Hits<br>\r\n",
        "This function reads raw GA4 event data from Azure Data Lake (stored as Parquet), enriches it with session-level metadata, and transforms it into a structured format suitable for downstream analytics. It extracts key fields from nested structures, calculates session-specific metrics such as hit_number and visit_start_time, and generates a unique_key hash used for identifying each event uniquely across systems. The timestamp formatting is carefully adjusted to ensure compatibility with existing systems like Snowflake by preserving millisecond precision.<br>\r\n",
        "Reads GA4 event data from a specified Parquet file in Azure Data Lake using a dynamic table path.<br>\r\n",
        "\r\n",
        "Converts event_timestamp from microseconds to a Spark timestamp with millisecond precision for accurate time-based analytics and hash generation.<br>\r\n",
        "\r\n",
        "Extracts specific fields from the nested event_params map, including both string and numeric types like page_path, event_value, and engagement_time_msec.<br>\r\n",
        "\r\n",
        "Generates a session_hkey as an MD5 hash based on user_pseudo_id, ga_session_id, and ga_session_number.<br>\r\n",
        "\r\n",
        "Calculates session-level metadata using window functions:<br>     • visit_start_time: earliest event timestamp per session<br>     • hit_number: sequential event number within a session<br>     • is_exit: flag for last event in a session<br>\r\n",
        "\r\n",
        "Truncates visit_start_time to milliseconds (3 digits) to match Snowflake formatting.<br>\r\n",
        "\r\n",
        "Builds a consistent unique_key by hashing session_hkey, visitor_key, hit_number, and visit_start_time — matching Snowflake's MD5 logic exactly.<br>\r\n",
        "\r\n",
        "Returns a DataFrame with selected output columns ready for merge or analytics.<br>\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {},
      "source": [
        "#trying to match unique key 2\r\n",
        "from pyspark.sql.functions import (\r\n",
        "    col, md5, concat_ws, coalesce, row_number, min, max,\r\n",
        "    lpad, hour, minute, lit, date_format\r\n",
        ")\r\n",
        "from pyspark.sql.window import Window\r\n",
        "\r\n",
        "def process_event_table_session_hits(table_name: str, raw_adls_path: str, base_folder: str):\r\n",
        "    full_path = f\"{raw_adls_path}{base_folder}/{table_name}\"\r\n",
        "    df_events = spark.read.parquet(full_path)\r\n",
        "\r\n",
        "    # Step 1: Convert event_timestamp to timestamp with millisecond precision\r\n",
        "    df_events = df_events.withColumn(\r\n",
        "        \"event_timestamp_ts\",\r\n",
        "        (col(\"event_timestamp\") / 1000000).cast(\"timestamp\")\r\n",
        "    )\r\n",
        "\r\n",
        "    # Step 2: Extract string fields from event_params\r\n",
        "    extract_fields = [\r\n",
        "        \"event_category\", \"event_action\", \"event_label\", \"value\",\r\n",
        "        \"entrances\", \"page_title\", \"page_path\", \"page_location\",\r\n",
        "        \"engagement_time_msec\"\r\n",
        "    ]\r\n",
        "    for field in extract_fields:\r\n",
        "        df_events = df_events.withColumn(f\"ep_{field}\", col(\"event_params\").getItem(field).getField(\"string_value\"))\r\n",
        "\r\n",
        "    # Step 3: Extract numeric fields\r\n",
        "    df_events = df_events.withColumn(\"ep_engagement_time_msec\", col(\"event_params\").getItem(\"engagement_time_msec\").getField(\"int_value\")) \\\r\n",
        "                         .withColumn(\"ep_entrances\", col(\"event_params\").getItem(\"entrances\").getField(\"int_value\")) \\\r\n",
        "                         .withColumn(\"ep_value\", col(\"event_params\").getItem(\"value\").getField(\"double_value\"))\r\n",
        "\r\n",
        "    # Step 4: Compute session_hkey\r\n",
        "    df_events = df_events.withColumn(\"session_hkey\", md5(concat_ws(\"-\",\r\n",
        "        coalesce(col(\"user_pseudo_id\").cast(\"string\"), lit(\"\")),\r\n",
        "        coalesce(col(\"event_params\").getItem(\"ga_session_id\").getField(\"int_value\").cast(\"string\"), lit(\"\")),\r\n",
        "        coalesce(col(\"event_params\").getItem(\"ga_session_number\").getField(\"int_value\").cast(\"string\"), lit(\"\"))\r\n",
        "    )))\r\n",
        "\r\n",
        "    # Step 5: Get visit_key\r\n",
        "    df_events = df_events.withColumn(\"visit_key\", col(\"event_params\").getItem(\"ga_session_id\").getField(\"int_value\").cast(\"string\"))\r\n",
        "\r\n",
        "    # Step 6: Define windows\r\n",
        "    session_window = Window.partitionBy(\"session_hkey\").orderBy(\r\n",
        "        col(\"event_timestamp_ts\").asc(),\r\n",
        "        col(\"ep_engagement_time_msec\").asc()\r\n",
        "    )\r\n",
        "    session_group = Window.partitionBy(\"session_hkey\")\r\n",
        "\r\n",
        "    # Step 7: Calculate base fields\r\n",
        "    df_events = df_events.withColumn(\"visitor_key\", col(\"user_id\")) \\\r\n",
        "        .withColumn(\"raw_visit_start_time\", min(\"event_timestamp_ts\").over(session_group)) \\\r\n",
        "        .withColumn(\"hit_number\", row_number().over(session_window)) \\\r\n",
        "        .withColumn(\"hour_minute\", concat_ws(\":\", hour(\"event_timestamp_ts\"), lpad(minute(\"event_timestamp_ts\"), 2, '0'))) \\\r\n",
        "        .withColumn(\"country_iso2\", col(\"geo.country\")) \\\r\n",
        "        .withColumn(\"is_entrance\", col(\"ep_entrances\")) \\\r\n",
        "        .withColumn(\"is_exit\", (col(\"event_timestamp_ts\") == max(\"event_timestamp_ts\").over(session_group)).cast(\"int\")) \\\r\n",
        "        .withColumn(\"page_pagetitle\", col(\"ep_page_title\")) \\\r\n",
        "        .withColumn(\"page_pagepath\", col(\"ep_page_path\")) \\\r\n",
        "        .withColumn(\"appinfo_landingscreenname\", min(\"ep_page_location\").over(session_group)) \\\r\n",
        "        .withColumn(\"eventinfo_eventaction\", col(\"ep_event_action\")) \\\r\n",
        "        .withColumn(\"eventinfo_eventcategory\", col(\"ep_event_category\")) \\\r\n",
        "        .withColumn(\"eventinfo_eventlabel\", col(\"ep_event_label\")) \\\r\n",
        "        .withColumn(\"eventinfo_eventvalue\", col(\"ep_value\"))\r\n",
        "\r\n",
        "    # Step 8: Truncate visit_start_time to milliseconds (match Snowflake precision)\r\n",
        "    df_events = df_events.withColumn(\r\n",
        "        \"visit_start_time\",\r\n",
        "        date_format(\"raw_visit_start_time\", \"yyyy-MM-dd HH:mm:ss.SSS\").cast(\"timestamp\")\r\n",
        "    )\r\n",
        "\r\n",
        "    # Step 9: Add effective_from, load_datetime, and unique_key\r\n",
        "    df_events = df_events.withColumn(\"effective_from\", col(\"visit_start_time\")) \\\r\n",
        "        .withColumn(\"load_datetime\", col(\"event_timestamp_ts\")) \\\r\n",
        "        .withColumn(\"unique_key\", md5(concat_ws(\"-\",\r\n",
        "            coalesce(col(\"session_hkey\").cast(\"string\"), lit(\"\")),\r\n",
        "            coalesce(col(\"visitor_key\").cast(\"string\"), lit(\"\")),\r\n",
        "            coalesce(col(\"hit_number\").cast(\"string\"), lit(\"\")),\r\n",
        "            coalesce(col(\"visit_start_time\").cast(\"string\"), lit(\"\"))\r\n",
        "        )))\r\n",
        "\r\n",
        "    # Step 10: Select final output columns\r\n",
        "    final_columns = [\r\n",
        "        \"event_timestamp\", \"event_name\",\r\n",
        "        \"session_hkey\", \"visitor_key\", \"visit_key\", \"visit_start_time\", \"hit_number\", \"hour_minute\",\r\n",
        "        \"country_iso2\", \"is_entrance\", \"is_exit\", \"page_pagetitle\", \"page_pagepath\",\r\n",
        "        \"appinfo_landingscreenname\", \"eventinfo_eventaction\", \"eventinfo_eventcategory\",\r\n",
        "        \"eventinfo_eventlabel\", \"eventinfo_eventvalue\", \"effective_from\", \"load_datetime\", \"unique_key\"\r\n",
        "    ]\r\n",
        "\r\n",
        "    return df_events.select(*final_columns)\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Save function with merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Merge 2\r\n",
        "from delta.tables import DeltaTable\r\n",
        "from pyspark.sql.functions import col\r\n",
        "\r\n",
        "def save_to_delta_merge(df, spark, raw_adls_path: str, delta_table_path: str):\r\n",
        "    account_name = raw_adls_path.split('@')[1].split('.')[0]\r\n",
        "\r\n",
        "    if DeltaTable.isDeltaTable(spark, delta_table_path):\r\n",
        "        delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
        "\r\n",
        "        merge_condition = (\r\n",
        "            (col(\"target.unique_key\") == col(\"source.unique_key\"))\r\n",
        "\r\n",
        "        )\r\n",
        "\r\n",
        "        delta_table.alias(\"target\").merge(\r\n",
        "            source=df.alias(\"source\"),\r\n",
        "            condition=merge_condition\r\n",
        "        ).whenMatchedUpdateAll() \\\r\n",
        "         .whenNotMatchedInsertAll() \\\r\n",
        "         .execute()\r\n",
        "    else:\r\n",
        "        df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define process_and_save_date_range_session_hits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "from datetime import datetime, timedelta\r\n",
        "\r\n",
        "def process_and_save_date_range_session_hits(\r\n",
        "    raw_adls_path: str,\r\n",
        "    base_folder: str,\r\n",
        "    delta_base_path: str,\r\n",
        "    start_date: str = None,\r\n",
        "    end_date: str = None,\r\n",
        "    file_prefix: str = \"events_\"  # Default prefix\r\n",
        "):\r\n",
        "    if not start_date or not end_date:\r\n",
        "        yesterday = (datetime.utcnow() - timedelta(days=1)).strftime(\"%Y%m%d\")\r\n",
        "        start_date = end_date = yesterday\r\n",
        "    \r\n",
        "    start = datetime.strptime(start_date, \"%Y%m%d\")\r\n",
        "    end = datetime.strptime(end_date, \"%Y%m%d\")\r\n",
        "    \r\n",
        "    current = start\r\n",
        "    while current <= end:\r\n",
        "        date_str = current.strftime(\"%Y%m%d\")\r\n",
        "        table_name = f\"{file_prefix}{date_str}\"\r\n",
        "        print(f\"Processing table: {table_name}\")\r\n",
        "        \r\n",
        "        try:\r\n",
        "            df_processed = process_event_table_session_hits(table_name, raw_adls_path, base_folder)\r\n",
        "            delta_table_path = f\"{delta_base_path}\"\r\n",
        "            save_to_delta_merge(df_processed, spark,raw_adls_path, delta_table_path)\r\n",
        "            print(f\"Saved to {delta_table_path}\")\r\n",
        "        except Exception as e:\r\n",
        "            print(f\"Error processing {table_name}: {e}\")\r\n",
        "        \r\n",
        "        current += timedelta(days=1)\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define Function to save data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Save with Append\r\n",
        "def save_to_delta(df, raw_adls_path: str, delta_table_path: str):\r\n",
        "    account_name = raw_adls_path.split('@')[1].split('.')[0]\r\n",
        "    df.write.format(\"delta\") \\\r\n",
        "        .mode(\"append\") \\\r\n",
        "       .save(delta_table_path)\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Process Session Hits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(raw_adls_path)\r\n",
        "print(base_folder)\r\n",
        "print(session_hits_delta_table_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Process Intraday Tables - Session Hits\r\n",
        "The tables with prefix events_fresh_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "from datetime import datetime\r\n",
        "\r\n",
        "# Compute today's date\r\n",
        "today = datetime.today().strftime(\"%Y%m%d\")\r\n",
        "\r\n",
        "# Try running the function for today's date\r\n",
        "try:\r\n",
        "    process_and_save_date_range_session_hits(\r\n",
        "        raw_adls_path=raw_adls_path,\r\n",
        "        base_folder=events_fresh_base_folder,\r\n",
        "        delta_base_path=session_hits_delta_table_path,\r\n",
        "        start_date=today,\r\n",
        "        end_date=today,\r\n",
        "        file_prefix=\"events_fresh_\"\r\n",
        "    )\r\n",
        "except Exception as e:\r\n",
        "    print(f\"No table found or error occurred for date {today}: {e}\")\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Event files with prefix events_intraday\r\n",
        "for example analytics_292120381.events_intraday_20250418"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "from datetime import datetime\r\n",
        "\r\n",
        "# Compute today's date\r\n",
        "today = datetime.today().strftime(\"%Y%m%d\")\r\n",
        "\r\n",
        "# Try running the function for today's date\r\n",
        "try:\r\n",
        "    process_and_save_date_range_session_hits(\r\n",
        "        raw_adls_path=raw_adls_path,\r\n",
        "        base_folder=events_intraday_base_folder,\r\n",
        "        delta_base_path=session_hits_delta_table_path,\r\n",
        "        start_date=today,\r\n",
        "        end_date=today,\r\n",
        "        file_prefix=\"events_intraday_\"\r\n",
        "    )\r\n",
        "except Exception as e:\r\n",
        "    print(f\"No table found or error occurred for date {today}: {e}\")\r\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Process Intraday tables in a specific Date range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {},
      "source": [
        "#The following code can be uncommented to process a specific date range\r\n",
        "'''\r\n",
        "#Intraday tables are processed several times during the day\r\n",
        "process_and_save_date_range_session_hits(\r\n",
        "    raw_adls_path=raw_adls_path,\r\n",
        "    base_folder=base_folder,\r\n",
        "    delta_base_path=session_hits_delta_table_path,\r\n",
        "    start_date=\"20250301\",\r\n",
        "    end_date=\"20250331\",\r\n",
        "    file_prefix = \"events_fresh_\"\r\n",
        ")\r\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Table 2 Sessions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Define Currency exchange dataframe\r\n",
        "We will use this as stub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "collapsed": false
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "# Start Spark session (if not already started)\r\n",
        "spark = SparkSession.builder.getOrCreate()\r\n",
        "\r\n",
        "# Define schema\r\n",
        "schema = StructType([\r\n",
        "    StructField(\"base_currency_code\", StringType(), True),\r\n",
        "    StructField(\"target_currency_code\", StringType(), True),\r\n",
        "    StructField(\"exchange_rate_amount\", DoubleType(), True),\r\n",
        "    StructField(\"valid_from\", TimestampType(), True),\r\n",
        "    StructField(\"valid_to\", TimestampType(), True),\r\n",
        "])\r\n",
        "\r\n",
        "# Create static data\r\n",
        "data = [\r\n",
        "    (\"USD\", \"GBP\", 0.79, datetime(2024, 1, 1), datetime(2025, 12, 31)),\r\n",
        "    (\"EUR\", \"GBP\", 0.85, datetime(2024, 1, 1), datetime(2025, 12, 31)),\r\n",
        "    (\"CAD\", \"GBP\", 0.59, datetime(2024, 1, 1), datetime(2025, 12, 31)),\r\n",
        "    (\"AUD\", \"GBP\", 0.52, datetime(2024, 1, 1), datetime(2025, 12, 31)),\r\n",
        "    (\"GBP\", \"GBP\", 1.00, datetime(2024, 1, 1), datetime(2025, 12, 31)),  # Optional: identity\r\n",
        "]\r\n",
        "\r\n",
        "# Create DataFrame\r\n",
        "exchange_rate_df = spark.createDataFrame(data, schema)\r\n",
        "\r\n",
        "# Show result\r\n",
        "display(exchange_rate_df)\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Define Table Name Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define Function to Process Sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import (\r\n",
        "    col, from_unixtime, md5, concat_ws, coalesce, row_number,\r\n",
        "    min, max, first, countDistinct, lit, when\r\n",
        ")\r\n",
        "from pyspark.sql.window import Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Code to be tested\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.window import Window\r\n",
        "\r\n",
        "def process_session_table(table_name: str, raw_adls_path: str, base_folder: str, exchange_rate_df):\r\n",
        "    full_path = f\"{raw_adls_path}{base_folder}/{table_name}\"\r\n",
        "    df_events = spark.read.parquet(full_path)\r\n",
        "\r\n",
        "    # Create session_hkey\r\n",
        "    df_events = df_events.withColumn(\"session_hkey\", md5(concat_ws(\"-\", \r\n",
        "        coalesce(col(\"user_pseudo_id\").cast(\"string\"), lit(\"\")), \r\n",
        "        coalesce(col(\"event_params\").getItem(\"ga_session_id\").getField(\"string_value\"),\r\n",
        "                 col(\"event_params\").getItem(\"ga_session_id\").getField(\"int_value\").cast(\"string\"), lit(\"\")), \r\n",
        "        coalesce(col(\"event_params\").getItem(\"ga_session_number\").getField(\"string_value\"),\r\n",
        "                 col(\"event_params\").getItem(\"ga_session_number\").getField(\"int_value\").cast(\"string\"), lit(\"\"))\r\n",
        "    )))\r\n",
        "\r\n",
        "    # Define windows\r\n",
        "    session_group = Window.partitionBy(\"session_hkey\").orderBy(\"event_timestamp\")\r\n",
        "    session_partition = Window.partitionBy(\"session_hkey\")\r\n",
        "\r\n",
        "    # Add session-level fields\r\n",
        "    df_sessions_base = df_events \\\r\n",
        "        .withColumn(\"session_start_time_raw\", min(\"event_timestamp\").over(session_partition)) \\\r\n",
        "        .withColumn(\"visitor_key\", first(when(col(\"user_id\") != \"undefined\", col(\"user_id\")), ignorenulls=True).over(session_group)) \\\r\n",
        "        .withColumn(\"visit_key\", first(\r\n",
        "            coalesce(\r\n",
        "                col(\"event_params\").getItem(\"ga_session_id\").getField(\"string_value\"),\r\n",
        "                col(\"event_params\").getItem(\"ga_session_id\").getField(\"int_value\").cast(\"string\")\r\n",
        "            ), ignorenulls=True\r\n",
        "        ).over(session_group)) \\\r\n",
        "        .withColumn(\"client_id\", first(\"user_pseudo_id\", ignorenulls=True).over(session_group)) \\\r\n",
        "        .withColumn(\"total_visits\", first(\r\n",
        "            coalesce(\r\n",
        "                col(\"event_params\").getItem(\"session_engaged\").getField(\"string_value\"),\r\n",
        "                col(\"event_params\").getItem(\"session_engaged\").getField(\"int_value\").cast(\"string\")\r\n",
        "            ), ignorenulls=True).over(session_group)) \\\r\n",
        "        .withColumn(\"campaign\", first(col(\"event_params\").getItem(\"campaign\").getField(\"string_value\"), ignorenulls=True).over(session_group)) \\\r\n",
        "        .withColumn(\"source\", first(col(\"event_params\").getItem(\"source\").getField(\"string_value\"), ignorenulls=True).over(session_group)) \\\r\n",
        "        .withColumn(\"medium\", first(col(\"event_params\").getItem(\"medium\").getField(\"string_value\"), ignorenulls=True).over(session_group)) \\\r\n",
        "        .withColumn(\"referrer_path\", first(col(\"event_params\").getItem(\"page_referrer\").getField(\"string_value\"), ignorenulls=True).over(session_group)) \\\r\n",
        "        .withColumn(\"transaction_currency\", max(when(col(\"event_name\") == \"purchase\", \r\n",
        "            coalesce(col(\"event_params\").getItem(\"currency\").getField(\"string_value\"),\r\n",
        "                     col(\"event_params\").getItem(\"currency\").getField(\"int_value\").cast(\"string\"))\r\n",
        "        )).over(session_partition)) \\\r\n",
        "        .withColumn(\"total_transaction_revenue_local_currency\", max(when(col(\"event_name\") == \"purchase\", col(\"ecommerce.purchase_revenue\"))).over(session_partition)) \\\r\n",
        "        .withColumn(\"total_transaction_revenue_usd\", max(when(col(\"event_name\") == \"purchase\", col(\"ecommerce.purchase_revenue_in_usd\"))).over(session_partition)) \\\r\n",
        "        .withColumn(\"units\", max(when(col(\"event_name\") == \"purchase\", col(\"ecommerce.total_item_quantity\"))).over(session_partition)) \\\r\n",
        "        .withColumn(\"load_datetime\", from_unixtime(col(\"event_server_timestamp_offset\") / 1000000)) \\\r\n",
        "        .withColumn(\"global_customer_hkey\", first(col(\"user_properties\").getItem(\"global_customer_id\").getField(\"string_value\"), ignorenulls=True).over(session_group)) \\\r\n",
        "        .withColumn(\"country_iso2\", first(col(\"event_params\").getItem(\"country_iso_2\").getField(\"string_value\"), ignorenulls=True).over(session_group)) \\\r\n",
        "        .withColumn(\"new_returning\", first(col(\"user_properties\").getItem(\"customer_existing\").getField(\"string_value\"), ignorenulls=True).over(session_group)) \\\r\n",
        "        .withColumn(\"device_category\", first(col(\"device.category\"), ignorenulls=True).over(session_group)) \\\r\n",
        "        .withColumn(\"device_language\", first(col(\"device.language\"), ignorenulls=True).over(session_group)) \\\r\n",
        "        .withColumn(\"customer_channel_grouping\", first(col(\"event_params\").getItem(\"custom_channel_grouping\").getField(\"string_value\"), ignorenulls=True).over(session_group)) \\\r\n",
        "        .withColumn(\"trafficsource_adwordsclickinfo_customerid\", first(col(\"collected_traffic_source.gclid\"), ignorenulls=True).over(session_group)) \\\r\n",
        "        .withColumn(\"country\", first(col(\"geo.country\"), ignorenulls=True).over(session_group)) \\\r\n",
        "        .withColumn(\"state\", first(col(\"geo.region\"), ignorenulls=True).over(session_group)) \\\r\n",
        "        .withColumn(\"city\", first(col(\"geo.city\"), ignorenulls=True).over(session_group))\r\n",
        "\r\n",
        "    # Format timestamps\r\n",
        "    df_sessions_base = df_sessions_base \\\r\n",
        "        .withColumn(\"session_start_time\", date_format((col(\"session_start_time_raw\") / 1000000).cast(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss.SSS\").cast(\"timestamp\")) \\\r\n",
        "        .withColumn(\"session_start_time_str\", date_format((col(\"session_start_time_raw\") / 1000000).cast(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss.SSS\")) \\\r\n",
        "        .withColumn(\"load_datetime\", date_format(col(\"load_datetime\"), \"yyyy-MM-dd HH:mm:ss.SSS\").cast(\"timestamp\"))\r\n",
        "\r\n",
        "    # Transaction counts\r\n",
        "    transaction_counts = df_events \\\r\n",
        "        .filter(col(\"event_name\") == \"purchase\") \\\r\n",
        "        .withColumn(\"transaction_id\", col(\"event_params\").getItem(\"transaction_id\").getField(\"string_value\")) \\\r\n",
        "        .filter(col(\"transaction_id\").isNotNull()) \\\r\n",
        "        .select(\"session_hkey\", \"transaction_id\").dropDuplicates() \\\r\n",
        "        .groupBy(\"session_hkey\").agg(countDistinct(\"transaction_id\").alias(\"transactions\"))\r\n",
        "\r\n",
        "    # Pageview counts\r\n",
        "    pageview_counts = df_events \\\r\n",
        "        .filter(col(\"event_name\") == \"page_view\") \\\r\n",
        "        .withColumn(\"page_location\", col(\"event_params\").getItem(\"page_location\").getField(\"string_value\")) \\\r\n",
        "        .filter(col(\"page_location\").isNotNull()) \\\r\n",
        "        .select(\"session_hkey\", \"page_location\").dropDuplicates() \\\r\n",
        "        .groupBy(\"session_hkey\").agg(countDistinct(\"page_location\").alias(\"session_pageviews\"))\r\n",
        "\r\n",
        "    # Join counts\r\n",
        "    df_sessions = df_sessions_base \\\r\n",
        "        .join(transaction_counts, on=\"session_hkey\", how=\"left\") \\\r\n",
        "        .join(pageview_counts, on=\"session_hkey\", how=\"left\")\r\n",
        "\r\n",
        "    # Join with exchange rates\r\n",
        "    df_sessions = df_sessions.alias(\"s\").join(\r\n",
        "        exchange_rate_df.alias(\"er\"),\r\n",
        "        (\r\n",
        "            (col(\"s.transaction_currency\") == col(\"er.base_currency_code\")) &\r\n",
        "            (col(\"s.session_start_time\").between(col(\"er.valid_from\"), col(\"er.valid_to\"))) &\r\n",
        "            (col(\"er.target_currency_code\") == lit(\"GBP\"))\r\n",
        "        ),\r\n",
        "        how=\"left\"\r\n",
        "    ).withColumn(\r\n",
        "        \"total_transaction_revenue_gbp\",\r\n",
        "        when(col(\"transaction_currency\") == \"GBP\", col(\"total_transaction_revenue_local_currency\"))\r\n",
        "        .otherwise(col(\"total_transaction_revenue_local_currency\") * coalesce(col(\"er.exchange_rate_amount\"), lit(1)))\r\n",
        "    )\r\n",
        "\r\n",
        "    # Unique key using session_start_time_str for strict precision\r\n",
        "    df_sessions = df_sessions.withColumn(\r\n",
        "        \"unique_key\",\r\n",
        "        md5(concat_ws(\"-\",\r\n",
        "            coalesce(col(\"session_hkey\").cast(\"string\"), lit(\"\")),\r\n",
        "            coalesce(col(\"visitor_key\").cast(\"string\"), lit(\"\")),\r\n",
        "            coalesce(col(\"client_id\").cast(\"string\"), lit(\"\")),\r\n",
        "            coalesce(col(\"session_start_time_str\"), lit(\"\")),\r\n",
        "            coalesce(col(\"transaction_currency\").cast(\"string\"), lit(\"\"))\r\n",
        "        ))\r\n",
        "    )\r\n",
        "\r\n",
        "    # Final columns\r\n",
        "    final_cols = [\r\n",
        "        \"session_hkey\", \"session_start_time\", \"session_start_time_str\", \"visitor_key\", \"visit_key\", \"client_id\",\r\n",
        "        \"total_visits\", \"global_customer_hkey\", \"country_iso2\", \"new_returning\",\r\n",
        "        \"referrer_path\", \"device_category\", \"device_language\", \"customer_channel_grouping\",\r\n",
        "        \"campaign\", \"source\", \"medium\", \"trafficsource_adwordsclickinfo_customerid\",\r\n",
        "        \"country\", \"state\", \"city\", \"session_pageviews\", \"transactions\", \"units\",\r\n",
        "        \"transaction_currency\", \"total_transaction_revenue_local_currency\",\r\n",
        "        \"total_transaction_revenue_usd\", \"total_transaction_revenue_gbp\",\r\n",
        "        \"load_datetime\", \"unique_key\"\r\n",
        "    ]\r\n",
        "\r\n",
        "    return df_sessions.select(*final_cols)\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define Date range function \r\n",
        "## Intraday Session Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {},
      "source": [
        "from datetime import datetime, timedelta\r\n",
        "\r\n",
        "def process_and_save_date_range_session(\r\n",
        "    raw_adls_path: str,\r\n",
        "    base_folder: str,\r\n",
        "    delta_base_path: str,\r\n",
        "    exchange_rate_df: DataFrame,\r\n",
        "    start_date: str = None,\r\n",
        "    end_date: str = None\r\n",
        "    \r\n",
        "):\r\n",
        "    if not start_date or not end_date:\r\n",
        "        yesterday = (datetime.utcnow() - timedelta(days=1)).strftime(\"%Y%m%d\")\r\n",
        "        start_date = end_date = yesterday\r\n",
        "    \r\n",
        "    start = datetime.strptime(start_date, \"%Y%m%d\")\r\n",
        "    end = datetime.strptime(end_date, \"%Y%m%d\")\r\n",
        "    \r\n",
        "    current = start\r\n",
        "    while current <= end:\r\n",
        "        date_str = current.strftime(\"%Y%m%d\")\r\n",
        "        #events_fresh_ will process Intra Day tables\r\n",
        "        table_name = f\"events_fresh_{date_str}\"\r\n",
        "        print(f\"Processing table: {table_name}\")\r\n",
        "        \r\n",
        "        try:\r\n",
        "            #process_session_table(table_name: str, raw_adls_path: str, base_folder: str, exchange_rate_df):\r\n",
        "            df_processed = process_session_table(table_name, raw_adls_path, events_fresh_base_folder,exchange_rate_df)\r\n",
        "            df_processed = df_processed.dropDuplicates()\r\n",
        "            delta_table_path = f\"{delta_base_path}\"\r\n",
        "            save_to_delta_merge(df_processed,spark, raw_adls_path, delta_table_path)\r\n",
        "            print(f\"Saved to {delta_table_path}\")\r\n",
        "            current += timedelta(days=1)\r\n",
        "            \r\n",
        "        except Exception as e:\r\n",
        "            print(f\"Error processing {table_name}: {e}\")\r\n",
        "            \r\n",
        "\r\n",
        "        #Process table that starts with prefix Intraday\r\n",
        "        table_name = f\"events_intraday_{date_str}\"\r\n",
        "        print(f\"Processing table: {table_name}\")\r\n",
        "        \r\n",
        "        try:\r\n",
        "            #process_session_table(table_name: str, raw_adls_path: str, base_folder: str, exchange_rate_df):\r\n",
        "            df_processed = process_session_table(table_name, raw_adls_path, events_intraday_base_folder,exchange_rate_df)\r\n",
        "            df_processed = df_processed.dropDuplicates()\r\n",
        "            delta_table_path = f\"{delta_base_path}\"\r\n",
        "            save_to_delta_merge(df_processed,spark, raw_adls_path, delta_table_path)\r\n",
        "            print(f\"Saved to {delta_table_path}\")\r\n",
        "            current += timedelta(days=1)\r\n",
        "            \r\n",
        "        except Exception as e:\r\n",
        "            print(f\"Error processing {table_name}: {e}\")\r\n",
        "            raise\r\n",
        "        \r\n",
        "        current += timedelta(days=1)    \r\n",
        "        \r\n",
        "        \r\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(raw_adls_path)\r\n",
        "print(base_folder)\r\n",
        "print(session_delta_table_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Process Session Intraday for Today"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "from datetime import datetime\r\n",
        "\r\n",
        "# Compute today's date\r\n",
        "today = datetime.today().strftime(\"%Y%m%d\")\r\n",
        "\r\n",
        "# Try running the function for today's date\r\n",
        "try:\r\n",
        "    process_and_save_date_range_session(\r\n",
        "        raw_adls_path=raw_adls_path,\r\n",
        "        base_folder=base_folder,\r\n",
        "        delta_base_path=session_delta_table_path,\r\n",
        "        exchange_rate_df=exchange_rate_df,\r\n",
        "        start_date=today,\r\n",
        "        end_date=today\r\n",
        "    )\r\n",
        "except Exception as e:\r\n",
        "    print(f\"No table found or error occurred for date {today}: {e}\")\r\n",
        "    raise\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Process Session for a Date Range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {},
      "source": [
        "#The following code can be uncommented to process data for a specific date range\r\n",
        "'''\r\n",
        "process_and_save_date_range_session(\r\n",
        "    raw_adls_path=raw_adls_path,\r\n",
        "    base_folder=base_folder,\r\n",
        "    delta_base_path=session_delta_table_path,\r\n",
        "    exchange_rate_df=exchange_rate_df,\r\n",
        "    start_date=\"20250325\",\r\n",
        "    end_date=\"20250331\"    \r\n",
        ")\r\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define function to delete the delta table <br>\r\n",
        "We can delte delta table for a clean run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {},
      "source": [
        "from delta.tables import DeltaTable\r\n",
        "\r\n",
        "def truncate_delta_table(spark, delta_table_path: str):\r\n",
        "    try:\r\n",
        "        delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
        "        delta_table.delete()  # No condition = delete all rows\r\n",
        "        print(f\"All data deleted from Delta table at {delta_table_path}\")\r\n",
        "    except Exception as e:\r\n",
        "        print(f\"Failed to truncate Delta table at {delta_table_path}: {e}\")\r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    }
  }
}