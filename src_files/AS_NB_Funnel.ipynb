{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 198,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Read sftp environment and copy to raw\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.types import DecimalType\r\n",
        "from delta.tables import *\r\n",
        "import requests\r\n",
        "from pyspark.sql import functions as F\r\n",
        "from datetime import datetime, timedelta, date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Move files to raw data lake in Funnel folder\r\n",
        "files = mssparkutils.fs.ls(f'abfss://funnel@{sftp_adls_path}/data')\r\n",
        "print('Found', len(files), 'files.')\r\n",
        "\r\n",
        "# Check if there are any files to move\r\n",
        "if len(files) == 0 and (env_var == env_dict['prod'] or env_var == env_dict['prod_backup']):\r\n",
        "    # Warn there might be an issue:\r\n",
        "    print(\"No files found to move. Ending the job and sending notification.\")\r\n",
        "    response = requests.post(\r\n",
        "        'https://prod-85.eastus.logic.azure.com:443/workflows/7367758ef3da4d76b4e64670220d6135/triggers/manual/paths/invoke?api-version=2016-10-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=3qXzAxaCqincRFQ358oeDwq_SGn5vgzaNgW26QxUDMs',\r\n",
        "        '{\"email_to\": \"ededl@wwgroups.net\", \"email_subject\": \"No Funnel Data Available to Process\", \"email_body\": \"Did not find any file to process for daily Funnel data.\", \"email_from\": \"AzureSynapse@wwwinc.com\"}',\r\n",
        "        headers={\"Content-Type\": \"application/json\"}\r\n",
        "    )\r\n",
        "\r\n",
        "    # Exit the notebook when no files are available\r\n",
        "    notebookutils.mssparkutils.notebook.exit(0)\r\n",
        "\r\n",
        "elif len(files) == 0 and (env_var == env_dict['dev'] or env_var == env_dict['test']):\r\n",
        "    # For Dev and Test environments\r\n",
        "    print(\"No files found in Dev/Test environment. Skipping file processing.\")\r\n",
        "    \r\n",
        "    # Exit the notebook when no files are available\r\n",
        "    notebookutils.mssparkutils.notebook.exit(0)\r\n",
        "\r\n",
        "else:\r\n",
        "# Proceed if there are files\r\n",
        "    for file in files:\r\n",
        "        # Move the file to the raw data lake\r\n",
        "        mssparkutils.fs.mv(src=file.path, dest=f'{raw_adls_path}Funnel/{datetime.now().strftime(\"%Y/%m/%d\")}/{file.name}', create_path=True, overwrite=True)\r\n",
        "\r\n",
        "    print('All files moved successfully.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Set up path prefixes\r\n",
        "#today = date.today()\r\n",
        "#history = today - timedelta(days=1) # YYYY-MM-DD Modify based on current date and data availability in raw folder (history)\r\n",
        "#fileprefix = history.strftime(\"%Y/%m/%d/\")\r\n",
        "fileprefix = datetime.now().strftime(\"%Y/%m/%d/\")\r\n",
        "raw_path = f\"{raw_adls_path}Funnel/{fileprefix}\"\r\n",
        "gold_path = f\"{gold_adls_path}Funnel\"\r\n",
        "\r\n",
        "# Get max DqLoadDatetime from the Delta table\r\n",
        "gold_df = spark.read.format(\"delta\").load(gold_path)\r\n",
        "max_loaddatetime_row = gold_df.select(max(\"DqLoadDatetime\").alias(\"max_loaddatetime\")).collect()\r\n",
        "max_loaddatetime = max_loaddatetime_row[0][\"max_loaddatetime\"]\r\n",
        "    \r\n",
        "#Look for all files in Funnel root folder\r\n",
        "raw_file_paths = mssparkutils.fs.ls(raw_path)\r\n",
        "\r\n",
        "#Filter the list of files to only get the ones with modifyTime after our watermark time\r\n",
        "new_raw_files_to_process = [\r\n",
        "    f for f in raw_file_paths\r\n",
        "    if f.isFile and datetime.utcfromtimestamp(f.modifyTime / 1000) > max_loaddatetime\r\n",
        "]\r\n",
        "\r\n",
        "#Output results - these are the files to process since the last watermark.\r\n",
        "for f in new_raw_files_to_process:\r\n",
        "    print(f\"{f.name} - {datetime.utcfromtimestamp(f.modifyTime / 1000)}\")\r\n",
        "\r\n",
        "#Skip ingestion if no new files to process\r\n",
        "if len(new_raw_files_to_process) == 0:\r\n",
        "    print(\"All Funnel files already processed. Skipping ingestion.\")\r\n",
        "    notebookutils.mssparkutils.notebook.exit(0)\r\n",
        "\r\n",
        "#Load the raw Funnel CSV files\r\n",
        "files_to_load = [f.path for f in new_raw_files_to_process]\r\n",
        "\r\n",
        "funnel_df = spark.read.format(\"csv\") \\\r\n",
        "    .option(\"compression\", \"gzip\") \\\r\n",
        "    .option(\"header\", True) \\\r\n",
        "    .load(files_to_load)\r\n",
        "\r\n",
        "print(f\"Loaded {len(files_to_load)} file(s) into DataFrame.\")\r\n",
        "\r\n",
        "# Check if the DataFrame is empty\r\n",
        "if funnel_df.count() == 0 and (env_var == env_dict['prod'] or env_var == env_dict['prod_backup']):\r\n",
        "    print(\"Funnel Data File is empty. Exiting the notebook.\")\r\n",
        "    response = requests.post(\r\n",
        "        'https://prod-85.eastus.logic.azure.com:443/workflows/7367758ef3da4d76b4e64670220d6135/triggers/manual/paths/invoke?api-version=2016-10-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=3qXzAxaCqincRFQ358oeDwq_SGn5vgzaNgW26QxUDMs',\r\n",
        "        '{\"email_to\": \"ededl@wwgroups.net\", \"email_subject\": \"Funnel Data File is Empty\", \"email_body\": \"Did not find any records in the file to process for daily Funnel data.\", \"email_from\": \"AzureSynapse@wwwinc.com\"}',\r\n",
        "        headers={\"Content-Type\": \"application/json\"}\r\n",
        "    )\r\n",
        "    # Exit the notebook\r\n",
        "    notebookutils.mssparkutils.notebook.exit(0)\r\n",
        "\r\n",
        "elif funnel_df.count() == 0 and (env_var == env_dict['dev'] or env_var == env_dict['test']):\r\n",
        "    # For Dev and Test environments\r\n",
        "    print(\"Funnel data file in Dev/Test environment is empty. Skipping file processing.\")\r\n",
        "    \r\n",
        "    # Exit the notebook when no data are available\r\n",
        "    notebookutils.mssparkutils.notebook.exit(0)\r\n",
        "else:\r\n",
        "\r\n",
        "    # Continue with the processing if there is data\r\n",
        "    print(\"File contains data. Proceeding with further processing.\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Apply Pascal Case for Columns\r\n",
        "funnel_df = funnel_df\\\r\n",
        "            .withColumn(\"dq_load_datetime\", current_timestamp())\\\r\n",
        "            .select(\r\n",
        "               col(\"custom_market\").alias(\"CustomMarket\"),\r\n",
        "               col(\"custom_tactic\").alias(\"CustomTactic\"),\r\n",
        "               col(\"campaign\").alias(\"Campaign\"),\r\n",
        "               col(\"FB_Campaign_Type_PRORETConsideration\").alias(\"FBCampaignTypePRORETConsideration\"),\r\n",
        "               col(\"G_Ads_Campaign_Type_searchPmaxYTDisplay\").alias(\"GAdsCampaignTypeSearchPmaxYTDisplay\"),\r\n",
        "               col(\"media_type\").alias(\"MediaType\"),\r\n",
        "               col(\"date\").alias(\"Date\"),\r\n",
        "               col(\"month\").alias(\"Month\"),\r\n",
        "               col(\"month_number\").alias(\"MonthNumber\"),\r\n",
        "               col(\"week\").alias(\"Week\"),\r\n",
        "               col(\"year\").alias(\"Year\"),\r\n",
        "               col(\"data_source_type\").alias(\"DataSourceType\"),\r\n",
        "               col(\"data_source_id\").alias(\"DataSourceId\"),               \r\n",
        "               col(\"currency\").alias(\"Currency\"),              \r\n",
        "               col(\"clicks\").alias(\"Clicks\"),               \r\n",
        "               col(\"cost\").alias(\"Cost\"),\r\n",
        "               col(\"impressions\").alias(\"Impressions\"),\r\n",
        "               col(\"ad_platform_transactions\").alias(\"AdPlatformTransactions\"),\r\n",
        "               col(\"dq_load_datetime\").alias(\"DqLoadDatetime\")\r\n",
        "            )\\\r\n",
        "    .distinct() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Write dataframe to bronze \r\n",
        "\r\n",
        "bronze_path = f\"{bronze_adls_path}Funnel\"\r\n",
        "\r\n",
        "if DeltaTable.isDeltaTable(spark, bronze_path):\r\n",
        "    delta_table = DeltaTable.forPath(spark, bronze_path)\r\n",
        "\r\n",
        "  # Collect distinct Date values\r\n",
        "    date_values = [row[\"Date\"] for row in funnel_df.select(\"Date\").distinct().dropna().collect()]\r\n",
        "    \r\n",
        "    if date_values:\r\n",
        "        # Delete all distinct dates from the existing Delta table\r\n",
        "        formatted_dates = \",\".join([f\"'{d}'\" for d in date_values])\r\n",
        "        delta_table.delete(f\"Date IN ({formatted_dates})\")\r\n",
        "\r\n",
        "    # Insert the latest rows\r\n",
        "    funnel_df.write.format(\"delta\").mode(\"append\").partitionBy(\"Date\").save(bronze_path)\r\n",
        "\r\n",
        "    print(\"Deleted old data by Date and inserted updated rows.\")\r\n",
        "\r\n",
        "else:\r\n",
        "   #Initial table creation if it doesn't exist\r\n",
        "    funnel_df.write.option(\"overwriteSchema\", \"true\") \\\r\n",
        "        .mode(\"overwrite\") \\\r\n",
        "        .format(\"delta\") \\\r\n",
        "        .partitionBy(\"Date\") \\\r\n",
        "        .save(bronze_path)\r\n",
        "\r\n",
        "    print(\"Created delta table.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Do Transformations for gold\r\n",
        "funnel_gold_df = funnel_df\\\r\n",
        "           .withColumn(\"dq_load_datetime\", current_timestamp())\\\r\n",
        "           .withColumn(\"cost\", col(\"cost\").cast(DecimalType(38,6)))\\\r\n",
        "           .withColumn(\"impressions\", col(\"impressions\").cast(DecimalType(38, 0)))\\\r\n",
        "           .withColumn(\"clicks\", col(\"clicks\").cast(DecimalType(38, 0)))\\\r\n",
        "           .withColumn(\"AdPlatformTransactions\", col(\"AdPlatformTransactions\").cast(DecimalType(38, 3)))\\\r\n",
        "           .withColumn(\"campaign_type_consideration_vs_consideration\",lit(None).cast(\"string\"))\\\r\n",
        "           .withColumn(\"campaign_type\",\r\n",
        "            when(\r\n",
        "                    (col(\"GAdsCampaignTypeSearchPmaxYTDisplay\").isNotNull()) & (col(\"GAdsCampaignTypeSearchPmaxYTDisplay\") != \"\"),\r\n",
        "                        when(\r\n",
        "                                (col(\"FBCampaignTypePRORETConsideration\").isNotNull()) & (col(\"FBCampaignTypePRORETConsideration\") != \"\"),\r\n",
        "                                col(\"GAdsCampaignTypeSearchPmaxYTDisplay\") + lit(\" - \") + col(\"FBCampaignTypePRORETConsideration\")\r\n",
        "                            ).otherwise(col(\"GAdsCampaignTypeSearchPmaxYTDisplay\"))\r\n",
        "                 ).otherwise(col(\"FBCampaignTypePRORETConsideration\"))\r\n",
        "                )\\\r\n",
        "            .select(\r\n",
        "               col(\"date\").alias(\"Date\"),\r\n",
        "               col(\"DataSourceId\"),\r\n",
        "               col(\"year\").alias(\"Year\"),\r\n",
        "               col(\"month\").alias(\"Month\"),\r\n",
        "               col(\"MonthNumber\"),\r\n",
        "               col(\"week\").alias(\"Week\"),\r\n",
        "               col(\"DataSourceType\"),\r\n",
        "               col(\"CustomMarket\"),\r\n",
        "               col(\"campaign\").alias(\"Campaign\"),\r\n",
        "               col(\"CustomTactic\"),\r\n",
        "               col(\"campaign_type_consideration_vs_consideration\").alias(\"CampaignTypeConsiderationVsConsideration\"),\r\n",
        "               col(\"MediaType\"),\r\n",
        "               col(\"campaign_type\").alias(\"CampaignType\"),\r\n",
        "               col(\"currency\").alias(\"Currency\"),\r\n",
        "               col(\"cost\").alias(\"Cost\"),\r\n",
        "               col(\"impressions\").alias(\"Impressions\"),\r\n",
        "               col(\"clicks\").alias(\"Clicks\"),\r\n",
        "               col(\"AdPlatformTransactions\"),\r\n",
        "               col(\"dq_load_datetime\").alias(\"DqLoadDatetime\")\r\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Write dataframe to gold \r\n",
        "\r\n",
        "gold_path = f\"{gold_adls_path}Funnel\"\r\n",
        "\r\n",
        "if DeltaTable.isDeltaTable(spark, gold_path):\r\n",
        "    delta_table = DeltaTable.forPath(spark, gold_path)\r\n",
        "\r\n",
        "  # Collect distinct Date values\r\n",
        "    date_values = [row[\"Date\"] for row in funnel_df.select(\"Date\").distinct().dropna().collect()]\r\n",
        "    \r\n",
        "    if date_values:\r\n",
        "        # Delete all distinct dates from the existing Delta table\r\n",
        "        formatted_dates = \",\".join([f\"'{d}'\" for d in date_values])\r\n",
        "        delta_table.delete(f\"Date IN ({formatted_dates})\")\r\n",
        "\r\n",
        "    # Insert the latest rows\r\n",
        "    funnel_gold_df.write.format(\"delta\").mode(\"append\").partitionBy(\"Date\").save(gold_path)\r\n",
        "\r\n",
        "    print(\"Deleted old data by Date and inserted updated rows.\")\r\n",
        "\r\n",
        "else:\r\n",
        "   #Initial table creation if it doesn't exist\r\n",
        "    funnel_gold_df.write.option(\"overwriteSchema\", \"true\") \\\r\n",
        "        .mode(\"overwrite\") \\\r\n",
        "        .format(\"delta\") \\\r\n",
        "        .partitionBy(\"Date\") \\\r\n",
        "        .save(gold_path)\r\n",
        "\r\n",
        "    print(\"Created delta table.\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "notebookutils.mssparkutils.notebook.exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Load history file\r\n",
        "\r\n",
        "schema = StructType([\r\n",
        "    StructField(\"custom_market\", StringType(), True),\r\n",
        "    StructField(\"custom_tactic\", StringType(), True),\r\n",
        "    StructField(\"campaign\", StringType(), True),\r\n",
        "    StructField(\"FB_Campaign_Type_PRORETConsideration\", StringType(), True),\r\n",
        "    StructField(\"G_Ads_Campaign_Type_searchPmaxYTDisplay\", StringType(), True),\r\n",
        "    StructField(\"media_type\", StringType(), True),\r\n",
        "    StructField(\"date\", StringType(), True),\r\n",
        "    StructField(\"month\", StringType(), True),\r\n",
        "    StructField(\"month_number\", StringType(), True),\r\n",
        "    StructField(\"week\", StringType(), True),\r\n",
        "    StructField(\"year\", StringType(), True),\r\n",
        "    StructField(\"data_source_type\", StringType(), True),\r\n",
        "    StructField(\"data_source_id\", StringType(), True),\r\n",
        "    StructField(\"currency\", StringType(), True),\r\n",
        "    StructField(\"clicks\", StringType(), True),\r\n",
        "    StructField(\"cost\", StringType(), True),\r\n",
        "    StructField(\"impressions\", StringType(), True),\r\n",
        "    StructField(\"ad_platform_transactions\", StringType(), True)\r\n",
        "])\r\n",
        "\r\n",
        "raw_path = f\"{raw_adls_path}Funnel\"\r\n",
        "#print(f\"{raw_path}\")\r\n",
        "funnel_df = spark.read.format(\"csv\").option(\"compression\",\"gzip\").option(\"header\",True).schema(schema).option(\"recursiveFileLookup\", \"true\").load(raw_path)\r\n",
        "\r\n",
        "# Rerun steps above for bronze and gold saves to Delta table"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}