{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": "move PFAS data from raw to gold",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Revision History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Change_date         revision_number     change_description                           author\r\n",
        "# 02/16/2024          1                   initial check-in                             Kranthi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyspark.sql.functions as F\r\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType,DateType\r\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"DYNAMIC\")\r\n",
        "from pyspark.sql.window import *\r\n",
        "#from pyspark.sql.functions import row_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# define file schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "schema = StructType([\r\n",
        "    StructField(\"Warehouse\", StringType(), True),\r\n",
        "    StructField(\"Plant\", StringType(), True),\r\n",
        "    StructField(\"CartonNbr\", StringType(), True),\r\n",
        "    StructField(\"PoNbr\", StringType(), True),\r\n",
        "    StructField(\"PoLine\", StringType(), True),\r\n",
        "    StructField(\"Material\", StringType(), True),\r\n",
        "    StructField(\"Size\", StringType(), True),\r\n",
        "    StructField(\"Width\", StringType(), True),\r\n",
        "    StructField(\"UPC\", StringType(), True),\r\n",
        "    StructField(\"CaseQuantity\", DoubleType(), True),\r\n",
        "    StructField(\"DateReceived\", IntegerType(), True),\r\n",
        "    StructField(\"InventoryLockCode\", StringType(), True),\r\n",
        "    StructField(\"InventoryLockCode2\", StringType(), True),\r\n",
        "    StructField(\"InventoryLockCode3\", StringType(), True),\r\n",
        "    StructField(\"InventoryLockCode4\", StringType(), True),\r\n",
        "    StructField(\"InventoryLockCode5\", StringType(), True),\r\n",
        "    StructField(\"SnapshotDate\", IntegerType(), True),\r\n",
        "    StructField(\"PurchasingDocumentNumber\", StringType(), True),\r\n",
        "    StructField(\"PurchasingDocumentItem\", StringType(), True)\r\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Validate the number of files and date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from datetime import datetime\r\n",
        "\r\n",
        "class InvalidInputError(Exception):\r\n",
        "    pass\r\n",
        "\r\n",
        "file_cnt = 0\r\n",
        "file_dates = []\r\n",
        "today_dt = datetime.today().strftime('%Y-%m-%d')\r\n",
        "print(\"today_dt::\",today_dt)\r\n",
        "for j in mssparkutils.fs.ls(f'{raw_adls_path}AS400/INVDATA'):\r\n",
        "  if j.size>0:  ## ignore archive folder\r\n",
        "    file_dates.append(datetime.strftime(datetime.strptime(j.name.split('_')[0],'%Y%m%d'),'%Y-%m-%d'))\r\n",
        "    file_cnt = file_cnt+1\r\n",
        "print('cnt::',file_cnt, 'file_date::',file_dates)\r\n",
        "print(\"set to string date::\",''.join(set(file_dates))) ## convert set to string\r\n",
        "try: \r\n",
        "    if (file_cnt == 4 and ''.join(set(file_dates)) ==  today_dt):\r\n",
        "        print('count is 4 and all dates belong to Today - continue processing')\r\n",
        "    else:\r\n",
        "        raise InvalidInputError(\"Incorrect date or # of files\") \r\n",
        "except Exception as e:\r\n",
        "    print(\"Error::\", str(e))\r\n",
        "    raise                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Move the data to Gold layer - original code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_raw = spark.read.format('csv')\\\r\n",
        "       .option(\"header\", \"false\")\\\r\n",
        "       .schema(schema)\\\r\n",
        "       .load(f\"{raw_adls_path}AS400/INVDATA/\")\r\n",
        "df = df_raw.filter('SnapshotDate is not null').selectExpr(\"Warehouse\"\r\n",
        ",\"Plant\"\r\n",
        ",\"CartonNbr\" \r\n",
        ",\"PoNbr\" \r\n",
        ",'substring(''PoNbr'',1,10) as PurchasingDocumentNumber'\r\n",
        ",'substring(''PoNbr'',11,5) as PurchasingDocumentItem'\r\n",
        ",\"PoLine\" \r\n",
        ",\"trim(Material) as Material\" \r\n",
        ",\"Size\" \r\n",
        ",\"Width\" \r\n",
        ",\"UPC\" \r\n",
        ",\"CaseQuantity\"\r\n",
        ",\"to_date(cast(DateReceived as string),'yyyyMMdd') as DateReceived\"\r\n",
        ",\"InventoryLockCode\" \r\n",
        ",\"InventoryLockCode2\" \r\n",
        ",\"InventoryLockCode3\" \r\n",
        ",\"InventoryLockCode4\" \r\n",
        ",\"InventoryLockCode5\" \r\n",
        ",\"to_date(cast(SnapshotDate as string),'yyyyMMdd') as SnapshotDate\" \r\n",
        ")\r\n",
        "\r\n",
        "df.repartition('SnapshotDate')\\\r\n",
        "    .write.format(\"delta\")\\\r\n",
        "    .mode(\"overwrite\")\\\r\n",
        "    .option(\"path\",f\"{gold_adls_path}AS400/INVDATA/\")\\\r\n",
        "    .option(\"replaceWhere\", f\"SnapshotDate='{today_dt}'\")\\\r\n",
        "    .option(\"mergeSchema\", \"true\")\\\r\n",
        "    .partitionBy('SnapshotDate')\\\r\n",
        "    .saveAsTable('lakedb_gold.pfas_snapshotdata')\r\n",
        "    \r\n",
        "if file_cnt ==4:\r\n",
        "  for j in mssparkutils.fs.ls(f\"{raw_adls_path}AS400/INVDATA\"):\r\n",
        "    if j.size>0:  \r\n",
        "      print(f'moving ', j.name, ' to archive' )\r\n",
        "      mssparkutils.fs.mv(f\"{raw_adls_path}AS400/INVDATA/{j.name}\", f\"{raw_adls_path}AS400/INVDATA/archive/{j.name}\",overwrite=True)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# move material look up file to gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "lkp_up_schema = StructType([\r\n",
        "    StructField(\"WWWSeason\", StringType(), True),\r\n",
        "    StructField(\"Brand\", StringType(), True),\r\n",
        "    StructField(\"VendorNumber\", StringType(), True),\r\n",
        "    StructField(\"Vendor\", StringType(), True),\r\n",
        "    StructField(\"Pattern\", StringType(), True),\r\n",
        "    StructField(\"Material\", StringType(), True),\r\n",
        "    StructField(\"Pairs\", StringType(), True),\r\n",
        "    StructField(\"CurrentXFDate\", StringType(), True),\r\n",
        "    StructField(\"SAPPO\", StringType(), True),\r\n",
        "    StructField(\"SAPPOItem\", StringType(), True),\r\n",
        "    StructField(\"CorporateRegion\", StringType(), True),\r\n",
        "    StructField(\"PriorToF23orders\", StringType(), True),\r\n",
        "    StructField(\"PriorToF24Gore\", StringType(), True),\r\n",
        "    StructField(\"YKKZippersUsed\", StringType(), True),\r\n",
        "    StructField(\"PriorToS25BOA\", StringType(), True),\r\n",
        "    StructField(\"Yokota1stOrdersinBD\", StringType(), True),\r\n",
        "    StructField(\"AgentProduct\", StringType(), True),\r\n",
        "    StructField(\"PFASUNDER50PPMShipment\", StringType(), True),\r\n",
        "    StructField(\"VibranOutsoleUsed\", StringType(), True),\r\n",
        "    StructField(\"PFASUNDER20PPMShipment\", StringType(), True),\r\n",
        "    StructField(\"PFASUNDER20to50PPMShipment\", StringType(), True),\r\n",
        "    StructField(\"WpOrGoreTexCertified\", StringType(), True),\r\n",
        "    StructField(\"NonZSKUs\", StringType(), True),\r\n",
        "    StructField(\"PocGtnReceiptDate\", StringType(), True),\r\n",
        "    StructField(\"FactoryGroup\", StringType(), True),\r\n",
        "])\r\n",
        "\r\n",
        "df_lkp = spark.read.format('csv')\\\r\n",
        "       .option(\"header\", \"true\")\\\r\n",
        "       .schema(lkp_up_schema)\\\r\n",
        "       .load(f\"{raw_adls_path}AS400/PFAS_Materials_Flag.csv\")\r\n",
        "\r\n",
        "#print(df_lkp.dtypes) \r\n",
        "display(df_lkp) \r\n",
        "df_lkp.write.format(\"delta\")\\\r\n",
        "    .mode(\"overwrite\")\\\r\n",
        "    .option(\"path\",f\"{gold_adls_path}AS400/po_lkp\")\\\r\n",
        "    .option(\"mergeSchema\", \"true\")\\\r\n",
        "    .saveAsTable('lakedb_gold.pfas_po_lkp')\r\n",
        "\r\n",
        "df_lkp = spark.read.format('csv')\\\r\n",
        "       .option(\"header\", \"true\")\\\r\n",
        "       .schema(lkp_up_schema)\\\r\n",
        "       .load(f\"{raw_adls_path}AS400/PFAS_Materials_Flag.csv\")   \r\n",
        "\r\n",
        "relabel_schema = StructType([\r\n",
        "    StructField(\"Brand\", StringType(), True),    \r\n",
        "    StructField(\"Material\", StringType(), True),\r\n",
        "    StructField(\"IsRelabelingRequired\", StringType(), True),\r\n",
        "    StructField(\"NewMaterialCreated\", StringType(), True),\r\n",
        "    StructField(\"NewMaterialNumber\", StringType(), True),\r\n",
        "    StructField(\"Status\", StringType(), True)\r\n",
        "])\r\n",
        "\r\n",
        "df_relabel = spark.read.format('csv')\\\r\n",
        "       .option(\"header\", \"true\")\\\r\n",
        "       .schema(relabel_schema)\\\r\n",
        "       .load(f\"{raw_adls_path}AS400/PFASMaterialRelabel.csv\")\r\n",
        "\r\n",
        "#print(df_lkp.dtypes) \r\n",
        "display(df_relabel) \r\n",
        "df_relabel.write.format(\"delta\")\\\r\n",
        "    .mode(\"overwrite\")\\\r\n",
        "    .option(\"path\",f\"{gold_adls_path}AS400/material_relabel\")\\\r\n",
        "    .option(\"mergeSchema\", \"true\")\\\r\n",
        "    .saveAsTable('lakedb_gold.pfas_material_relabel')        \r\n",
        "     \r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Validate the end result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "--drop table lakedb_gold.pfas_snapshotdata;\r\n",
        "select * from lakedb_gold.pfas_snapshotdata;\r\n",
        "-- where snapshotdate = date_format(CURRENT_DATE,'yyyy-MM-dd')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Check the raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "spark.sql(f\"create table if not exists raw.pfs_raw_data USING CSV LOCATION '{raw_adls_path}AS400/INVDATA/archive'\") \r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "select * from raw.pfs_raw_data;"
      ]
    }
  ]
}