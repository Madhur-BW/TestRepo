{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Read sftp environment and copy to raw\n",
        "from datetime import *\n",
        "from pyspark.sql.functions import *\n",
        "import delta #from delta import *\n",
        "import requests, json\n",
        "import azure.storage.blob #from azure.storage.blob import BlobServiceClient\n",
        "import notebookutils\n",
        "from pyspark.sql import Window, DataFrame\n",
        "from pyspark.sql.types import * "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Added 2025-09-11 KETTNECH to resolve production date issue\n",
        "spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
        "spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Get the current files from RAW container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "daily_files = []\n",
        "#prefix = f\"PIM_AkeneoExport_Barcode-UPC_2025-06-17\"\n",
        "previous_day = (date.today() - timedelta(days=1))\n",
        "prefix = f\"PIM_Akeneo_Full_Export_Limited_Fields_{str(previous_day.strftime('%Y-%m-%d'))}\"\n",
        "\n",
        "files = mssparkutils.fs.ls(f\"{raw_adls_path}Akeneo/\")\n",
        "\n",
        "daily_files = [file for file in files if file.name.startswith(prefix)]\n",
        "\n",
        "# Check if there are any files to move\n",
        "if len(daily_files) == 0 and (env_var == env_dict['prod'] or env_var == env_dict['prod_backup']):\n",
        "    # Warn there might be an issue:\n",
        "    print(\"No files found to move. Ending the job and sending notification.\")\n",
        "    response = requests.post(\n",
        "        'https://prod-85.eastus.logic.azure.com:443/workflows/7367758ef3da4d76b4e64670220d6135/triggers/manual/paths/invoke?api-version=2016-10-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=3qXzAxaCqincRFQ358oeDwq_SGn5vgzaNgW26QxUDMs',\n",
        "        '{\"email_to\": \"ededl@wwgroups.net\", \"email_subject\": \"No Akeneo Data Available to Process\", \"email_body\": \"Did not find any file to process for daily Akeneo data in RAW/Akeneo.\", \"email_from\": \"AzureSynapse@wwwinc.com\"}',\n",
        "        headers={\"Content-Type\": \"application/json\"}\n",
        "    )\n",
        "    # Exit the notebook when no files are available\n",
        "    notebookutils.mssparkutils.notebook.exit(0)\n",
        "elif len(daily_files) == 0 and (env_var == env_dict['dev'] or env_var == env_dict['test']):\n",
        "    # For Dev and Test environments\n",
        "    print(\"No files found in Dev/Test environment. Skipping file processing.\")\n",
        "    # Exit the notebook when no files are available\n",
        "    notebookutils.mssparkutils.notebook.exit(0)\n",
        "else:\n",
        "    # Proceed if there are files\n",
        "    for file in daily_files:\n",
        "        print(f\"Processing File: {file.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Process Akeneo CSV files from RAW to BRONZE (OVERWRITE only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Get the  latest file raw, overwrite the delta table (full export)\n",
        "akeneo_raw_df = spark.read.option(\"header\", \"true\").option(\"quote\", \"\\\"\").option(\"multiLine\", \"true\").csv(f\"{raw_adls_path}Akeneo/{prefix}*.csv\", header=True, inferSchema=True, sep=\";\").withColumn(\"source_file_path\", input_file_name()).distinct()\n",
        "if \"CategoryName\" in akeneo_raw_df.columns:\n",
        "    # To accomodate this logic from Akeneo: CategoryName is coming from PLM and is technically the sub-category, \n",
        "    #       Category_Name is generated by a rule in Akeneo and used for display purposes in the product grid to know which Akeneo main category they are assigned belong to.  \n",
        "    #       Category_Name is assigned based on the category code from PLM.\n",
        "    # Renaming to SubCategoryName since there's already existing SubCategory in the schema\n",
        "    akeneo_raw_df = akeneo_raw_df.withColumnRenamed(\"CategoryName\", \"Sub_Category_Name\")\n",
        "akeneo_raw_df.write.option(\"overwriteSchema\", \"true\").mode(\"overwrite\").format(\"delta\").save(f'{bronze_adls_path}Akeneo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Perform Melting using Transpose, Stack and Explode functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def melt_data(df, new_col_name, cols_to_transpose):\n",
        "    # Create an array of structs\n",
        "    # Each struct will contain the column name and its value\n",
        "    stacked_df = akeneo_raw_df.select(\"Barcode\", array(*[struct(lit(c).alias(\"key\"), akeneo_raw_df[c].alias(new_col_name)) for c in cols_to_transpose]).alias(\"stacked_data\"))\n",
        "\n",
        "    #Explose the array to create new rows\n",
        "    unpivoted_df = stacked_df.select(\"Barcode\", explode(stacked_df[\"stacked_data\"]).alias(\"data\"))\n",
        "\n",
        "    #Extract key and value from the struct\n",
        "    final_df = unpivoted_df.select(\"Barcode\", \"data.key\", f\"data.{new_col_name}\")\n",
        "    final_df = final_df.withColumn(\"Region\", when(final_df.key.contains(\"US\"), \"US\").when(final_df.key.contains(\"GB\"), \"GB\").when(final_df.key.contains(\"IE\"), \"IE\").otherwise(\"SB\"))\n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Get the Regional Columns [US, IE, GB, SB]\n",
        "regional_columns = [col for col in akeneo_raw_df.columns if (\"-en_\" in col) ]\n",
        "\n",
        "# Dictionary that will contain the new column name and list of related column per region\n",
        "merge_regional_columns = {}\n",
        "\n",
        "# list all the columns to drop from the raw dataframe\n",
        "cols_to_drop = [] #Add CategoryName due to duplicate column issue from RAW file\n",
        "\n",
        "for col in regional_columns:\n",
        "    new_name = col.replace(\"-en_GB\", \"\").replace(\"-en_IE\", \"\").replace(\"-en_US\", \"\").replace(\"-SB\", \"\")\n",
        "    if new_name in merge_regional_columns:\n",
        "        item = merge_regional_columns.get(new_name)\n",
        "        item.append(col)\n",
        "    else:\n",
        "        merge_regional_columns[new_name] = [col]\n",
        "    cols_to_drop.append(col)\n",
        "\n",
        "# Create a dataframe from the list of barcode\n",
        "stacked_df = akeneo_raw_df.select(\"Barcode\").distinct()\n",
        "# Initialize a column for ALL region\n",
        "stacked_df = stacked_df.withColumn(\"Region\", explode(lit(['US', 'IE', 'GB'])))\n",
        "\n",
        "for key in merge_regional_columns:\n",
        "    cols_to_transpose = merge_regional_columns.get(key)\n",
        "    # Melt the columns into rows\n",
        "    new_df = melt_data(akeneo_raw_df.select(\"Barcode\", *cols_to_transpose), key, cols_to_transpose)\n",
        "    # Join stacked data to the existing barcode list\n",
        "    stacked_df = stacked_df.join(new_df.select(\"Barcode\", \"Region\", key).dropna().distinct(), [\"Barcode\",\"Region\"], \"left\")\n",
        "\n",
        "normalized_df = stacked_df.dropna(how=\"all\", subset=list(merge_regional_columns.keys()))\n",
        "#normalized_df.orderBy(\"BarCode\", \"Region\").show(100, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Merged normalized_df to raw data\n",
        "akeneo_filtered_df = akeneo_raw_df.drop(*cols_to_drop)\n",
        "akeneo_processed_df = akeneo_filtered_df.join(normalized_df, \"Barcode\", \"inner\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# TODO: Normalized data type of all columns before storing to silver layer\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Convert date and timestamp columns\n",
        "normalized_akeneo_df = akeneo_processed_df\\\n",
        "    .withColumn(\"created\", col(\"created\").cast(TimestampType()))\\\n",
        "    .withColumn(\"updated\", col(\"created\").cast(TimestampType()))\\\n",
        "    .withColumn(\"Barcode\", col(\"Barcode\").cast(StringType()))\\\n",
        "    .withColumn(\"Coming_Soon_Ecommerce\", col(\"Coming_Soon_Ecommerce\").cast(StringType()))\\\n",
        "    .withColumn(\"John_Lewis_Exclusive\", col(\"John_Lewis_Exclusive\").cast(StringType()))\\\n",
        "    .withColumn(\"Latest_Launch_Date_Ecommerce\", to_date(col(\"Latest_Launch_Date_Ecommerce\"), \"MM/dd/yyyy\"))\\\n",
        "    .withColumn(\"Online_From_Barcode_Ecommerce\", to_date(col(\"Online_From_Barcode_Ecommerce\"), \"MM/dd/yyyy\"))\\\n",
        "    .withColumn(\"Online_To_Barcode_Ecommerce\", to_date(col(\"Online_To_Barcode_Ecommerce\"), \"MM/dd/yyyy\"))\\\n",
        "    .withColumn(\"Online_From_Ecommerce\", to_date(col(\"Online_From_Ecommerce\"), \"MM/dd/yyyy\"))\\\n",
        "    .withColumn(\"Online_To_Ecommerce\", to_date(col(\"Online_To_Ecommerce\"), \"MM/dd/yyyy\"))\\\n",
        "##    .withColumn(\"Web_Base_Price-USD\", col(\"Web_Base_Price-USD\").cast(DecimalType()))\\\n",
        "##    .withColumn(\"Web_Base_Price-GBP\", col(\"Web_Base_Price-GBP\").cast(DecimalType()))\\\n",
        "##    .withColumn(\"Web_Base_Price-EUR\", col(\"Web_Base_Price-EUR\").cast(DecimalType()))\\\n",
        "##    .withColumn(\"Web_Sale_Price-USD\", col(\"Web_Sale_Price-USD\").cast(DecimalType()))\\\n",
        "##   .withColumn(\"Web_Sale_Price-GBP\", col(\"Web_Sale_Price-GBP\").cast(DecimalType()))\\\n",
        "##    .withColumn(\"Web_Sale_Price-EUR\", col(\"Web_Sale_Price-EUR\").cast(DecimalType()))\\\n",
        "##    .withColumn(\"Weight\", col(\"Weight\").cast(DoubleType()))\\\n",
        "##    .withColumn(\"Available_Flag_Ecommerce\", col(\"Available_Flag_Ecommerce\").cast(IntegerType()))\\\n",
        "\n",
        "# Apply PascalCase naming convention\n",
        "columns_list = normalized_akeneo_df.columns\n",
        "akeneo_silver_naming = {}\n",
        "hashdiff_cols = []\n",
        "for col in columns_list:\n",
        "    new_name = str(col).replace(\"-\", \" \").replace(\"_\", \" \").title().replace(\" \", \"\")\n",
        "    akeneo_silver_naming[col] = new_name\n",
        "    if new_name != \"Created\":\n",
        "        hashdiff_cols.append(new_name)\n",
        "\n",
        "normalized_akeneo_df = normalized_akeneo_df.withColumnsRenamed(akeneo_silver_naming).distinct()\n",
        "\n",
        "# Calculate Derived Columns\n",
        "# IS_ONLINE_IN_DATE - OnlineFlagEcommerce = 1 and OnlineFromEcommerce and OnlineToEcommerce dates are within current date\n",
        "# IS_ONLINE_IN_DATE_BARCODE - OnlineFlagBarcode = 1 and OnlineFromBarcodeEcommerce and OnlineToBarcodeEcommerce dates are within current date\n",
        "normalized_akeneo_df = normalized_akeneo_df.withColumn(\"IsOnlineInDate\", when((trim(normalized_akeneo_df.OnlineFlagEcommerce)=='1' ) & (lit(current_date()).between(normalized_akeneo_df.OnlineFromEcommerce, normalized_akeneo_df.OnlineToEcommerce)), 1).otherwise(0))\n",
        "normalized_akeneo_df = normalized_akeneo_df.withColumn(\"IsOnlineInDateBarcode\", when((trim(normalized_akeneo_df.OnlineFlagBarcode)=='1' ) & (lit(current_date()).between(normalized_akeneo_df.OnlineFromBarcodeEcommerce, normalized_akeneo_df.OnlineToBarcodeEcommerce)), 1).otherwise(0))\n",
        "\n",
        "hashdiff_cols = normalized_akeneo_df.columns\n",
        "normalized_akeneo_df = normalized_akeneo_df.withColumn(\"ProductHashDiff\", md5(concat_ws(\"||\",*hashdiff_cols)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Append Historical data to Silver Delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Silver layer: We need to keep the historical delta table for every snapshot (date column) of 1st day of the month\n",
        "firstOfMon = date(previous_day.year, previous_day.month, 1)\n",
        "normalized_akeneo_df = normalized_akeneo_df.withColumn(\"SnapshotDate\", lit(firstOfMon))\n",
        "\n",
        "## Append as delta table\n",
        "if delta.DeltaTable.isDeltaTable(spark, f'{silver_adls_path}AkeneoHistory'):\n",
        "    if previous_day == firstOfMon:\n",
        "        print(f\"First Day of Month: {firstOfMon}\")\n",
        "        normalized_akeneo_df.write.option(\"mergeSchema\", \"true\").mode(\"append\").format(\"delta\").save(f'{silver_adls_path}AkeneoHistory')\n",
        "else:\n",
        "    normalized_akeneo_df.write.option(\"overwriteSchema\", \"true\").mode(\"overwrite\").format(\"delta\").save(f'{silver_adls_path}AkeneoHistory')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Overwrite Gold Delta (Latest record only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "normalized_akeneo_df.write.option(\"overwriteSchema\", \"true\").mode(\"overwrite\").format(\"delta\").save(f'{gold_adls_path}Akeneo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "notebookutils.mssparkutils.notebook.exit(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Data Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "normalized_akeneo_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "spark.read.format(\"delta\").load(f'{gold_adls_path}Akeneo').createOrReplaceTempView(\"vwAkeneoGold\")\n",
        "spark.read.format(\"delta\").load(f'{bronze_adls_path}Akeneo').createOrReplaceTempView(\"vwAkeneoBronze\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def manual_delta_column_renamed(column_mapping, delta_table_path):\n",
        "    df = spark.read.format(\"delta\").load(delta_table_path)\n",
        "    # Rename the column\n",
        "    for old_name, new_name in column_mapping.items():\n",
        "        if old_name in df.columns:  # Check if the column exists\n",
        "            df = df.withColumnRenamed(old_name, new_name)\n",
        "\n",
        "    df.write.format(\"delta\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .option(\"overwriteSchema\", \"true\") \\\n",
        "        .save(delta_table_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# manual column name change for bronze table\n",
        "column_rename_map = {\n",
        "    \"Country_Of_Origin-en_GB-SB_Ecommerce\": \"Country_Of_Origin-en_GB\",\n",
        "    \"Country_Of_Origin-en_IE-SB_Ecommerce\": \"Country_Of_Origin-en_IE\",\n",
        "    \"Country_Of_Origin-en_US-SB_Ecommerce\": \"Country_Of_Origin-en_US\",\n",
        "    \"HTS_Code\": \"hsCode\",\n",
        "    \"Unit_of_Measurement\": \"unitMeasure\",\n",
        "    \"Weight-unit\": \"weight_uom\"\n",
        "        # Add more mappings as needed\n",
        "}\n",
        "\n",
        "manual_delta_column_renamed(column_rename_map, f'{bronze_adls_path}Akeneo')\n",
        "\n",
        "print(f\"Columns {column_rename_map} in Delta table at '{bronze_adls_path}Akeneo' has been ranamed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# manual column name change for bronze table\n",
        "column_rename_map = {\n",
        "    \"CountryOfOriginEcommerce\": \"CountryOfOrigin\",\n",
        "    \"HtsCode\": \"Hscode\",\n",
        "    \"UnitOfMeasurement\": \"Unitmeasure\",\n",
        "    \"WeightUnit\": \"WeightUom\"\n",
        "        # Add more mappings as needed\n",
        "}\n",
        "\n",
        "manual_delta_column_renamed(column_rename_map, f'{gold_adls_path}Akeneo')\n",
        "\n",
        "print(f\"Columns {column_rename_map} in Delta table at '{gold_adls_path}Akeneo' has been ranamed.\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Manually add new columns to delta table\n",
        "df = spark.read.format(\"delta\").load(f'{gold_adls_path}Akeneo')\n",
        "new_df = df.withColumn(\"Badgecode\", lit(None).cast(StringType()))\n",
        "new_df = new_df.withColumn(\"ProductMedia\", lit(None).cast(StringType()))\n",
        "new_df.write.format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"mergeSchema\", \"true\") \\\n",
        "    .save(f'{gold_adls_path}Akeneo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### convert silver table data type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format(\"delta\").load(f'{silver_adls_path}AkeneoHistory')\n",
        "df2 = df.withColumn(\"OriginalLaunchDateSbEcommerce\", to_date(\"OriginalLaunchDateSbEcommerce\",\"yyyy-MM-dd\")) \\\n",
        "        .withColumn(\"Weight\", df[\"Weight\"].cast(\"double\")) \\\n",
        "        .withColumn(\"OnlineFlagEcommerce\", df[\"OnlineFlagEcommerce\"].cast(\"int\")) \\\n",
        "        .withColumn(\"OnlineFlagBarcode\", df[\"OnlineFlagBarcode\"].cast(\"int\")) \\\n",
        "        .withColumn(\"DroppedEcommerce\", df[\"DroppedEcommerce\"].cast(\"int\")) \\\n",
        "        .withColumn(\"SearchableEcommerce\", df[\"SearchableEcommerce\"].cast(\"int\")) \n",
        "df2.write.option(\"overwriteSchema\", \"true\").mode(\"overwrite\").format(\"delta\").save(f'{silver_adls_path}AkeneoHistory')\n",
        "#df2.printSchema()"
      ]
    }
  ]
}