{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Read new traffic data via API, store in raw, process as Delta table\r\n",
        "from delta import *\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType, LongType, BinaryType\r\n",
        "import datetime\r\n",
        "from datetime import timedelta\r\n",
        "import requests, json\r\n",
        "from azure.storage.blob import BlobServiceClient\r\n",
        "import base64\r\n",
        "import delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Get the Auth Bearer token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Obtain the auth token using Client Credntials and OAuth 2\r\n",
        "# https://developer.salesforce.com/docs/commerce/b2c-commerce/references/b2c-commerce-ocapi/oauth.html\r\n",
        "# Making this a function since we would need to call it to get a new token if our orig token expires (3600s)\r\n",
        "\r\n",
        "def get_access_token():\r\n",
        "    try:\r\n",
        "        # Only one token endpoint for all environments\r\n",
        "        token_url = 'https://account.demandware.com/dwsso/oauth2/access_token'\r\n",
        "        # Using a client credentials grant with the clientId=username and clientSecret=password. (The word doc in LastPass says \"Client Credentials\")\r\n",
        "        # Convert the username and password, combine with a colon, and convert into a base64 encoded value.\r\n",
        "        username = '498a02fc-e957-4bfa-bbed-3c771d9a9c60'\r\n",
        "        password = mssparkutils.credentials.getSecret(kv_name, 'emwbis-client-secret', 'ls_kv_adap')\r\n",
        "        client_credentials = f\"{username}:{password}\"\r\n",
        "        creds_base64 = base64.b64encode(client_credentials.encode(\"utf-8\")).decode(\"utf-8\")\r\n",
        "\r\n",
        "        headers = {\r\n",
        "            'Host': 'account.demandware.com',\r\n",
        "            'Authorization': f\"Basic {creds_base64}\",\r\n",
        "            'Content-Type': 'application/x-www-form-urlencoded'\r\n",
        "        }\r\n",
        "\r\n",
        "        token_body = {\r\n",
        "            'grant_type': 'client_credentials'\r\n",
        "        }\r\n",
        "\r\n",
        "        response = requests.post(url=token_url, headers=headers, data=token_body) \r\n",
        "        response_json = json.loads(response.text)\r\n",
        "        #print(response_json) \r\n",
        "        access_token = response_json[\"access_token\"] \r\n",
        "        print('Acquired access token', access_token[:20],'...from',token_url)\r\n",
        "\r\n",
        "        return access_token\r\n",
        "    except Exception as e:\r\n",
        "        print(\"error=\",str(e))\r\n",
        "        raise Exception(\"Could not obtain Bearer token.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Make the API Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Make a Body template\r\n",
        "body = {\r\n",
        "    \"query\": {\r\n",
        "        \"bool_query\": {\r\n",
        "            \"must\": [\r\n",
        "                {\r\n",
        "                    \"term_query\": {\r\n",
        "                        \"fields\": [\r\n",
        "                            \"site_id\"\r\n",
        "                        ],\r\n",
        "                        \"operator\": \"is\",\r\n",
        "                        \"values\": [\r\n",
        "                            \"<SiteID>\"\r\n",
        "                        ]\r\n",
        "                    }\r\n",
        "                },\r\n",
        "                {\r\n",
        "                    \"term_query\": {\r\n",
        "                        \"fields\": [\r\n",
        "                            \"last_modified\"\r\n",
        "                        ],\r\n",
        "                        \"operator\": \"greater\",\r\n",
        "                        \"values\": [\r\n",
        "                            \"<LastModified>\"\r\n",
        "                        ]\r\n",
        "                    }\r\n",
        "                }\r\n",
        "            ]\r\n",
        "        }\r\n",
        "    },\r\n",
        "    \"count\": 200,\r\n",
        "    \"start\": 50,\r\n",
        "    \"select\": \"(**)\"\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# A helper function (from the orig source)\r\n",
        "def format_emwbis_timestamp(dt: datetime.datetime) -> str:\r\n",
        "    \"\"\"\r\n",
        "    Formats a datetime for use with the EMWBIS API\r\n",
        "    Args:\r\n",
        "        dt: a datetime object\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        formatted timestamp string\r\n",
        "    \"\"\"\r\n",
        "    return f\"{dt.isoformat(timespec='milliseconds')}Z\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Make API calls \r\n",
        "\r\n",
        "# Get a bearer token\r\n",
        "access_token = get_access_token()\r\n",
        "\r\n",
        "headers = {\r\n",
        "    \"Authorization\": f\"Bearer {access_token}\", \r\n",
        "    \"Content-Type\": \"application/json\"\r\n",
        "}\r\n",
        "\r\n",
        "# The API URL\r\n",
        "if env_var == env_dict['dev']:\r\n",
        "    api_url = 'https://development.sweatybetty.com/s/-/dw/data/v22_6/custom_objects_search/EmailMeWhenAvailable'\r\n",
        "    # For prod testing in dev (comment out when done testing)\r\n",
        "    # Note that one test had about 800 items total, so testing prod data in dev appears to be harmless.\r\n",
        "    # api_url = 'https://www.sweatybetty.com/s/-/dw/data/v22_6/custom_objects_search/EmailMeWhenAvailable'\r\n",
        "elif env_var in [env_dict['test'], env_dict['prod']]:\r\n",
        "    api_url = 'https://www.sweatybetty.com/s/-/dw/data/v22_6/custom_objects_search/EmailMeWhenAvailable'\r\n",
        "else:\r\n",
        "    raise Exception('Failed to set API url.')\r\n",
        "\r\n",
        "# We will make paginated API calls (max 200 items at a time) for each siteID in our list of valid Site IDs. Valid site IDs are SB, SB-EU, SB-AU, SB-US\r\n",
        "# This code handles pagination and the potential for an access token expiration.\r\n",
        "\r\n",
        "# For now, the last modified timestamp will look back two days. May consider a watermark store design down the road.\r\n",
        "two_days_ago_midnight = (datetime.datetime.now() - datetime.timedelta(days=2)).replace(hour=0, minute=0, second=0, microsecond=0)\r\n",
        "# Format it as An ISO 8601 datetime string format\r\n",
        "# Example: last_modifed = '2025-02-12T00:00:00Z'\r\n",
        "last_modifed = format_emwbis_timestamp(two_days_ago_midnight)\r\n",
        " \r\n",
        "\r\n",
        "# We concatenate ALL sites results into a single list var, results\r\n",
        "results = []\r\n",
        "\r\n",
        "sites = ['SB', 'SB-EU', 'SB-AU', 'SB-US']\r\n",
        "for site in sites:\r\n",
        "    print(f\"processing site: {site}\")\r\n",
        "\r\n",
        "    # Replace API Body Parameters\r\n",
        "    # <SiteID>\r\n",
        "    # Element 0 of query.bool_query.must\r\n",
        "    body[\"query\"][\"bool_query\"][\"must\"][0][\"term_query\"][\"values\"] = [site]\r\n",
        "    # <LastModified> - e.g. 2022-07-29T13:12:11Z (IOS 8601)\r\n",
        "    # Element 1 for query.bool_query.must\r\n",
        "    body[\"query\"][\"bool_query\"][\"must\"][1][\"term_query\"][\"values\"] = [last_modifed]\r\n",
        "    # <Start> and <Count> for pagination? Start with 0 and Count 200, then start with 200\r\n",
        "    # https://developer.salesforce.com/docs/commerce/b2c-commerce/references/b2c-commerce-ocapi/pagination.html\r\n",
        "    page_item_count = 200\r\n",
        "    body[\"count\"] = page_item_count\r\n",
        "    page_start_value = 0\r\n",
        "    body[\"start\"] = page_start_value\r\n",
        "\r\n",
        "    print(body)\r\n",
        "\r\n",
        "    # Now get chunks of 200 rows at a time.\r\n",
        "    # Loop to get more pages until we have no more to process\r\n",
        "    while True:\r\n",
        "        print(f\"Requesting next {page_item_count} items starting at index {page_start_value} from {api_url}\")\r\n",
        "        try:\r\n",
        "            # The API call\r\n",
        "            #response = requests.post(url=api_url, data=body, headers=headers)\r\n",
        "            response = requests.post(url=api_url, data=json.dumps(body), headers=headers)\r\n",
        "\r\n",
        "            # Handle various response status_codes\r\n",
        "            # Definitiely need to handle token expired error, get a new token and try again.\r\n",
        "            status_code = response.status_code\r\n",
        "            print(f\"response status_code: {status_code}\")\r\n",
        "            if status_code < 200:\r\n",
        "                # 100s - request received, but no response\r\n",
        "                print(\"status 100s\")\r\n",
        "                raise Exception(f\"Unhandled response.status_code: {status_code}\")\r\n",
        "            elif status_code < 300:\r\n",
        "                # 200s - request successfully processed (could be no items returned, tho?).  Fo\r\n",
        "                print(\"status 200s\")\r\n",
        "                # We want to just fall through to the next lines of code.\r\n",
        "            elif status_code < 400:\r\n",
        "                # 300s - resource is in a different location - redirect\r\n",
        "                print(\"status 300s\")\r\n",
        "                raise Exception(f\"Unhandled response.status_code: {status_code}\")\r\n",
        "            elif status_code < 500:\r\n",
        "                # 400s - client error\r\n",
        "                print(\"status 400s\")\r\n",
        "                # If it's a 401, try to get a new access token and try this page again.\r\n",
        "                if status_code == 401:\r\n",
        "                    access_token = get_access_token()\r\n",
        "                    headers = {\r\n",
        "                        \"Authorization\": f\"Bearer {access_token}\", \r\n",
        "                        \"Content-Type\": \"application/json\"\r\n",
        "                    }\r\n",
        "                    continue\r\n",
        "                else:\r\n",
        "                    print(response.text)\r\n",
        "                    raise Exception(f\"Unhandled client side error status code {status_code} received.\")\r\n",
        "            else:\r\n",
        "                # 500s - server side error\r\n",
        "                print(\"status 500s\")\r\n",
        "                print(response.text)\r\n",
        "                raise Exception('500s Server Side error status code received')\r\n",
        "\r\n",
        "            response_data = response.json()\r\n",
        "            response_item_count = response_data['count']\r\n",
        "            response_item_total = response_data['total']\r\n",
        "            \r\n",
        "            print(f\"response_data count: {response_data['count']}\")\r\n",
        "            print(f\"response_data start: {response_data['start']}\")\r\n",
        "            print(f\"response_data total: {response_data['total']}\")\r\n",
        "\r\n",
        "            #print(response.text)\r\n",
        "            \r\n",
        "            # Concatenate/Collect each page's hits collection/array/list\r\n",
        "            # We have PII here - customer email address (c_emailID) - Chad says to just ingest into raw and he will lock down the folder.\r\n",
        "            # results += response_data.get(\"hits\", []) # ORIGINAL COMMAND\r\n",
        "            response_data_hits = response_data.get(\"hits\", [])\r\n",
        "\r\n",
        "            for hit in response_data_hits:\r\n",
        "                hit[\"site_id\"] = site\r\n",
        "            \r\n",
        "            results += response_data_hits\r\n",
        "\r\n",
        "            # See if we can end the processing for this site.\r\n",
        "            # This assumes that response_item_count is the count of items in just this most recent call\r\n",
        "            if response_item_count < page_item_count:\r\n",
        "                # Then we didn't even have a full page of hits items - we can exit the while\r\n",
        "                break\r\n",
        "            else:\r\n",
        "                # we had a full page. Set the new page start position and keep collecting pages, 200 at a time.\r\n",
        "                page_start_value += page_item_count\r\n",
        "                body[\"start\"] = page_start_value\r\n",
        "\r\n",
        "        except:\r\n",
        "            print(\"error=\", str(e))\r\n",
        "            raise Exception('Could not obtain API data.')\r\n",
        "            # Break out of processing this site\r\n",
        "            break\r\n",
        "\r\n",
        "    # end while *********************************\r\n",
        "# end for each site\r\n",
        "print(\"-----------------------------------------------\")\r\n",
        "print(f\"results count: {len(results)}\")\r\n",
        "print(f\"SUCCESS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Save to raw zone in Data Lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# #Save formatted_response as raw file\r\n",
        "account_key = mssparkutils.credentials.getSecret(kv_name, 'storage-key', 'ls_kv_adap')\r\n",
        "blob_service_client = BlobServiceClient(account_url=blob_adls_path, credential=account_key)\r\n",
        "container_client = blob_service_client.get_container_client(\"raw\")\r\n",
        "fileprefix = datetime.datetime.now().strftime(\"%Y/%m/%d/%H%M%S\")\r\n",
        "# blob_name = f\"StoreTech/Traffic/{fileprefix}.json\"\r\n",
        "blob_name = f\"EMWBIS/{fileprefix}.json\"\r\n",
        "blob_client = container_client.get_blob_client(blob_name)\r\n",
        "# Take our results from previous cell and make it JSON for the BLOB upload\r\n",
        "formatted_response = json.dumps(results)\r\n",
        "blob_client.upload_blob(formatted_response, overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### **Copy Raw to Bronze**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Read EMWBIS from raw container\r\n",
        "# FULL LOAD\r\n",
        "# emwbis_raw_df = spark.read.json(f'{raw_adls_path}EMWBIS/*/*/*/*.json') \r\n",
        "\r\n",
        "# DAILY LOAD\r\n",
        "current_date = datetime.datetime.now().strftime(\"%Y/%m/%d\")\r\n",
        "current_date_path = f\"{raw_adls_path}EMWBIS/{current_date}/*.json\"\r\n",
        "emwbis_raw_df = spark.read.json(current_date_path)\r\n",
        "\r\n",
        "# # # TEST DATE\r\n",
        "# current_date = (datetime.datetime.now() - timedelta(days=5)).strftime(\"%Y/%m/%d\")\r\n",
        "# current_date_path = f\"{raw_adls_path}EMWBIS/{current_date}/*.json\"\r\n",
        "# emwbis_raw_df = spark.read.json(current_date_path)\r\n",
        "\r\n",
        "if emwbis_raw_df.count() == 0:\r\n",
        "    print(\"emwbis_raw_df has no rows. Ending notebook run.\")\r\n",
        "    notebookutils.mssparkutils.notebook.exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, BinaryType, DateType, TimestampType, DecimalType, BooleanType\r\n",
        "\r\n",
        "\r\n",
        "# Include File Path\r\n",
        "emwbis_raw_df = emwbis_raw_df.withColumn(\"file_path\", input_file_name()).distinct()\r\n",
        "\r\n",
        "# Extract the date part from the file path\r\n",
        "emwbis_raw_df = emwbis_raw_df.withColumn(\"date_part\", regexp_extract(\"file_path\", r'/(\\d{4}/\\d{2}/\\d{2})/', 1))\r\n",
        "\r\n",
        "# Extract the time part from the file name\r\n",
        "emwbis_raw_df = emwbis_raw_df.withColumn(\"time_part\", regexp_extract(\"file_path\", r'/(\\d{6})\\.json$', 1))\r\n",
        "\r\n",
        "# Combine the date and time parts into a single string\r\n",
        "emwbis_raw_df = emwbis_raw_df.withColumn(\"datetime_string\", concat_ws(\" \", \"date_part\", \"time_part\"))\r\n",
        "\r\n",
        "# Convert the combined string to a timestamp\r\n",
        "emwbis_raw_df = emwbis_raw_df.withColumn(\"timestamp\", to_timestamp(\"datetime_string\", \"yyyy/MM/dd HHmmss\"))\r\n",
        "\r\n",
        "\r\n",
        "# Rearrange fields and rename to PascalCase format\r\n",
        "# Check if 'site_id' column exists in the source DataFrame\r\n",
        "columns = emwbis_raw_df.columns\r\n",
        "if 'site_id' not in columns:\r\n",
        "    emwbis_raw_df = emwbis_raw_df.withColumn('site_id', lit(''))\r\n",
        "\r\n",
        "# Select and cast columns\r\n",
        "emwbis_df = emwbis_raw_df.select(\r\n",
        "    col('_resource_state').alias('ResourceState').cast(StringType()),\r\n",
        "    col('c_emailID').alias('CEmailId').cast(StringType()),\r\n",
        "    col('object_type').alias('ObjectType').cast(StringType()),\r\n",
        "    col('c_productAvailabilityDate').alias('CProductAvailabilityDate').cast(DateType()),\r\n",
        "    col('c_locale').alias('CLocale').cast(StringType()),\r\n",
        "    col('last_modified').alias('LastModified').cast(TimestampType()),\r\n",
        "    col('creation_date').alias('CreationDate').cast(TimestampType()),\r\n",
        "    col('_type').alias('Type').cast(StringType()),\r\n",
        "    col('c_productID').alias('CProductId').cast(StringType()),\r\n",
        "    col('key_property').alias('KeyProperty').cast(StringType()),\r\n",
        "    col('key_value_string').alias('KeyValueString').cast(StringType()),\r\n",
        "    col('site_id').alias('SiteId').cast(StringType()),\r\n",
        "    col(\"timestamp\").alias('IngestedAt').cast(TimestampType()),\r\n",
        "    col('file_path').alias('IngestedFrom').cast(StringType()),\r\n",
        "    col('c_selectedCountry').alias('CSelectedCountry').cast(StringType())\r\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "#Write dataframe to bronze\r\n",
        "if delta.DeltaTable.isDeltaTable(spark, f'{bronze_adls_path}/EMWBIS/DQ_EMWBIS/'):\r\n",
        "    deltaTable = delta.DeltaTable.forPath(spark, f'{bronze_adls_path}/EMWBIS/DQ_EMWBIS/')\r\n",
        "    \r\n",
        "    # Find new rows that are not in the existing table\r\n",
        "    # existing_df = deltaTable.toDF()\r\n",
        "    # new_rows_df = emwbis_df.exceptAll(existing_df)\r\n",
        "\r\n",
        "    max_ingest_date = deltaTable.toDF().agg({\"IngestedAt\": \"max\"}).collect()[0][0]\r\n",
        "    new_rows_df = emwbis_df.filter(col(\"IngestedAt\") > max_ingest_date)\r\n",
        "\r\n",
        "    # Append only new rows to the Delta table\r\n",
        "    new_rows_df.write.format(\"delta\").mode(\"append\").save(f'{bronze_adls_path}/EMWBIS/DQ_EMWBIS/')\r\n",
        "    print('Merged delta table.')\r\n",
        "\r\n",
        "else:\r\n",
        "    emwbis_df.write.option(\"overwriteSchema\", \"true\").mode(\"overwrite\").format(\"delta\").save(f'{bronze_adls_path}/EMWBIS/DQ_EMWBIS/')\r\n",
        "    print('Created delta table.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Bronze to Silver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Read EMWBIS from bronze container\r\n",
        "dq_emwbis = delta.DeltaTable.forPath(spark, f'{bronze_adls_path}/EMWBIS/DQ_EMWBIS/').toDF()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Create Link\r\n",
        "# dv_prep__email_with_customer_key not yet in place. using CEmailId for now as replacement to CustomerKey\r\n",
        "# IS_BC_CUSTOMER_KEY, IS_OPTIMOVE_CUSTOMER_KEY not available\r\n",
        "\r\n",
        "dv_prep_emwbis_lnk = (\r\n",
        "            dq_emwbis.select(dq_emwbis[\"CEmailId\"].alias(\"CustomerKey\"),\r\n",
        "                              dq_emwbis[\"CProductId\"].alias(\"ItemBarcodeKey\"),\r\n",
        "                              \"IngestedAt\",\r\n",
        "                              \"LastModified\")\r\n",
        "            .withColumn(\"LoadDateTime\", current_timestamp())\r\n",
        "            .withColumn(\"RecordSource\", lit('EMWBIS'))\r\n",
        "            )\r\n",
        "\r\n",
        "# Hash CustomerKey+ItemBarcodeKey , CustomerKey, ItemBarcodeKey\r\n",
        "dv_stg_emwbis_lnk = (\r\n",
        "                        dv_prep_emwbis_lnk\r\n",
        "                        .withColumn(\"LnkEmwbisHkey\",\r\n",
        "                              md5(\r\n",
        "                                    when(\r\n",
        "                                    (trim(upper(col(\"CustomerKey\"))) != '') | (trim(upper(col(\"ItemBarcodeKey\"))) != ''),\r\n",
        "                                    concat_ws(\r\n",
        "                                          '||',\r\n",
        "                                          when(trim(upper(col(\"CustomerKey\"))) != '', trim(upper(col(\"CustomerKey\")))).otherwise('^^'),\r\n",
        "                                          when(trim(upper(col(\"ItemBarcodeKey\"))) != '', trim(upper(col(\"ItemBarcodeKey\")))).otherwise('^^')\r\n",
        "                                    )\r\n",
        "                                    ).otherwise(lit('0000000000000000'))\r\n",
        "                              ).cast(BinaryType())\r\n",
        "                        )\r\n",
        "                        .withColumn(\"CustomerHkey\",\r\n",
        "                              md5(\r\n",
        "                                    when(\r\n",
        "                                    trim(upper(col(\"CustomerKey\"))) != '',\r\n",
        "                                    trim(upper(col(\"CustomerKey\")))\r\n",
        "                                    ).otherwise(lit('0000000000000000'))\r\n",
        "                              ).cast(BinaryType())\r\n",
        "                        )\r\n",
        "                        .withColumn(\"ItemBarcodeHkey\",\r\n",
        "                              md5(\r\n",
        "                                    when(\r\n",
        "                                    trim(upper(col(\"ItemBarcodeKey\"))) != '',\r\n",
        "                                    trim(upper(col(\"ItemBarcodeKey\")))\r\n",
        "                                    ).otherwise(lit('0000000000000000'))\r\n",
        "                              ).cast(BinaryType())\r\n",
        "                        )\r\n",
        ")\r\n",
        "\r\n",
        "# Derive Valid Records for Silver\r\n",
        "# Valid Records = NEW + earliest LnkEmwbisHkey based on LoadDateTime and RecordSource + CustomerHkey is not null + ItemBarcodeKey is not null\r\n",
        "\r\n",
        "from pyspark.sql import Window\r\n",
        "window_spec1 = Window.partitionBy(\"LnkEmwbisHkey\").orderBy(\"IngestedAt\", \"LastModified\") # orderBy LoadDateTime replaced to orderBy IngestedAt+LastModified as dv_stg_emwbis_lnk is not written as table\r\n",
        "window_spec2 = Window.partitionBy(\"LnkEmwbisHkey\").orderBy(\"IngestedAt\", \"LastModified\", \"RecordSource\") # orderBy LoadDateTime replaced to orderBy IngestedAt+LastModified as dv_stg_emwbis_lnk is not written as table\r\n",
        "\r\n",
        "dv_rdv_link_emwbis = (dv_stg_emwbis_lnk.select(\r\n",
        "                                    \"LnkEmwbisHkey\",\r\n",
        "                                    \"CustomerHkey\",\r\n",
        "                                    \"ItemBarcodeHkey\",\r\n",
        "                                    \"CustomerKey\",\r\n",
        "                                    \"ItemBarcodeKey\",\r\n",
        "                                    \"LoadDateTime\",\r\n",
        "                                    \"IngestedAt\",\r\n",
        "                                    \"LastModified\",\r\n",
        "                                    \"RecordSource\")\r\n",
        "                                    .withColumn(\"RowNumber\", row_number().over(window_spec1))\r\n",
        "                                    .filter(col(\"RowNumber\") == 1)\r\n",
        "                                    .drop(\"RowNumber\")\r\n",
        "                                    .filter((col(\"CustomerHkey\").isNotNull()) & (col(\"ItemBarcodeHkey\").isNotNull()))\r\n",
        "                                    .withColumn(\"RowNumber\", row_number().over(window_spec2))\r\n",
        "                                    .filter(col(\"RowNumber\") == 1)\r\n",
        "                                    .drop(\"RowNumber\")\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Write dataframe to silver\r\n",
        "if DeltaTable.isDeltaTable(spark, f'{silver_adls_path}/EMWBIS/LINK_EMWBIS/'):\r\n",
        "    deltaTable = DeltaTable.forPath(spark, f'{silver_adls_path}/EMWBIS/LINK_EMWBIS/')\r\n",
        "\r\n",
        "    deltaTable.alias(\"base\").merge(\r\n",
        "        source = dv_rdv_link_emwbis.alias(\"updates\"),\r\n",
        "        condition = \"base.LnkEmwbisHkey = updates.LnkEmwbisHkey\"\r\n",
        "    ).whenNotMatchedInsertAll().execute()\r\n",
        "    print('Merged delta table.')\r\n",
        "else:\r\n",
        "    dv_rdv_link_emwbis.write.option(\"overwriteSchema\", \"true\").mode(\"overwrite\").format(\"delta\").save(f'{silver_adls_path}/EMWBIS/LINK_EMWBIS/')\r\n",
        "    print('Created delta table.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Create SAT\r\n",
        "# dv_prep__email_with_customer_key not yet in place. using CEmailId for now as replacement to CustomerKey\r\n",
        "# IS_BC_CUSTOMER_KEY, IS_OPTIMOVE_CUSTOMER_KEY not available\r\n",
        "\r\n",
        "dv_prep_emwbis_sat = (\r\n",
        "    dq_emwbis.withColumn(\"CustomerKey\", col(\"CEmailId\"))\r\n",
        "    .withColumn(\"ItemBarcodeKey\", col(\"CProductId\"))\r\n",
        "    .withColumn(\"IsDeleted\", lit(False))\r\n",
        "    .withColumn(\"LoadDateTime\", current_timestamp())\r\n",
        "    .withColumn(\"EffectiveFrom\", col(\"LastModified\"))\r\n",
        "    .withColumn(\"RecordSource\", lit('EMWBIS'))\r\n",
        ")\r\n",
        "\r\n",
        "# Hash CustomerKey+ItemBarcodeKey , CustomerKey, ItemBarcodeKey, EmailMeWhenBackInStockHashdiff\r\n",
        "dv_stg_emwbis_sat = (\r\n",
        "                        dv_prep_emwbis_sat\r\n",
        "                        .withColumn(\"LnkEmwbisHkey\",\r\n",
        "                              md5(\r\n",
        "                                    when(\r\n",
        "                                    (trim(upper(col(\"CustomerKey\"))) != '') | (trim(upper(col(\"ItemBarcodeKey\"))) != ''),\r\n",
        "                                    concat_ws(\r\n",
        "                                          '||',\r\n",
        "                                          when(trim(upper(col(\"CustomerKey\"))) != '', trim(upper(col(\"CustomerKey\")))).otherwise('^^'),\r\n",
        "                                          when(trim(upper(col(\"ItemBarcodeKey\"))) != '', trim(upper(col(\"ItemBarcodeKey\")))).otherwise('^^')\r\n",
        "                                    )\r\n",
        "                                    ).otherwise(lit('0000000000000000'))\r\n",
        "                              ).cast(BinaryType())\r\n",
        "                        )\r\n",
        "                        .withColumn(\"CustomerHkey\",\r\n",
        "                              md5(\r\n",
        "                                    when(\r\n",
        "                                    trim(upper(col(\"CustomerKey\"))) != '',\r\n",
        "                                    trim(upper(col(\"CustomerKey\")))\r\n",
        "                                    ).otherwise(lit('0000000000000000'))\r\n",
        "                              ).cast(BinaryType())\r\n",
        "                        )\r\n",
        "                        .withColumn(\"ItemBarcodeHkey\",\r\n",
        "                              md5(\r\n",
        "                                    when(\r\n",
        "                                    trim(upper(col(\"ItemBarcodeKey\"))) != '',\r\n",
        "                                    trim(upper(col(\"ItemBarcodeKey\")))\r\n",
        "                                    ).otherwise(lit('0000000000000000'))\r\n",
        "                              ).cast(BinaryType())\r\n",
        "                        )\r\n",
        "                        .withColumn(\"EmailMeWhenBackInStockHashdiff\",\r\n",
        "                            md5(\r\n",
        "                                concat_ws(\r\n",
        "                                    '||',\r\n",
        "                                    when(trim(upper(col(\"ResourceState\"))) != '', trim(upper(col(\"ResourceState\")))).otherwise('^^'),\r\n",
        "                                    when(trim(upper(col(\"Type\"))) != '', trim(upper(col(\"Type\")))).otherwise('^^'),\r\n",
        "                                    when(trim(upper(col(\"CEmailId\"))) != '', trim(upper(col(\"CEmailId\")))).otherwise('^^'),\r\n",
        "                                    when(trim(upper(col(\"CLocale\"))) != '', trim(upper(col(\"CLocale\")))).otherwise('^^'),\r\n",
        "                                    when(trim(upper(col(\"CProductAvailabilityDate\"))) != '', trim(upper(col(\"CProductAvailabilityDate\")))).otherwise('^^'),\r\n",
        "                                    when(trim(upper(col(\"CProductId\"))) != '', trim(upper(col(\"CProductId\")))).otherwise('^^'),\r\n",
        "                                    when(trim(upper(col(\"CSelectedCountry\"))) != '', trim(upper(col(\"CSelectedCountry\")))).otherwise('^^'),\r\n",
        "                                    when(trim(upper(col(\"CreationDate\"))) != '', trim(upper(col(\"CreationDate\")))).otherwise('^^'),\r\n",
        "                                    when(trim(upper(col(\"CustomerKey\"))) != '', trim(upper(col(\"CustomerKey\")))).otherwise('^^'),\r\n",
        "                                    when(trim(upper(col(\"IngestedFrom\"))) != '', trim(upper(col(\"IngestedFrom\")))).otherwise('^^'),\r\n",
        "                                    when(trim(upper(col(\"IsDeleted\"))) != '', trim(upper(col(\"IsDeleted\")))).otherwise('^^'),\r\n",
        "                                    when(trim(upper(col(\"ItemBarcodeKey\"))) != '', trim(upper(col(\"ItemBarcodeKey\")))).otherwise('^^'),\r\n",
        "                                    when(trim(upper(col(\"KeyProperty\"))) != '', trim(upper(col(\"KeyProperty\")))).otherwise('^^'),\r\n",
        "                                    when(trim(upper(col(\"KeyValueString\"))) != '', trim(upper(col(\"KeyValueString\")))).otherwise('^^'),\r\n",
        "                                    when(trim(upper(col(\"LastModified\"))) != '', trim(upper(col(\"LastModified\")))).otherwise('^^'),\r\n",
        "                                    when(trim(upper(col(\"ObjectType\"))) != '', trim(upper(col(\"ObjectType\")))).otherwise('^^'),\r\n",
        "                                    when(trim(upper(col(\"RecordSource\"))) != '', trim(upper(col(\"RecordSource\")))).otherwise('^^'),\r\n",
        "                                    when(trim(upper(col(\"SiteId\"))) != '', trim(upper(col(\"SiteId\")))).otherwise('^^')\r\n",
        "                                )\r\n",
        "                            )\r\n",
        "                        )\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Write Valid Records to Silver\r\n",
        "# Valid Records = NEW + with update base on EmailMeWhenBackInStockHashdiff\r\n",
        "\r\n",
        "from pyspark.sql import Window\r\n",
        "window_spec1 = Window.partitionBy(\"LnkEmwbisHkey\").orderBy(desc(\"LoadDateTime\"), desc(\"IngestedAt\"), desc(\"LastModified\")) # orderBy LoadDateTime replaced to orderBy LoadDateTime+IngestedAt+LastModified as dv_stg_emwbis_sat is not written as table\r\n",
        "\r\n",
        "dv_rdv_sat_emwbis = dv_stg_emwbis_sat.filter(col(\"LnkEmwbisHkey\").isNotNull()).drop(\"CustomerHkey\", \"ItemBarcodeHkey\").distinct()\r\n",
        "dv_rdv_sat_emwbis_LnkEmwbisHkey = dv_rdv_sat_emwbis.select(col(\"LnkEmwbisHkey\").alias(\"LnkEmwbisHkey2\")).distinct()\r\n",
        "\r\n",
        "# Write dataframe to silver\r\n",
        "if DeltaTable.isDeltaTable(spark, f'{silver_adls_path}/EMWBIS/SAT_EMWBIS'):\r\n",
        "    deltaTable = DeltaTable.forPath(spark, f'{silver_adls_path}/EMWBIS/SAT_EMWBIS')\r\n",
        "\r\n",
        "    deltaTable_df = deltaTable.toDF()\r\n",
        "    deltaTable_df.select(\"LnkEmwbisHkey\", \"EmailMeWhenBackInStockHashdiff\", \"LoadDateTime\", \"IngestedAt\", \"LastModified\").show(truncate=False)\r\n",
        " \r\n",
        "\r\n",
        "    latest_records = (deltaTable_df.select(\"LnkEmwbisHkey\", \"EmailMeWhenBackInStockHashdiff\", \"LoadDateTime\", \"IngestedAt\", \"LastModified\").distinct()\r\n",
        "                                .join(dv_rdv_sat_emwbis_LnkEmwbisHkey, deltaTable_df.LnkEmwbisHkey == dv_rdv_sat_emwbis_LnkEmwbisHkey.LnkEmwbisHkey2, \"inner\")\r\n",
        "                                .withColumn(\"RowNumber\", row_number().over(window_spec1))\r\n",
        "                                .filter(col(\"RowNumber\") == 1)\r\n",
        "                                .drop(\"RowNumber\", \"LnkEmwbisHkey2\")\r\n",
        "    )\r\n",
        "    latest_records.select(\"LnkEmwbisHkey\", \"EmailMeWhenBackInStockHashdiff\", \"LoadDateTime\", \"IngestedAt\", \"LastModified\").show(truncate=False)\r\n",
        "\r\n",
        "    new_records = dv_rdv_sat_emwbis.join(\r\n",
        "                                        latest_records,\r\n",
        "                                        (latest_records.LnkEmwbisHkey == dv_rdv_sat_emwbis.LnkEmwbisHkey),\r\n",
        "                                        \"left\"\r\n",
        "                                    ).filter(\r\n",
        "                                        ((latest_records.EmailMeWhenBackInStockHashdiff != dv_rdv_sat_emwbis.EmailMeWhenBackInStockHashdiff) |\r\n",
        "                                        (latest_records.EmailMeWhenBackInStockHashdiff.isNull()))\r\n",
        "                                        & ((latest_records.IngestedAt < dv_rdv_sat_emwbis.IngestedAt) |\r\n",
        "                                        (latest_records.IngestedAt.isNull()))\r\n",
        "                                    ).select(dv_rdv_sat_emwbis[\"*\"])\r\n",
        "\r\n",
        "    new_records.select(\"LnkEmwbisHkey\", \"EmailMeWhenBackInStockHashdiff\", \"LoadDateTime\", \"IngestedAt\", \"LastModified\").show(truncate=False)\r\n",
        "\r\n",
        "\r\n",
        "    new_records.write.format(\"delta\").mode(\"append\").save(f'{silver_adls_path}/EMWBIS/SAT_EMWBIS/')\r\n",
        "    print('Merged delta table.')\r\n",
        "else:\r\n",
        "    dv_rdv_sat_emwbis.write.option(\"overwriteSchema\", \"true\").mode(\"overwrite\").format(\"delta\").save(f'{silver_adls_path}/EMWBIS/SAT_EMWBIS')\r\n",
        "    print('Created delta table.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Silver to Gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "sat_emwbis = DeltaTable.forPath(spark, f'{silver_adls_path}/EMWBIS/SAT_EMWBIS/').toDF()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Add Valid_ columns to create historical data\r\n",
        "windowSpec = Window.partitionBy(\"LnkEmwbisHkey\").orderBy(col(\"IngestedAt\"), col(\"LastModified\"))\r\n",
        "\r\n",
        "sat_emwbis_hist = (sat_emwbis.withColumn(\"ValidFrom\", col(\"IngestedAt\"))\r\n",
        "                            .withColumn(\"ValidTo\", coalesce(lead(\"IngestedAt\", 1).over(windowSpec), to_date(lit(\"9999-12-31\"))))\r\n",
        "                            .withColumn(\"ValidFlag\", when(col(\"ValidTo\") == to_date(lit(\"9999-12-31\")), lit(\"Y\")).otherwise(lit(\"N\")))\r\n",
        ")\r\n",
        "\r\n",
        "# Filter ValidFlag = Y to get active data\r\n",
        "sat_emwbis_act = sat_emwbis_hist.filter(col(\"ValidFlag\") == \"Y\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Delete Insert to gold container\r\n",
        "fact_emwbis = sat_emwbis_act.select(\r\n",
        "    col(\"CreationDate\"),\r\n",
        "    col(\"LastModified\"),\r\n",
        "    col(\"SiteId\"),\r\n",
        "    col(\"CLocale\"),\r\n",
        "    col(\"ItemBarcodeKey\"),\r\n",
        "    col(\"CEmailId\"),\r\n",
        "    col(\"CProductAvailabilityDate\"),\r\n",
        "    col(\"CSelectedCountry\"),\r\n",
        "    col(\"Type\"),\r\n",
        "    col(\"ObjectType\"),\r\n",
        "    col(\"KeyProperty\"),\r\n",
        "    col(\"KeyValueString\"),\r\n",
        "    col(\"ResourceState\"),\r\n",
        "    col(\"IngestedFrom\"),\r\n",
        "    col(\"IngestedAt\"),\r\n",
        "    col(\"LoadDatetime\")\r\n",
        ").distinct()\r\n",
        "\r\n",
        "delta_table_path = f\"{gold_adls_path}/EMWBIS/FACT_EMWBIS/\"\r\n",
        "fact_emwbis.write.format(\"delta\").option(\"overwriteSchema\", \"true\").mode(\"overwrite\").save(delta_table_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# output = DeltaTable.forPath(spark, f'{gold_adls_path}/EMWBIS/FACT_EMWBIS/').toDF()\r\n",
        "# print(output.count())\r\n",
        "# display(output)"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}