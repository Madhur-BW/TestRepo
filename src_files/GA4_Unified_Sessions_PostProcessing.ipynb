{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Post Processing\n",
        "This notebook will take care of making any final changes to the columns. For example we needed to capitalize BrandCountryKey and replace Hyphen with space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "account_name = raw_adls_path.split('@')[1].split('.')[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Location of Sweaty Betty Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "gold_container = 'gold'\n",
        "gold_sb_sessions_folder = 'GA4/Sessions_SweatyBetty'\n",
        "gold_delta_table_path_sb = f\"abfss://{gold_container}@{account_name}.dfs.core.windows.net/{gold_sb_sessions_folder}\"\n",
        "print(gold_delta_table_path_sb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Location of Wolverine Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "gold_container = 'gold'\n",
        "gold_wolverine_sessions_folder_API = 'GA4/Sessions_wolverine_summary_API_Final'\n",
        "gold_delta_table_path_wolverine_API = f\"abfss://{gold_container}@{account_name}.dfs.core.windows.net/{gold_wolverine_sessions_folder_API}\"\n",
        "print(gold_delta_table_path_wolverine_API)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Read Sweaty Betty Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "collapsed": false
      },
      "source": [
        "# Read the Delta table from the specified path\n",
        "df_sb = spark.read.format(\"delta\").load(gold_delta_table_path_sb)\n",
        "\n",
        "# Display the DataFrame (in Synapse this will render a table)\n",
        "display(df_sb)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "df_sb.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Read Wolverine Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "collapsed": false
      },
      "source": [
        "# Read the Delta table from the specified path\n",
        "df_WWW = spark.read.format(\"delta\").load(gold_delta_table_path_wolverine_API)\n",
        "\n",
        "# Display the DataFrame (in Synapse this will render a table)\n",
        "#display(df_WWW)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_WWW.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Removing Duplicates from Wolverine Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "collapsed": false
      },
      "source": [
        "www_unique = df_WWW.dropDuplicates()\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Define window partitioned by BrandCountryKey, device_type, calday\n",
        "window_spec = Window.partitionBy(\"BrandCountryKey\", \"device_type\", \"calday\") \\\n",
        "                    .orderBy(F.desc(\"sessions\"))\n",
        "\n",
        "# Rank rows in each group by sessions (highest first)\n",
        "df_ranked = www_unique.withColumn(\"rank\", F.row_number().over(window_spec))\n",
        "\n",
        "# Keep only the top row from each group\n",
        "www_best = df_ranked.filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
        "\n",
        "# Sort for display\n",
        "www_best = www_best.orderBy(\"BrandCountryKey\", \"device_type\", \"calday\")\n",
        "\n",
        "www_best = www_best.drop_duplicates()\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Replace CHACOS with CHACO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "www_best = (\n",
        "    www_best\n",
        "    .withColumn(\n",
        "        \"BrandCountryKey\",\n",
        "        F.regexp_replace(F.col(\"BrandCountryKey\"), r\"^CHACOS\", \"CHACO\")\n",
        "    )\n",
        "    .dropDuplicates()\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define function to capitalize and remove Hyphen    \n",
        "    Transforms the BrandCountryKey column and saves the DataFrame as a Delta table.\n",
        "\n",
        "    Transformations:\n",
        "    - Capitalize all values in BrandCountryKey\n",
        "    - Replace hyphens with spaces in BrandCountryKey\n",
        "\n",
        "    Parameters:\n",
        "    - df: Input Spark DataFrame\n",
        "    - output_path: Output path where the transformed DataFrame should be saved as Delta\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Additional Transformations for WWW META SHOPPING and Hytest-B2B2C US\n",
        "'''\n",
        "from pyspark.sql.functions import upper, regexp_replace, when, col\n",
        "\n",
        "def transform_and_save(df, output_path):\n",
        "    # Apply initial uppercase and replace hyphens with spaces\n",
        "    transformed_df = df.withColumn(\n",
        "        \"BrandCountryKey\",\n",
        "        upper(regexp_replace(\"BrandCountryKey\", \"-\", \" \"))\n",
        "    )\n",
        "\n",
        "    # Apply specific replacements\n",
        "    transformed_df = transformed_df.withColumn(\n",
        "        \"BrandCountryKey\",\n",
        "        when(col(\"BrandCountryKey\").contains(\"SWEATYBETTY\"),\n",
        "             regexp_replace(col(\"BrandCountryKey\"), \"SWEATYBETTY\", \"SWEATY BETTY\"))\n",
        "        .when(col(\"BrandCountryKey\") == \"WWW META SHOPPING\", \"Meta US\")\n",
        "        .when(col(\"BrandCountryKey\") == \"WOLVERINE4WORK\", \"Hytest-B2B2C US\")\n",
        "        .otherwise(col(\"BrandCountryKey\"))\n",
        "    )\n",
        "\n",
        "    # Overwrite and save as Delta\n",
        "    transformed_df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
        "\n",
        "    print(f\"Data successfully written to {output_path}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Addional replacements: Merrell EMEA Emerging to 'Merrell IE', and Saucony EMEA Emerging to 'Saucony IE'\n",
        "\n",
        "from pyspark.sql.functions import upper, regexp_replace, when, col\n",
        "\n",
        "def transform_and_save(df, output_path):\n",
        "    # Apply initial uppercase and replace hyphens with spaces\n",
        "    transformed_df = df.withColumn(\n",
        "        \"BrandCountryKey\",\n",
        "        upper(regexp_replace(\"BrandCountryKey\", \"-\", \" \"))\n",
        "    )\n",
        "\n",
        "    # Apply specific replacements\n",
        "    transformed_df = transformed_df.withColumn(\n",
        "        \"BrandCountryKey\",\n",
        "        when(col(\"BrandCountryKey\").contains(\"SWEATYBETTY\"),\n",
        "             regexp_replace(col(\"BrandCountryKey\"), \"SWEATYBETTY\", \"SWEATY BETTY\"))\n",
        "        .when(col(\"BrandCountryKey\") == \"WWW META SHOPPING\", \"Meta US\")\n",
        "        .when(col(\"BrandCountryKey\") == \"WOLVERINE4WORK\", \"Hytest-B2B2C US\")\n",
        "        .when(col(\"BrandCountryKey\") == \"MERRELL EMEA EMERGING\", \"MERRELL IE\")\n",
        "        .when(col(\"BrandCountryKey\") == \"SAUCONY EMEA EMERGING\", \"SAUCONY IE\")\n",
        "        .otherwise(col(\"BrandCountryKey\"))\n",
        "    )\n",
        "\n",
        "    # Overwrite and save as Delta\n",
        "    transformed_df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
        "\n",
        "    print(f\"Data successfully written to {output_path}\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Overwrite SB data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(gold_delta_table_path_sb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {},
      "source": [
        "transform_and_save(df_sb, gold_delta_table_path_sb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Overwrite Wolverine Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(gold_delta_table_path_wolverine_API)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "transform_and_save(www_best.dropDuplicates(), gold_delta_table_path_wolverine_API)"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}