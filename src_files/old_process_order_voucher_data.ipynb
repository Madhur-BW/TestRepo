{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Revision History\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Change_date         revision_number     change_description                           author\r\n",
        "# 08/15/2023          1                   initial check-in                             Kranthi\r\n",
        "# 01/06/2024          2                   OrderFlags tag fix                           Kranthi \r\n",
        "# 02/15/2024          3                   replace structured_adls with lakedb_gold     kranthi  \r\n",
        "#03/12/2024           4                   add upc to unique combination                kranthi \r\n",
        "#03/25/2024           5                   add shippigstateshort, subtotal columns      kranthi\r\n",
        "#04/23/2024           6                   display subsidy only for one line            kranthi   \r\n",
        "#05/08/2024           7                   if a single order, then extracting voucher code should handled kranthi\r\n",
        "#05/15/2024           8                   add retries for 20 mins                       kranthi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import from_json, col, concat, lit,array,explode,first,when,expr\r\n",
        "import pyspark.sql.functions as f\r\n",
        "from pyspark import HiveContext\r\n",
        "from pyspark.sql.types import *\r\n",
        "from pyspark.sql import Row, functions as F\r\n",
        "from pyspark.sql.window import Window\r\n",
        "import concurrent.futures\r\n",
        "import datetime\r\n",
        "import re\r\n",
        "from dateutil import tz\r\n",
        "from time import sleep\r\n",
        "from pyspark.sql.functions import input_file_name,regexp_extract\r\n",
        "from pyspark.sql.functions import from_utc_timestamp\r\n",
        "import xml.etree.ElementTree as ET\r\n",
        "spark.conf.set(\"spark.sql.parquet.mergeSchema\", True)   \r\n",
        "spark.conf.set(\"spark.hadoop.parquet.enable.summary-metadata\", True)\r\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print(mssparkutils.env.getWorkspaceName())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "%run /utils/merge_data_notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /b2b2c/config/b2b2c_config_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Move the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "\r\n",
        "def move_files_from_sftp(file_extension,raw_b2b2cfolder):\r\n",
        "  v_file_cnt =0\r\n",
        "  print(\"file_extension::\",file_extension,\"raw_folder::\",raw_b2b2cfolder)  \r\n",
        "  file_lst=[c.path for c in mssparkutils.fs.ls(sftp_path) if re.search(f\"\\{file_extension}$\",c.path) is not None]\r\n",
        "  print(\"file_lst in SFTP::\",file_lst)\r\n",
        "  mssparkutils.fs.ls(sftp_path)\r\n",
        "  for j in file_lst :\r\n",
        "    file_name = j.split(\"/\")[-1].split('.')[0]\r\n",
        "    if  env_var == 'azwwwprodprdadapsyn01' and 'prod' in file_name :\r\n",
        "      print(\"PROD SFTP file_name::\",file_name, ' moving ', file_name, ' to raw_path' )\r\n",
        "      v_file_cnt = v_file_cnt+1\r\n",
        "      mssparkutils.fs.mv(j,f\"{raw_path}{raw_b2b2cfolder}/{file_name}_{dt}{current_cst_time}{file_extension}\",True)\r\n",
        "    elif (env_var == 'azwwwnonproddevadapsyn01' or env_var == 'azwwwnonprodtestadapsyn01') and ('staging' in file_name or 'development' in file_name) :\r\n",
        "      print(\"TEST SFTP file_name::\",file_name, ' moving ', file_name, ' to raw_path' )\r\n",
        "      v_file_cnt = v_file_cnt+1\r\n",
        "      mssparkutils.fs.mv(j,f\"{raw_path}{raw_b2b2cfolder}/{file_name}_{dt}{current_cst_time}{file_extension}\",True)\r\n",
        "\r\n",
        "  return v_file_cnt    \r\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## First Step - move the files from SFTP to raw folder "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from notebookutils import mssparkutils\r\n",
        "spark.sql(\"SET spark.databricks.delta.schema.autoMerge.enabled = true\")\r\n",
        "retry_attempt = 0\r\n",
        "v_xml_file_cnt = 0\r\n",
        "#print(\"env_var::\", env_var)\r\n",
        "\r\n",
        "#dt = (datetime.datetime.now()).strftime('%Y-%m-%d')\r\n",
        "dt = (datetime.datetime.now()).strftime('%Y-%m-%d')\r\n",
        "dt_time = (datetime.datetime.now()).strftime('%Y-%m-%d %H:%M:%S')\r\n",
        "\r\n",
        "print(\"date of load::\", dt)\r\n",
        "dt_date_type = datetime.datetime.strptime(dt, '%Y-%m-%d')\r\n",
        "\r\n",
        "dt_datetime_type = datetime.datetime.strptime(dt_time, '%Y-%m-%d %H:%M:%S')\r\n",
        "\r\n",
        "print(\"time of load UTC::\", dt_datetime_type)\r\n",
        "\r\n",
        "\r\n",
        "from_zone = tz.gettz('UTC')\r\n",
        "to_zone = tz.gettz('America/Chicago')\r\n",
        "dt_time = (datetime.datetime.now()).strftime('%Y-%m-%dT%H:%M:%SZ')\r\n",
        "\r\n",
        "utc = datetime.datetime.strptime(dt_time, \"%Y-%m-%dT%H:%M:%SZ\")\r\n",
        "print(\"utc::\",utc)\r\n",
        "utc = utc.replace(tzinfo=from_zone)\r\n",
        "cst = utc.astimezone(to_zone)\r\n",
        "print(\"utc2::\",utc)\r\n",
        "print(\"cst1::\",cst)\r\n",
        "print(\"cst2::\",cst.strftime('%H:%M:%S').replace(':',''))\r\n",
        "current_cst_time = cst.strftime('%H:%M:%S').replace(':','')\r\n",
        "print(\"current_cst_time\",current_cst_time)\r\n",
        "\r\n",
        "#v_csv_file_cnt = move_files_from_sftp('.csv','dpcce')\r\n",
        "while (retry_attempt < 4) :  \r\n",
        "  v_xml_file_cnt = move_files_from_sftp('.xml','sfcc')\r\n",
        "  print('v_xml_file_cnt::',v_xml_file_cnt,'retry_attempt::', retry_attempt)\r\n",
        "  retry_attempt = retry_attempt +1\r\n",
        "  if v_xml_file_cnt ==0 :\r\n",
        "    time.sleep(300)\r\n",
        "  else:\r\n",
        "    break  \r\n",
        "\r\n",
        "#print(\"v_csv_file_cnt::\",v_csv_file_cnt)\r\n",
        "\r\n",
        "print(\"v_xml_file_cnt::\",v_xml_file_cnt)\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Exit the notebook if no XML files are found in the path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import sys\r\n",
        "\r\n",
        "#file_lst_cnt = mssparkutils.fs.ls(f\"{raw_path}sfcc/\")\r\n",
        "file_lst_cnt = [j for j in mssparkutils.fs.ls(f\"{raw_path}sfcc/\") if j.isDir== False]\r\n",
        "print(\"xml file count::\",len(file_lst_cnt))\r\n",
        "if len(file_lst_cnt)<1:\r\n",
        "  mssparkutils.notebook.exit(\"None\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## read the XML file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# df = spark.read.format('xml').option(\"rootTag\", \"orders\").option(\"rowtag\",\"order\").load(\"abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/b2b2c/sfcc/Order-20231220_2023-12-21063658.xml\")\r\n",
        "# dfcustom_attributes = df.select(col(\"_order-no\").alias(\"order_no\"),col(\"order-date\").alias(\"order_date\"),\"custom-attributes.*\")\r\n",
        "# display(dfcustom_attributes)\r\n",
        "# dfcustom_attributes = dfcustom_attributes.select(\"order_no\",\"order_date\",explode('custom-attribute').alias(\"col1\"))\r\n",
        "# dfcustom_attributes = dfcustom_attributes.select(\"order_no\",\"order_date\",\"col1.*\")\r\n",
        "# display(dfcustom_attributes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#df_xml = spark.read.format('xml').option(\"rootTag\", \"orders\").option(\"rowtag\",\"order\").load(\"abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/b2b2c/sfcc/Order-20231220_2023-12-21120844.xml\")\r\n",
        "#malformed\r\n",
        "# df_xml = spark.read.format('xml').option(\"rootTag\", \"orders\")\\\r\n",
        "# .option(\"rowtag\",\"order\")\\\r\n",
        "# .option(\"nullValue\",\"\")\\\r\n",
        "# .option(\"inferSchema\", 'false')\\\r\n",
        "# .option(\"mode\",\"PERMISSIVE\")\\\r\n",
        "# .load(\"abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/b2b2c/sfcc/Order-20231220_2023-12-21063658.xml\")\r\n",
        "#df_xml = spark.read.format('xml').option(\"rootTag\", \"orders\").option(\"rowtag\",\"order\").load(\"abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/b2b2c/sfcc/short_xml_test.xml\")\r\n",
        "df_xml = spark.read.format('xml').option(\"rootTag\", \"orders\").option(\"rowtag\",\"order\").load(f\"{raw_path}sfcc/\")\r\n",
        "display(df_xml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get Customer details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "\r\n",
        "dfCustomer = df_xml.select(\"customer.*\")  \r\n",
        "dfCustomer = dfCustomer.select(\"customer-email\",\"customer-name\",\"customer-no\",\"billing-address.*\") \r\n",
        "display(dfCustomer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## get payment details only for subsidy data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "subsidy_payment_schema = StructType([\r\n",
        "    StructField(\"order_no\", StringType(), True),\r\n",
        "    #StructField(\"order_date\", TimestampType(), True),\r\n",
        "    StructField(\"is_redeemed\", StringType(), True),\r\n",
        "    StructField(\"voucher_amount\", IntegerType(), True),\r\n",
        "    StructField('voucher_code',StringType(), True)\r\n",
        "                      ])\r\n",
        "dummy_data = [('a','a',0,'a')]  \r\n",
        "stringified_array_schema = ArrayType(StructType([StructField(\"_VALUE\",StringType(),True)\r\n",
        ",StructField(\"_attribute_id\",StringType(),True)]))                   \r\n",
        "\r\n",
        "try:\r\n",
        "    dfPayment = df_xml.select(col(\"_order-no\").alias(\"order_no\"),col(\"order-date\").alias(\"order_date\"),\"payments.*\")\r\n",
        "   #dfPayment = dfPayment.select(col(\"_order-no\").alias(\"order_no\"),explode('payment').alias(\"payment\"))\r\n",
        "   #dfPayment = dfPayment.select('*','payment.custom-attributes.*')\r\n",
        "   #display(dfPayment)\r\n",
        "    dfPayment = dfPayment.select('order_no','order_date',explode(\"payment.custom-method\").alias('payment_custom_attr'))\r\n",
        "    dfPayment = dfPayment.select('order_no','order_date','payment_custom_attr.custom-attributes.custom-attribute')\r\n",
        "    dfPayment = dfPayment.select('order_no','order_date',explode('custom-attribute').alias('payment_custom_attr'))\r\n",
        "    dfPayment = dfPayment.groupBy(\"order_no\",\"order_date\").pivot(\"payment_custom_attr._attribute-id\").agg(first(\"payment_custom_attr._VALUE\"))\r\n",
        "    dfPayment = dfPayment.selectExpr('order_no'\r\n",
        "    ,'order_date'\r\n",
        "    ,'isRedeemed as is_redeemed'\r\n",
        "    , 'isSubsidy as is_subsidy'\r\n",
        "    ,'voucherAmount as voucher_amount'\r\n",
        "    ,'voucherCode as voucher_code')\r\n",
        "    display(dfPayment)    \r\n",
        "except Exception as e:\r\n",
        "    if  \"cannot resolve 'explode(\" in str(e):\r\n",
        "      print('payment is of type STRUCT, cannot apply explode()')  \r\n",
        "      dfPayment = dfPayment.select('order_no','order_date','payment.*')\r\n",
        "      dfPayment = dfPayment.select('order_no','order_date',\"custom-method.custom-attributes.*\")\r\n",
        "      dfPayment = dfPayment.select('order_no','order_date',\"custom-attribute\")\r\n",
        "      dfPayment = dfPayment.select('order_no','order_date',explode('custom-attribute').alias('custom-attribute'))     \r\n",
        "      dfPayment = dfPayment.groupBy(\"order_no\",\"order_date\").pivot(\"custom-attribute._attribute-id\").agg(first(\"custom-attribute._VALUE\"))\r\n",
        "      dfPayment = dfPayment.selectExpr('order_no'\r\n",
        "                                    , 'order_date'\r\n",
        "                                    , 'isRedeemed as is_redeemed'\r\n",
        "                                    , 'isSubsidy as is_subsidy'\r\n",
        "                                    , 'voucherAmount as voucher_amount'\r\n",
        "                                    , 'voucherCode as voucher_code')\r\n",
        "  \r\n",
        "    else:    \r\n",
        "      print(\"no voucher/custom-method payment present::\") \r\n",
        "      dfPayment =   spark.createDataFrame(dummy_data, subsidy_payment_schema)\r\n",
        "      dfPayment = dfPayment.withColumn(\"order_date\",F.current_timestamp()) \r\n",
        "display(dfPayment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#display(dfPayment.select('*').where(\"\"\"order_no IN ('T1080027936','T1080027937','T1080027939','T1080027940','T1080027941','T1080027942','T1080027943','T1080027951','T1080027952','T1080027953','T1080027945','T1080027946',\r\n",
        "#'T1080027947','T1080027954','T1080027955','T1080027957')\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## get the payment data for creditcard when partially paid by voucher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "cc_payment_schema = StructType([\r\n",
        "    StructField(\"order_no\", StringType(), True),     \r\n",
        "    StructField(\"pay_amt\", IntegerType(), True),\r\n",
        "    StructField('card-holder',StringType(), True),\r\n",
        "    StructField('card-number',StringType(), True),\r\n",
        "    StructField('card-token',StringType(), True),\r\n",
        "    StructField('card-type',StringType(), True),\r\n",
        "    StructField('expiration-month',StringType(), True),\r\n",
        "    StructField('expiration-year',StringType(), True)\r\n",
        "                      ])\r\n",
        "dummy_data = [('a',0,'a','a','a','a','a','a')]  \r\n",
        "try:\r\n",
        "    dfPaymentcc0 = df_xml.select(col(\"_order-no\").alias(\"order_no\"),col(\"order-date\").alias(\"order_date\"),\"payments.*\")\r\n",
        "    #display(dfPaymentcc0)\r\n",
        "    dfPaymentcc = dfPaymentcc0.select('order_no','order_date','payment.amount',explode(\"payment.credit-card\").alias('payment_credit_card'))\r\n",
        "    #display(dfPaymentcc)\r\n",
        "    dfPaymentcc = dfPaymentcc.selectExpr('order_no','order_date','amount as pay_amt','payment_credit_card.*')\r\n",
        "    display(dfPaymentcc)\r\n",
        "except Exception as e:\r\n",
        "    print(\"no credit card payment present::\") \r\n",
        "    dfPaymentcc =   spark.createDataFrame(dummy_data, cc_payment_schema)\r\n",
        "    dfPaymentcc = dfPaymentcc.withColumn(\"order_date\",F.current_timestamp()) \r\n",
        "    display(dfPaymentcc)\r\n",
        "\r\n",
        "# dfPaymentccattr = dfPaymentcc0.select('order_no','order_date', explode(array('payment.custom-attributes')).alias('payment_cc_custom-attr'))\r\n",
        "# dfPaymentccattr = dfPaymentccattr.select('order_no','order_date',explode(array('payment_cc_custom-attr.custom-attribute')).alias('payment_cc_custom-attr'))  \r\n",
        "# dfPaymentccattr = dfPaymentccattr.groupBy(\"order_no\",\"order_date\").pivot(\"payment_cc_custom-attr._attribute-id\").agg(first(\"payment_cc_custom-attr._VALUE\"))\r\n",
        "# dfPaymentcc = dfPaymentcc.join(dfPaymentccattr,['order_no','order_date']).selectExpr('order_no','order_date','cardType as cc_card_type','authAmount as cc_amount_paid').distinct()\r\n",
        "# display(dfPaymentcc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#display(dfPaymentcc.select('*').where(\"\"\"order_no IN ('T1080027936','T1080027937','T1080027939','T1080027940','T1080027941','T1080027942','T1080027943','T1080027951','T1080027952','T1080027953','T1080027945','T1080027946',\r\n",
        "#'T1080027947','T1080027954','T1080027955','T1080027957')\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## join subsidy payment data with creditcard payment data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#dfPayment = dfPayment.join(dfPaymentcc,['order_no','order_date'],\"left\")\r\n",
        "#display(dfPayment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# get product lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "\r\n",
        "try:\r\n",
        "    df_product_lines = df_xml.select(col(\"_order-no\").alias(\"order_no\")\r\n",
        "    ,col(\"order-date\").alias(\"order_date_time\")\r\n",
        "    ,explode(\"product-lineitems.product-lineitem\").alias('line_item'))\r\n",
        "    df_product_lines = df_product_lines.select(\"order_no\"\r\n",
        "    ,\"order_date_time\"\r\n",
        "    ,\"line_item.`gross-price`\"\r\n",
        "    ,\"line_item.`lineitem-text`\"\r\n",
        "    ,\"line_item.`product-id`\"\r\n",
        "    ,\"line_item.quantity\"\r\n",
        "    ,\"line_item.`product-name`\" ).select('*').distinct()\r\n",
        "\r\n",
        "    display(df_product_lines)\r\n",
        "except Exception as e:\r\n",
        "  #print('e',type(e).__name__, str(e)[0:30])  \r\n",
        "  if  \"cannot resolve 'explode(\" in str(e):\r\n",
        "    df_product_lines = df_xml.select(col(\"_order-no\").alias(\"order_no\")\r\n",
        "    ,col(\"order-date\").alias(\"order_date_time\")\r\n",
        "    ,col(\"product-lineitems.product-lineitem\")\\\r\n",
        "    .alias('line_item'))\r\n",
        "    df_product_lines = df_product_lines.selectExpr(\"order_no\"\r\n",
        "    ,\"order_date_time\"\r\n",
        "    ,\"line_item.`gross-price`\"\r\n",
        "    ,\"line_item.`lineitem-text`\"\r\n",
        "    ,\"line_item.`product-id`\"\r\n",
        "    ,\"line_item.quantity\"\r\n",
        "    ,\"line_item.`product-name`\" ).select('*').distinct()\r\n",
        "    display(df_product_lines)\r\n",
        "  else:\r\n",
        "    print('Different error', str(e))\r\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#dfcustom_attributes = df_xml.select(col(\"_order-no\").alias(\"order_no\"),col(\"order-date\").alias(\"order_date\"),\"custom-attributes.*\")\r\n",
        "#dfcustom_attributes = dfcustom_attributes.select(\"order_no\",\"order_date\",explode('custom-attribute').alias(\"col1\"))\r\n",
        "#display(dfcustom_attributes.select('*'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## get custom attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "file_list = []\r\n",
        "custom_attributes_lod = []\r\n",
        "custom_attrib_schema = StructType([\r\n",
        "    StructField(\"order_no\", StringType(), True),\r\n",
        "    #StructField(\"order_date_time\", TimestampType(), True),\r\n",
        "    StructField(\"POLocation\", StringType(), True),\r\n",
        "    StructField(\"accountName\", StringType(), True),\r\n",
        "    StructField('blanketPO',StringType(), True),\r\n",
        "    StructField('brandBusinessUnit',StringType(), True),\r\n",
        "    StructField('costcenter',StringType(), True),\r\n",
        "    StructField('employeeID',StringType(), True),\r\n",
        "    StructField('employeeLocation',StringType(), True),\r\n",
        "    StructField('firstName',StringType(), True),\r\n",
        "    StructField('lastName',StringType(), True),\r\n",
        "    StructField('districtOrLocationCodeOrBranch',StringType(), True),\r\n",
        "    StructField('storeOrDept',StringType(), True),\r\n",
        "    StructField('salesCenter',StringType(), True),\r\n",
        "    StructField('POValue',StringType(), True),\r\n",
        "                      ])\r\n",
        "\r\n",
        "def parse_xml(file_name):\r\n",
        "    ## read the text file, convert to string and parse the string\r\n",
        "    file_text = spark.read.text(f\"{raw_path}sfcc/{file_name}\", wholetext=True)\r\n",
        "    xml_data = file_text.collect()[0][0]\r\n",
        "    root = ET.fromstring(xml_data)\r\n",
        "\r\n",
        "    # Get the dynamic namespace\r\n",
        "    namespace = {'ns': root.tag.split('}')[0][1:]}\r\n",
        "   \r\n",
        "    # Iterate over each order\r\n",
        "    for order in root.findall('.//ns:order', namespaces=namespace):\r\n",
        "        custom_attributes = {}\r\n",
        "        custom_attributes['order_no'] = order.get('order-no')\r\n",
        "        #datetime.strptime(timestamp_str, '%Y-%m-%dT%H:%M:%S.%fZ')\r\n",
        "        #custom_attributes['order_date'] = datetime.datetime.strptime(order.find('./ns:order-date', namespaces=namespace).text,'%Y-%m-%dT%H:%M:%S.%fZ')\r\n",
        "                                                                        \r\n",
        "        \r\n",
        "        # Extract parent-level custom-attributes\r\n",
        "        #parent_level_attributes = order.findall('./ns:custom-attributes/ns:custom-attribute', namespaces=namespace)\r\n",
        "        parent_level_attributes = order.findall('./ns:custom-attributes/ns:custom-attribute', namespaces=namespace)\r\n",
        "        # Print order details\r\n",
        "        #print(f\"\\nOrder Number: {order_no}\")\r\n",
        "        #print(f\"Order Date: {order_date}\")\r\n",
        "       \r\n",
        "        \r\n",
        "        # Print parent-level custom-attributes\r\n",
        "        for attr in parent_level_attributes:\r\n",
        "            attribute_id = attr.get('attribute-id')\r\n",
        "            value = attr.text\r\n",
        "            custom_attributes[attribute_id] = value\r\n",
        "            #print(f\"{attribute_id}: {value}\")\r\n",
        "        #print(custom_attributes)\r\n",
        "        custom_attributes_lod.append(custom_attributes)\r\n",
        " \r\n",
        "  \r\n",
        "\r\n",
        "    \r\n",
        "# extract file names \r\n",
        "file_list = [j.name for j in mssparkutils.fs.ls(f\"{raw_path}sfcc/\") if j.size>0]\r\n",
        " \r\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\r\n",
        "        results = list(executor.map(parse_xml, file_list))\r\n",
        "#print(custom_attributes_lod)           \r\n",
        "# Create a DataFrame from the list of dictionaries\r\n",
        "dfcustom_attributes = spark.createDataFrame(data=custom_attributes_lod,schema=custom_attrib_schema)\r\n",
        "#display(dfcustom_attributes)   \r\n",
        "\r\n",
        "# dfcustom_attributes = df_xml.select(col(\"_order-no\").alias(\"order_no\"),col(\"order-date\").alias(\"order_date\"),\"custom-attributes.*\")\r\n",
        "# display(dfcustom_attributes)\r\n",
        "# #display(dfcustom_attributes.select(\"*\").where(\"order_no='T1080008309'\"))\r\n",
        "# dfcustom_attributes = dfcustom_attributes.select(\"order_no\",\"order_date\",explode('custom-attribute').alias(\"col1\"))\r\n",
        "# #dfcustom_attributes = dfcustom_attributes.selectExpr(\"_order-no\",\"explode('custom-attribute') as col1\")\r\n",
        "# display(dfcustom_attributes.select(\"order_no\",\"col1.*\"))\r\n",
        "\r\n",
        "# dfcustom_attributes = dfcustom_attributes.select(\"order_no\",\"order_date\",\"col1.*\")\r\n",
        "# #display(dfcustom_attributes.select(\"*\").where(\"order_no='T1080008309'\"))\r\n",
        "#dfcustom_attributes  = dfcustom_attributes.groupBy(\"order_no\",\"order_date\").pivot(\"_attribute-id\").agg(first(\"_VALUE\"))\r\n",
        "dfcustom_attributes = dfcustom_attributes.join(df_product_lines,[\"order_no\"],\"inner\")\r\n",
        "dfcustom_attributes = dfcustom_attributes.select(\"*\").distinct()\r\n",
        "display(dfcustom_attributes)\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#display(dfcustom_attributes.select('*').where(\"\"\"order_no IN ('T1080027945','T1080027946',\r\n",
        "#'T1080027947','T1080027954','T1080027955','T1080027957')\"\"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "voucher_api_pwd = mssparkutils.credentials.getSecret(kv_name,'b2b2cVoucherApiPassword','ls_kv_adap' )\r\n",
        "#voucher_api_pwd = mssparkutils.credentials.getSecret(kv_name,'b2b2cVoucherApiPassword')\r\n",
        "# import sys  \r\n",
        "# from pyspark.sql import SparkSession  \r\n",
        "  \r\n",
        "# sc = SparkSession.builder.getOrCreate()  \r\n",
        "# token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary  \r\n",
        "  \r\n",
        "# voucher_api_pwd = token_library.getSecret(kv_name, 'b2b2cVoucherApiPassword')  \r\n",
        "\r\n",
        "print(voucher_api_pwd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Get the token to be used for calling coupon API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "import requests\r\n",
        "import json\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "headers = {\r\n",
        "    'Content-Type': 'application/x-www-form-urlencoded',\r\n",
        "    'Authorization': 'Basic aW50ZWdyYXRpb25Ad3d3aW5jLmNvbS5mdWxsOkFwcGFyZWwyMTY1IXFqaXVoMDEyTVY2Vk5Lc2ZOeHF6QVROVA==',\r\n",
        "   \r\n",
        "}\r\n",
        "\r\n",
        "data = {\r\n",
        "    'client_id': b2b_client_id,\r\n",
        "    'client_secret': b2b_client_secret,\r\n",
        "    'password': voucher_api_pwd,\r\n",
        "    'grant_type': 'password',\r\n",
        "    'username': b2b2c_voucher_api_user_name ,\r\n",
        "}\r\n",
        "\r\n",
        "response = requests.post(b2b2c_voucher_token_url,    \r\n",
        "                        headers=headers,\r\n",
        "                        data=data,\r\n",
        ")\r\n",
        "json_resp = json.loads(response.text)\r\n",
        "print(\"token::\",json_resp['access_token'])\r\n",
        "#print(json.load(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## read voucher data returned by  API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "import requests\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DateType\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "headers = {\r\n",
        "    'Content-Type': 'application/json',\r\n",
        "    'Authorization': f\"Bearer {json_resp['access_token']}\",\r\n",
        "    # 'Cookie': 'BrowserId=n5uowCD8Ee68SElowr2Prw; CookieConsentPolicy=0:1; LSKey-c$CookieConsentPolicy=0:1',\r\n",
        "}\r\n",
        "\r\n",
        "json_data = {\r\n",
        "    'status': '',\r\n",
        "    'employeeid': '',\r\n",
        "    'companyid': '',\r\n",
        "    'emailid': '',\r\n",
        "}\r\n",
        "\r\n",
        "response1 = requests.post(b2b2c_voucher_api_url,   \r\n",
        "                        headers=headers,\r\n",
        "                        json=json_data,\r\n",
        ")\r\n",
        "\r\n",
        "#print(response1.text)\r\n",
        "json_resp1 = json.loads(response1.text)\r\n",
        "print(\"response status::\",json_resp1['success'])\r\n",
        "if json_resp1['success'] == True:\r\n",
        "    print('success response')\r\n",
        "    #print(json_resp1['response'])\r\n",
        "    #prepare list of tuples\r\n",
        "    voucher_d = [(k, v) for k, v in json_resp1['response'].items()]\r\n",
        "    voucher_data = spark.createDataFrame(voucher_d, voucher_schema)\r\n",
        "\r\n",
        "    voucher_data = voucher_data.selectExpr('voucher_status','explode(voucher_data) as voucher_data')\r\n",
        "    df_csv_voucher = voucher_data.selectExpr('voucher_data.vouchercode as voucher_code'\r\n",
        "    ,'voucher_status as status'\r\n",
        "    ,'voucher_data.startDate as voucher_start_dt','voucher_data.expirydate as expiry_date'\r\n",
        "    ,'voucher_data.employeeid as employee_id', 'voucher_data.amount', 'voucher_data.companyid as company_id'\r\n",
        "    ,'voucher_data.redemptiondate as redemption_date','row_number() over(partition by voucher_data.vouchercode order by voucher_data.expirydate desc) as rn')\r\n",
        "    df_csv_write = write_to_file(df_csv_voucher,f\"{raw_path}dpcce/\")\r\n",
        "    df_csv_voucher_dups = df_csv_voucher.select(\"*\").where(\"rn>1\")\r\n",
        "    df_csv_voucher = df_csv_voucher.select(\"*\").where(\"rn=1\").drop(\"rn\")\r\n",
        "    display(df_csv_voucher)\r\n",
        "else:\r\n",
        "    print(\"No API response\")    \r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#display(df_csv_voucher.select('employee_id','company_id','status','voucher_start_dt','voucher_code').where(\"voucher_code like '20240228%'\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## print duplicate voucher codes from Voucher API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(df_csv_voucher_dups)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print(df_csv_voucher.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## get data from Fact.orders "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# from notebookutils import mssparkutils\r\n",
        "\r\n",
        "# import sys  \r\n",
        "# from pyspark.sql import SparkSession  \r\n",
        "  \r\n",
        "# sc = SparkSession.builder.getOrCreate()  \r\n",
        "# token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary  \r\n",
        "# #token_library.getSecret(\"kv-name\", \"secret-name\", \"linked-service\")  \r\n",
        "# jdbcPassword = token_library.getSecret(kv_name, \"SqlAdmin\", \"ls_kv_adap\")  \r\n",
        "# print(jdbcPassword)\r\n",
        "\r\n",
        "# #jdbcHostname = 'az-www-datahub-nonprod-dev-adap-sql.database.windows.net'\r\n",
        "# #\"azwwwnonproddevadapsyn01.sql.azuresynapse.net\"\r\n",
        "# #jdbc:sqlserver://azwwwnonproddevadapsyn01.sql.azuresynapse.net:1433;database=syndw01;user=sqlAdmin@azwwwnonproddevadapsyn01;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\r\n",
        "# jdbcPort = 1433\r\n",
        "\r\n",
        "# #az-www-datahub-prod-prd-adap-sql.database.windows.net\r\n",
        "\r\n",
        "# jdbcDatabase = \"dw\"\r\n",
        "\r\n",
        "# jdbcUsername = \"sqlAdmin\"\r\n",
        "\r\n",
        "# #jdbcPassword = mssparkutils.credentials.getSecret('az-www-dev-adap-kv', 'SqlAdmin')\r\n",
        "\r\n",
        "# jdbcDriver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
        "\r\n",
        "# #url = s\"jdbc:sqlserver://${database_host}:${database_port}/${database_name}\"\r\n",
        "\r\n",
        "# table = \"fact.orders\"\r\n",
        "\r\n",
        "# jdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};databaseName={jdbcDatabase}\"\r\n",
        "\r\n",
        "# df_fact_orders = spark.read.format(\"jdbc\")\\\r\n",
        "# .option(\"driver\", jdbcDriver)\\\r\n",
        "# .option(\"url\", jdbcUrl)\\\r\n",
        "# .option(\"dbtable\", table)\\\r\n",
        "# .option(\"user\", jdbcUsername)\\\r\n",
        "# .option(\"password\", jdbcPassword).load()\r\n",
        "# display(df_fact_orders.select(\"*\"))\r\n",
        "df_fact_orders = spark.read.format('delta').load(f'{bronze_adls_path}SAP/BW/ZSOM_D12')\r\n",
        "display(df_fact_orders.where(\"`/BIC/ZSOMOSMNM` IN ('T1080032560')\"))\r\n",
        "df_fact_orders = df_fact_orders.selectExpr('`/BIC/ZSOMOSMNM` as order_number'\r\n",
        "                                ,'`/BIC/ZUPCU` as upc' # ProductCode\r\n",
        "                                ,'`/BIC/ZMATNUM` as stock_number' # StockNumber\r\n",
        "                                ,'`/BIC/YQTY` as quantity' \r\n",
        "                                ,'`/BIC/ZSHOEWD`  as width'\r\n",
        "                                ,'`/BIC/ZSHOESZ` as size'\r\n",
        "                                ,'`/BIC/ZSOMOSTTL` as pre_tax_total' # PreTaxTotal/pre_tax_total\r\n",
        "                                ,'`/BIC/YTAXAMT1` as  item_tax' # TaxAmount1/item_tax\r\n",
        "                                ,'round(`/BIC/ZSOMOTTL`,3) as  order_total' #GrandTotal/order_total\r\n",
        "                                ,'`/BIC/ZREC_TYPE` as record_type' # RecordType/record_type\r\n",
        "                                ,'`ERDAT` as date_key' #OrderCreatedDate\r\n",
        "                                ,'`/BIC/ZSOMDLVST` as ShipToState' #ShipToRegion or ship_to_state\r\n",
        "                                ,'`/BIC/YAMOUNT` as sale_price' #UnitPrice\r\n",
        "                                ,'`/BIC/ZSOMVEMP` as voucheremployeeid' #voucheremployeeid\r\n",
        "                                ,'`/BIC/ZSOMVSTE` as vouchersiteid' # vouchersiteid\r\n",
        "                                ,'`/BIC/ZSOMVBBU` as voucherbrandbusinessunit' # VoucherBrandBusinessUnit\r\n",
        "                                ,'`/BIC/ZSOMLINE` as LineNumber'  \r\n",
        "                                ,'round(`/BIC/ZSOMVOUPY`,3) as LineItemSubsidyCharge' #LineItemSubsidyCharge\r\n",
        "                                ,'round(`RPA_TAT`,3) as LineItemCreditCardCharge' # TenderValue/LineItemCreditCardCharge                                \r\n",
        "                                ,'`/BIC/ZSOMOISST` as ship_status'\r\n",
        "                                ,'`/BIC/ZSOMSHPDT` as ship_date'\r\n",
        "                                ,'row_number() over(partition by `/BIC/ZSOMOSMNM`,`/BIC/ZREC_TYPE`,`/BIC/ZSOMLINE` order by  `AEDAT` DESC,`/BIC/ZAEDAT` DESC,`/BIC/ZCHGTIME` DESC) as rn') \r\n",
        "#display(df_fact_orders.select('*').where(\"order_number = 'T1080030340'\"))\r\n",
        "#display(df_fact_orders.where(\"order_number IN ('T1080030340')\"))\r\n",
        "df_fact_orders = df_fact_orders.where('rn=1')\r\n",
        "display(df_fact_orders.where(\"order_number IN ('T1080033220','T1080033222','T1080033221')\"))\r\n",
        "df_fact_order_one_upc = df_fact_orders.selectExpr('order_number','upc','LineNumber')\\\r\n",
        "                        .where(\"record_type = 'O' and upc is not null and upc != 'SHIPMENTCOSTS'\")\r\n",
        "\r\n",
        "df_fact_order_one_upc = df_fact_order_one_upc\\\r\n",
        "       .selectExpr('order_number','first(`upc`) over(partition by `order_number` order by `upc`,`LineNumber`) as upc')\\\r\n",
        "       .distinct()                       \r\n",
        "display(df_fact_order_one_upc.where(\"order_number IN ('T1080033220','T1080033222','T1080033221')\"))\r\n",
        "df_fact_ordersT = df_fact_orders.selectExpr('order_number', 'record_type as rec_type','LineItemSubsidyCharge','LineItemCreditCardCharge')\\\r\n",
        ".where(\"rec_type='T' \")\r\n",
        "df_fact_ordersT = df_fact_ordersT.join(df_fact_order_one_upc,['order_number'],\"inner\")\r\n",
        "display(df_fact_ordersT.where(\"order_number IN ('T1080033220','T1080033222','T1080033221')\")) #record_type='T' and LineItemSubsidyCharge>0 \r\n",
        "#df_fact_orders.printSchema()\r\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## join  SFCC and DPCCE on employeeId ,accountName/company_id and voucher_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "dpcce_join_sfcc = ''\r\n",
        "dpcce_join_sfcc = df_csv_voucher.where(\"status = 'Redeemed'\").alias('dc')\\\r\n",
        ".join(dfcustom_attributes.alias('da'), ((df_csv_voucher.employee_id==dfcustom_attributes.employeeID) & (F.lower(df_csv_voucher.company_id)==F.lower(dfcustom_attributes.accountName))) ,\"inner\" )\\\r\n",
        ".join(dfPayment.alias('dfp'),((dfcustom_attributes.order_no==dfPayment.order_no) & (dfPayment.voucher_code ==df_csv_voucher.voucher_code)),\"inner\")\r\n",
        "dpcce_join_sfcc = dpcce_join_sfcc.selectExpr(\"da.order_no as order_no\"\r\n",
        ",'da.`product-id` as upc'\r\n",
        ",\"da.order_date_time as order_date_time\"\r\n",
        ",\"date_format(da.order_date_time,'yyyy-MM-dd') as order_date\"\r\n",
        ",\"da.brandBusinessUnit as brand_business_unit\" \r\n",
        ",\"da.accountName as account_name\"\r\n",
        ",\"da.employeeID as employee_id\"\r\n",
        ",\"da.lastName as last_name\"\r\n",
        ",\"da.firstName as first_name\"\r\n",
        ",\"dc.voucher_start_dt\" \r\n",
        ",\"dc.voucher_code\"\r\n",
        ",\"dc.status as voucher_status\"\r\n",
        ",'da.employeeLocation as b2b_customer_location'\r\n",
        ",'da.costCenter as cost_center'\r\n",
        ",\"da.districtOrLocationCodeOrBranch as b2b_location_code\" ## b2b_location_code \r\n",
        ",\"da.storeOrDept as b2b_store_no\" #B2B - STORE # \r\n",
        ",\"da.salesCenter as b2b_sales_center\" #B2B - SALES CENTER #\r\n",
        ",\"da.POLocation as b2b_po_location\" #B2B PO-Location\r\n",
        ",\"da.blanketPO as b2b_blanket_po\"\r\n",
        ",\"da.POValue as POValue\" #this is po_number from SFCC--pending,this field is not there yet, only applies for a PO enabled site \r\n",
        "##,\"round(dfp.voucher_amount,3) as line_item_subsidy_charge\"\r\n",
        ")\r\n",
        "\r\n",
        "#dpcce_join_sfcc = dpcce_join_sfcc.alias('sfcc1').join(dfPayment.alias('dp'),['voucher_code'],\"inner\")\r\n",
        "#dpcce_join_sfcc = dpcce_join_sfcc.selectExpr(\"sfcc1.*\",'dp.is_redeemed','dp.is_subsidy','dp.voucher_amount as line_item_subsidy_charge','dp.cc_amount_paid as line_item_cc_charge')\r\n",
        "display(dpcce_join_sfcc.select('*').distinct())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(dfcustom_attributes.select('accountName').distinct())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "df_state_codes = spark.read.format('csv')\\\r\n",
        "       .option(\"header\", \"true\")\\\r\n",
        "       .load(f\"{raw_adls_path}b2b2c/state_codes/US_state_codes.csv\")\r\n",
        "display(df_state_codes)       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## join with fact_orders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "dpcce_sfcc_fact_orders=''\r\n",
        "dpcce_sfcc_fact_orders = dpcce_join_sfcc.alias('sf')\\\r\n",
        ".join(df_fact_orders.alias('fo'),(dpcce_join_sfcc.order_no == df_fact_orders.order_number)&  (dpcce_join_sfcc.upc == df_fact_orders.upc) ,\"inner\")\\\r\n",
        ".join(df_state_codes.alias('scodes'),(df_fact_orders.ShipToState == df_state_codes.us_state),\"left\")\\\r\n",
        ".join(df_fact_ordersT.alias('fot'),((df_fact_ordersT.order_number == df_fact_orders.order_number)&(df_fact_ordersT.upc == df_fact_orders.upc)),\"left\")\r\n",
        "dpcce_sfcc_fact_orders = dpcce_sfcc_fact_orders.selectExpr('sf.*'\r\n",
        ",'fo.stock_number'\r\n",
        ",'fo.quantity as product_qty'\r\n",
        ",'fo.width'\r\n",
        ", \"case when cast(ltrim('0',fo.size) as int) >20 then cast(ltrim('0',fo.size) as int)/10 else cast(ltrim('0',fo.size) as int) end as size\"\r\n",
        ",'fo.pre_tax_total as line_item_price'\r\n",
        ",'fo.item_tax line_item_tax'\r\n",
        ",'fo.order_total line_item_total'\r\n",
        ",'fo.record_type as order_type'\r\n",
        ",'fo.date_key'\r\n",
        ",'fo.shipToState as shipping_state'\r\n",
        ",'fo.sale_price'\r\n",
        ",'fo.voucheremployeeid as voucher_emp_id'\r\n",
        ",'fo.vouchersiteid as voucher_site_id'\r\n",
        ",'fo.voucherbrandbusinessunit as voucher_bbusiness_unit'\r\n",
        ",'round(nvl(fot.LineItemCreditCardCharge,0),3) as line_item_credit_card_charge'\r\n",
        ",'round(nvl(fot.LineItemSubsidyCharge,0),3) as line_item_subsidy_charge'\r\n",
        "#,'case when fo.order_total > round(nvl(fot.LineItemSubsidyCharge,0),3) then round(nvl(fot.LineItemSubsidyCharge,0),3) else round(fo.order_total,3) end as balance_national_acc_owes'\r\n",
        ", 'round(nvl(fot.LineItemSubsidyCharge,0),3) as balance_national_acc_owes'\r\n",
        ",'fo.ship_status as line_item_status'\r\n",
        ",'round(fo.quantity*fo.pre_tax_total,3) as subtotal'\r\n",
        ",'scodes.code as shipping_state_short'\r\n",
        ",'fo.size as size_original'\r\n",
        ",\"to_date(fo.ship_date,'yyyyMMdd') as ship_date\")\r\n",
        "dpcce_sfcc_fact_orders.createOrReplaceTempView('order_voucher_data')\r\n",
        "display(dpcce_sfcc_fact_orders.select('*'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## validate for duplicate data on the key (emp#,order#,item#,voucher#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "select * from (select * , row_number() over(partition by order_no,upc, employee_id, voucher_code,order_type order by order_date_time desc) rn \r\n",
        "from order_voucher_data) t where t.rn = 1;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## print duplicates from integrated(sfcc, fact.orders, voucher) data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "df_order_voucher_dups = spark.sql(f\"\"\"select * from (select * , row_number() over(partition by order_no, upc,employee_id, voucher_code,order_type order by order_date_time desc) rn \r\n",
        "from order_voucher_data) t where t.rn > 1\"\"\")\r\n",
        "display(df_order_voucher_dups)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## select one row per orderno+voucherno and empid "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "dpcce_sfcc_fact_orders = spark.sql(f\"\"\"select * from (select * , row_number() over(partition by order_no,upc, employee_id, voucher_code, order_type order by order_date_time desc) rn \r\n",
        "from order_voucher_data) t where t.rn = 1\"\"\")\r\n",
        "dpcce_sfcc_fact_orders = dpcce_sfcc_fact_orders.drop('rn')\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Merge to order_voucher_data table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "dict_tables = {\r\n",
        "      \"b2b2c/order_voucher_data\": { \"source_df\": dpcce_sfcc_fact_orders\r\n",
        "                 ,\"where_condition\":\"\"\"target.order_no = source.order_no \r\n",
        "                  and target.upc = source.upc \r\n",
        "                  and target.employee_id = source.employee_id \r\n",
        "                  and target.voucher_code= source.voucher_code \r\n",
        "                  and target.order_type = source.order_type\"\"\"                        \r\n",
        "                 ,\"target_table\":\"lakedb_gold.b2b2c_order_voucher_data\" \r\n",
        "                 ,\"partition\":\"order_date\"\r\n",
        "                 ,\"path\": gold_adls_path + 'b2b2c/order_voucher_data'\r\n",
        "                       \r\n",
        "                   }\r\n",
        "      ,\"b2b2c/voucher_master_data\": \r\n",
        "                { \"source_df\": df_csv_voucher\r\n",
        "                 ,\"where_condition\":\"target.voucher_code = source.voucher_code\"                        \r\n",
        "                 ,\"target_table\":\"lakedb_gold.b2b2c_voucher_master_data\" \r\n",
        "                 ,\"partition\":\"\"\r\n",
        "                 ,\"path\": gold_adls_path + 'b2b2c/voucher_master_data'\r\n",
        "                }\r\n",
        "                            \r\n",
        "               }\r\n",
        "perform_merge(dict_tables)   \r\n",
        "               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "select * from lakedb_gold.b2b2c_order_voucher_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## exit the notebook - send an email about the processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#move the processed files to archive\r\n",
        "if  v_xml_file_cnt >=1:  \r\n",
        "  for j in mssparkutils.fs.ls(f\"{raw_path}sfcc/\"):\r\n",
        "    if j.size>0:  \r\n",
        "      print(f'moving ', j.name, ' to archive' )\r\n",
        "      mssparkutils.fs.mv(f\"{raw_path}sfcc/{j.name}\", f\"{raw_path}sfcc/archive/{j.name}\",overwrite=True)\r\n",
        "  mssparkutils.notebook.exit(\"sfcc file\")  \r\n",
        "\r\n",
        "elif  v_xml_file_cnt <1:\r\n",
        "  mssparkutils.notebook.exit(\"None\")   \r\n",
        "\r\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# df_fact_order = spark.read.format('delta').load('abfss://bronze@azwwwnonproddevadapadls.dfs.core.windows.net/SAP/BW/ZSOM_D12')\r\n",
        "# (len(df_fact_order.columns))\r\n",
        "# cols1= ['YACTDATE', 'YAMOUNT', 'YARTICLE', 'YORGPLANT', 'YPLANT', 'YQTY', 'YREASON', 'YREGISTER', 'YRTLCOST', 'YSHIPCHRG', 'YSLSPRSN', 'YTAXAMT1', 'YTAXAMT2', 'ZAEDAT', 'ZCARRIER', 'ZCHGTIME', 'ZCOUPON', 'ZCOUPON2', 'ZCOUPON3', 'ZDEMANDLC', 'ZDISCMSG', 'ZDISCMSGP', 'ZFINALSAL', 'ZINVOICED', 'ZLOCALE', 'ZMATNUM', 'ZREC_TYPE', 'ZRPA_TCD', 'ZRTMSRP', 'ZSALETYPE', 'ZSDTRACK', 'ZSHOESZ', 'ZSHOEWD', 'ZSKU', 'ZSOMACTID', 'ZSOMBILCC', 'ZSOMBILCT', 'ZSOMBILPC', 'ZSOMBILST', 'ZSOMCSRNM', 'ZSOMDLVCC', 'ZSOMDLVCT', 'ZSOMDLVPC', 'ZSOMDLVST', 'ZSOMFSTAT', 'ZSOMFTIME', 'ZSOMGCORD', 'ZSOMGCTND', 'ZSOMHSTAT', 'ZSOMIACDT', 'ZSOMIACTM', 'ZSOMLINE', 'ZSOMODISC', 'ZSOMODLMD', 'ZSOMOISST', 'ZSOMOODT', 'ZSOMOORD', 'ZSOMOOTM', 'ZSOMOPTIN', 'ZSOMOSMNM', 'ZSOMOSTTL', 'ZSOMOTTL', 'ZSOMPDISC', 'ZSOMPYST', 'ZSOMRFST', 'ZSOMRTNDT', 'ZSOMSAPTD', 'ZSOMSAPTT', 'ZSOMSDISC', 'ZSOMSHPDT', 'ZSOMSLMD', 'ZSOMSTIME', 'ZSOMTYPE', 'ZSTOREID', 'ZUPCU', 'ZVATVAL', 'AEDAT', 'BASE_UOM', 'CALDAY', 'CALMONTH', 'CALMONTH2', 'CALWEEK', 'CALYEAR', 'CREA_TIME', 'CURRENCY', 'DOC_CURRCY', 'DOC_NUMBER', 'EMPLOYEE', 'ERDAT', 'FISCPER', 'FISCPER3', 'FISCVARNT', 'FISCYEAR', 'LOAD_DATE', 'LOC_CURRCY', 'RPA_DTC', 'RPA_REA', 'RPA_TAT', 'RPA_TCD', 'RPA_TTC', 'SR_COUNTER', 'TIME', 'ZEMPPURCH', 'ZPRODEAL', 'ZISMOBILE', 'ZISCUSTOM', 'ZCSRORD', 'ZPCIPAL', 'ZPREORDER', 'ZOUTLETF', 'ZINSTORE', 'ZSTORETYP', 'ZSOMSLSCH', 'ZISLEGACY', 'ZSOMCRDT', 'ZSOMVEMP', 'ZSOMVSTE', 'ZSOMVBBU', 'ZSOMVOUPY']\r\n",
        "# cols_renamed = ['ActualTransactionDate', 'UnitPrice', 'RetailProductCode', 'OriginatingStore', 'Site', 'Quantity', 'ReturnReason', 'Register', 'RetailCost', 'ShippingCharges', 'StoreSalesAssociate', 'TaxAmount1', 'TaxAmount2', 'ItemChangedDate', 'ShipmentCarrier', 'ItemChangedTime', 'ShipCouponCode', 'OrderCouponCode', 'ProductCouponCode', 'DemandLocalCurrency', 'OrderPromoMessage', 'ProductPromoMessage', 'FinalSale', 'Invoiced', 'Locale', 'StockNumber', 'RecordType', 'GiftCardPayIndicator', 'ListPrice', 'SaleType', 'TrackingNumber', 'ItemSize', 'ItemWidth', 'StockKeepingUnit', 'CustomerAccountId', 'BillToCountryCode', 'BillToCity', 'BillToPostalCode', 'BillToRegion', 'CSRWWWUserId', 'ShipToCountryCode', 'ShipToCity', 'ShipToPostalCode', 'ShipToRegion', 'FulfillmentStatus', 'FulfillOrderTime', 'GiftCardOrderIndicator', 'GiftCardTenderValue', 'OrderStatus', 'ItemAdjCreatedDate', 'ItemAdjCreatedTime', 'LineNumber', 'OrderDiscount', 'ShippingMethod', 'ItemStatus', 'OriginalOrderDate', 'OriginalOrderNumber', 'OriginalOrderTime', 'OptInFlag', 'OrderNumber', 'PreTaxTotal', 'GrandTotal', 'ProductDiscount', 'PaymentStatus', 'RefundStatus', 'ReturnDate', 'SAPTransactDate', 'SAPTransactTime', 'ShippingDiscount', 'ShipmentCreateDate', 'ShipLastModifyDate', 'ShipmentCreateTime', 'OrderType', 'CidStoreId', 'ProductCode', 'VATTax', 'OrderChangedDate', 'UnitOfMeasure', 'OrderedDate', 'CalendarYearMonth', 'CalendarMonth', 'OrderedWeek', 'CalendarYear', 'OrderCreatedTime', 'CurrencyKey', 'DocumentCurrency', 'SAPSalesOrder', 'EmployeeNumber', 'OrderCreatedDate', 'FiscalYearPeriod', 'FiscalPeriod', 'FiscalCalendar', 'FiscalYear', 'BWLoadDate', 'LocalCurrency', 'DiscountTypesFlag', 'TotalDiscount', 'TenderValue', 'MeansOfPayment', 'TransType', 'NumberOfTransactions', 'OrderedTime', 'EmployeePurchaseFlag', 'ProDealFlag', 'MobileOrderFlag', 'CustomOrderFlag', 'CsrOrderFlag', 'PciPalFlag', 'PreOrderFlag', 'OutletFlag', 'InStoreFlag', 'StoreType', 'SalesChannel', 'LegacyFlag', 'CancelReturnDate', 'VoucherEmployeeID', 'VoucherSiteID', 'VoucherBrandBusinessUnit', 'LineItemSubsidyCharge']\r\n",
        "# #cols_renamed_filter = [x for x in cols_renamed if x in []\r\n",
        "# cols_zip = list(zip(cols1,cols_renamed))\r\n",
        "# for j in cols_zip:\r\n",
        "#   print(j[0]+' as '+j[1])\r\n",
        "# # display(spark.sql(\"\"\"select * from parquet.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/SAP/BW/ZSOM_D12/2024/03/25/2024-03-25T17:59:31.9176378Z.parquet`\r\n",
        "# # where ZSOMOSMNM like 'T1080030340%'\"\"\"))  \r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "description": "b2b2c process vouchers",
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}
