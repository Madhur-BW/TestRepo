{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyspark.sql.functions as F\r\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"DYNAMIC\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\r\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\",\"true\")\r\n",
        "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\",\"true\")\r\n",
        "spark.conf.set(\"spark.databricks.adaptive.autoOptimizeShuffle.enabled\",\"true\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## get dates for Full load - comment after the first full load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "## full_load_dates = \"\"\"\r\n",
        "##      SELECT DISTINCT  dd.fiscalweek as fiscalweek, dd.weekbegindate as weekbegindate, dd.weekenddate as weekenddate\r\n",
        "##     FROM report.DateDim dd\r\n",
        "##    where daydate between '2023-02-05' and '2024-05-28'\r\n",
        "   \r\n",
        "# \"\"\"\r\n",
        "# df_date = spark.read.format(\"jdbc\")\\\r\n",
        "# .option(\"driver\", jdbcDriver)\\\r\n",
        "# .option(\"url\", jdbcUrl)\\\r\n",
        "# .option(\"query\",full_load_dates)\\\r\n",
        "# .option(\"user\", jdbcUsername)\\\r\n",
        "# .option(\"password\", jdbcPassword)\\\r\n",
        "# .load()\r\n",
        "# gua_dates_range = df_date.collect()\r\n",
        "# fiscal_week = gua_dates_range[0][0]\r\n",
        "# week_begin_date = gua_dates_range[0][1]\r\n",
        "# week_end_date = gua_dates_range[0][2]\r\n",
        "# print(\"fiscal_week::\",fiscal_week,\"week_begin_date::\",week_begin_date,\"week_end_date::\",week_end_date) \r\n",
        "# display(df_date)\r\n",
        "# df_date2 = df_date.createOrReplaceTempView('DateDim')\r\n",
        "# df_date.printSchema()\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## incremental load - if no date is passed, get the dates based on current day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false,
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "#if needed to give::\r\n",
        "#date_ranges = [('2024-05-26','2024-06-01'),('2024-05-05','2024-05-11')]\r\n",
        "#date_ranges = [('2024-05-26','2024-06-01')]\r\n",
        "date_ranges = []\r\n",
        "\r\n",
        "print(\"param date_ranges::\", date_ranges)\r\n",
        "if not date_ranges or len(date_ranges)==0:\r\n",
        "    print('dates_to_load is null - get weekbegindate and weekenddate based on current day')\r\n",
        "    df_max_weekend_date = spark.sql('select max(weekenddate) as max_weekenddate from lakedb_gold.ga4_weekly_stats')\r\n",
        "    df_max_weekend_date = df_max_weekend_date.collect()[0].max_weekenddate\r\n",
        "    print(df_max_weekend_date)\r\n",
        "    query = f\"\"\"\r\n",
        "   \t\t  SELECT DISTINCT dd.fiscalyear,dd.fiscalweek, dd.weekbegindate, dd.weekenddate\r\n",
        "     FROM report.DateDim dd\r\n",
        "     JOIN (SELECT top 1  daydate,fiscalyear, fiscalweek as nextfiscalweek\r\n",
        "         FROM report.DateDim\r\n",
        "         WHERE DayDate > convert(varchar,cast('{df_max_weekend_date}' as date),23) \r\n",
        "         --and daydate <= cast(getdate() as date)\r\n",
        "          order by daydate,fiscalyear,fiscalweek )t1 ON dd.fiscalweek = t1.nextfiscalweek AND dd.fiscalyear = t1.fiscalyear\r\n",
        "\"\"\"\r\n",
        "    df_date = spark.read.format(\"jdbc\")\\\r\n",
        "    .option(\"driver\", jdbcDriver)\\\r\n",
        "    .option(\"url\", jdbcUrl)\\\r\n",
        "    .option(\"query\",query)\\\r\n",
        "    .option(\"user\", jdbcUsername)\\\r\n",
        "    .option(\"password\", jdbcPassword)\\\r\n",
        "    .load()\r\n",
        "    ga4_dates_range = df_date.collect()\r\n",
        "    print(ga4_dates_range)\r\n",
        "    # fiscal_week = ga4_dates_range[0][0]\r\n",
        "    # week_begin_date = ga4_dates_range[0][1]\r\n",
        "    # week_end_date = ga4_dates_range[0][2]\r\n",
        "    # print(\"fiscal_week::\",fiscal_week,\"week_begin_date::\",week_begin_date,\"week_end_date::\",week_end_date) \r\n",
        "    display(df_date)\r\n",
        "    df_date2 = df_date.createOrReplaceTempView('DateDim') \r\n",
        "    df_date.printSchema() \r\n",
        "else:\r\n",
        "  print('dates_to_load is not null')\r\n",
        "  df_columns = ['weekbegindate','weekenddate']\r\n",
        "  df_date = spark.createDataFrame(data = date_ranges, schema = df_columns ) \r\n",
        "  df_date = df_date.withColumn('weekbegindate',F.to_date(F.date_format(df_date.weekbegindate,'yyyy-MM-dd')))\\\r\n",
        "                   .withColumn('weekenddate',F.to_date(F.date_format(df_date.weekenddate,'yyyy-MM-dd')))   \r\n",
        "  df_date2 = df_date.createOrReplaceTempView('DateDim')\r\n",
        "  df_date.printSchema()\r\n",
        "  display(df_date)\r\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Load the data using dates in DateDim table created above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(spark.sql(\"select * from datedim\"))\r\n",
        "df_stats = spark.sql(\"\"\"\r\n",
        "with dates as ( SELECT /*+BROADCAST(DateDim)*/ DISTINCT \r\n",
        " date_format(weekbegindate,'yyyyMMdd') as weekbegindate\r\n",
        ", date_format(weekenddate,'yyyyMMdd') as weekenddate\r\n",
        "    FROM DateDim \r\n",
        "  )\r\n",
        ",t_all_data as (select * from raw.ga4_events, dates d\r\n",
        "      where event_date between d.weekbegindate and d.weekenddate \r\n",
        "      and device.category != 'smart tv' )  \r\n",
        ",t1 as (\r\n",
        "select  property_brand\r\n",
        ", brand_country\r\n",
        ", device_category\r\n",
        ", weekbegindate \r\n",
        ", weekenddate\r\n",
        ", 'sessions' as metric\r\n",
        ", count( distinct concat(a.user_pseudo_id,a.value.int_value)) as cnt\r\n",
        " from (select  property_brand, brand_country,device.category as device_category\r\n",
        " , user_pseudo_id\r\n",
        " , weekbegindate \r\n",
        " , weekenddate\r\n",
        " , explode(event_params)\r\n",
        "                from t_all_data) a\r\n",
        "                where a.key = 'ga_session_id' \r\n",
        "  group by property_brand, brand_country,device_category,weekbegindate ,weekenddate) \r\n",
        "  , t2 as (select  property_brand\r\n",
        "  , brand_country\r\n",
        "  ,device.category\r\n",
        "  ,weekbegindate \r\n",
        "  , weekenddate\r\n",
        "  ,'carts'\r\n",
        "  ,COUNT_IF(event_name = 'add_to_cart') AS adds_to_cart   \r\n",
        "  from   t_all_data                          \r\n",
        "  group by property_brand, brand_country,device.category,weekbegindate, weekenddate)\r\n",
        "  ,t3 as (select  property_brand\r\n",
        "  , brand_country\r\n",
        "  ,device.category\r\n",
        "  ,weekbegindate\r\n",
        "  , weekenddate\r\n",
        "  ,'orders'\r\n",
        "  ,COUNT_IF(event_name IN ('purchase', 'transaction'))\r\n",
        "from  t_all_data\r\n",
        "                           \r\n",
        "  group by property_brand, brand_country,device.category,weekbegindate, weekenddate)\r\n",
        " , t5 as (select property_brand\r\n",
        " , brand_country\r\n",
        " ,device_category\r\n",
        " ,weekbegindate\r\n",
        " ,weekenddate\r\n",
        " , 'quantity'\r\n",
        " ,SUM(items.quantity) as qty\r\n",
        "        from (select /*+BROADCAST(d)*/ property_brand, brand_country,device.category as device_category\r\n",
        "        , weekbegindate,weekenddate, event_name, explode(items) as items\r\n",
        "        from t_all_data where event_name = 'purchase' ) \r\n",
        "                   group by property_brand, brand_country,device_category, weekbegindate,weekenddate\r\n",
        "        )\r\n",
        "  ,t6 as (select  /*+BROADCAST(d)*/  property_brand\r\n",
        " , brand_country\r\n",
        " ,device.category\r\n",
        " ,weekbegindate\r\n",
        " ,weekenddate\r\n",
        " , 'sales'\r\n",
        " , sum(nvl(ecommerce.purchase_revenue,0) - nvl(ecommerce.shipping_value,0) - nvl(ecommerce.tax_value,0))  as sales\r\n",
        "    from t_all_data                          \r\n",
        "  group by property_brand, brand_country,device.category,weekbegindate, weekenddate )      \r\n",
        "  ,t4 as (select * from t1 \r\n",
        "  union all\r\n",
        "  select * from t2 \r\n",
        "  union all\r\n",
        "  select * from t3\r\n",
        "  union all \r\n",
        "  select * from t5\r\n",
        "  union all\r\n",
        "  select * from t6)\r\n",
        "  SELECT property_brand, brand_country,concat(device_category,'_',metric) as metric,nvl(cnt,0) cnt\r\n",
        ",weekbegindate,weekenddate FROM t4\"\"\")\r\n",
        "\r\n",
        "df_stats = df_stats.groupBy('property_brand', 'brand_country','weekbegindate','weekenddate')\\\r\n",
        ".pivot('metric')\\\r\n",
        ".agg(F.first('cnt').alias('cnt'))\r\n",
        "\r\n",
        "\r\n",
        "#df_stats.createOrReplaceTempView('ga4_weekly_stats')\r\n",
        "display(df_stats)\r\n",
        "df_stats=  df_stats.repartition('weekenddate','property_brand','brand_country')\\\r\n",
        "    .write.format(\"delta\")\\\r\n",
        "    .mode(\"overwrite\")\\\r\n",
        "    .option(\"path\",f\"{gold_adls_path}GA4/weekly_stats\")\\\r\n",
        "    .option(\"mergeSchema\", \"true\")\\\r\n",
        "    .partitionBy('weekenddate','property_brand','brand_country')\\\r\n",
        "    .saveAsTable('lakedb_gold.ga4_weekly_stats')\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "select * from lakedb_gold.ga4_weekly_stats\r\n",
        "where 1=1\r\n",
        "        and weekenddate= 20240601 order by property_brand,brand_country\r\n",
        "   \r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# ,t1 as (\r\n",
        "# select  property_brand\r\n",
        "# , brand_country\r\n",
        "# , device_category\r\n",
        "# , weekbegindate \r\n",
        "# , weekenddate\r\n",
        "# , 'sessions' as metric\r\n",
        "# , count( distinct concat(a.user_pseudo_id,a.value.int_value)) as cnt\r\n",
        "#  from (select /*+BROADCAST(d)*/ property_brand, brand_country,device.category as device_category\r\n",
        "#  , user_pseudo_id\r\n",
        "#  , d.weekbegindate \r\n",
        "#  , d.weekenddate\r\n",
        "#  , explode(event_params)\r\n",
        "#                 from raw.ga4_events a, dates d\r\n",
        "#                 where event_date between d.weekbegindate and d.weekenddate) a\r\n",
        "#                 where a.key = 'ga_session_id' \r\n",
        "#   group by property_brand, brand_country,device_category,weekbegindate ,weekenddate) \r\n",
        "#   , t2 as (select /*+BROADCAST(d)*/ property_brand\r\n",
        "#   , brand_country\r\n",
        "#   ,device.category\r\n",
        "#   ,d.weekbegindate \r\n",
        "#   , d.weekenddate\r\n",
        "#   ,'carts'\r\n",
        "#   ,COUNT_IF(event_name = 'add_to_cart') AS adds_to_cart   \r\n",
        "#   from   raw.ga4_events a, dates d\r\n",
        "#                 where event_date between d.weekbegindate and d.weekenddate           \r\n",
        "#   group by property_brand, brand_country,device.category,d.weekbegindate, d.weekenddate)\r\n",
        "#   ,t3 as (select /*+BROADCAST(d)*/ property_brand\r\n",
        "#   , brand_country\r\n",
        "#   ,device.category\r\n",
        "#   ,d.weekbegindate\r\n",
        "#   , d.weekenddate\r\n",
        "#   ,'orders'\r\n",
        "#   ,COUNT_IF(event_name IN ('purchase', 'transaction'))\r\n",
        "# from   raw.ga4_events a, dates d\r\n",
        "#                 where event_date between d.weekbegindate and d.weekenddate            \r\n",
        "#   group by property_brand, brand_country,device.category,d.weekbegindate, d.weekenddate)\r\n",
        "#  , t5 as (select property_brand\r\n",
        "#  , brand_country\r\n",
        "#  ,device_category\r\n",
        "#  ,weekbegindate\r\n",
        "#  ,weekenddate\r\n",
        "#  , 'quantity'\r\n",
        "#  ,SUM(items.quantity) as qty\r\n",
        "#         from (select /*+BROADCAST(d)*/ property_brand, brand_country,device.category as device_category\r\n",
        "#         , d.weekbegindate,d.weekenddate, event_name, explode(items) as items\r\n",
        "#         from raw.ga4_events a, dates d\r\n",
        "#                 where event_date between d.weekbegindate and d.weekenddate   and  event_name = 'purchase' ) \r\n",
        "#                    group by property_brand, brand_country,device_category, weekbegindate,weekenddate\r\n",
        "#         )\r\n",
        "#   ,t6 as (select  /*+BROADCAST(d)*/  property_brand\r\n",
        "#  , brand_country\r\n",
        "#  ,device.category\r\n",
        "#  ,weekbegindate\r\n",
        "#  ,weekenddate\r\n",
        "#  , 'sales'\r\n",
        "#  , sum(nvl(ecommerce.purchase_revenue,0) - nvl(ecommerce.shipping_value,0) - nvl(ecommerce.tax_value,0))  as sales\r\n",
        "#     from raw.ga4_events a, dates d\r\n",
        "#                 where event_date between d.weekbegindate and d.weekenddate            \r\n",
        "#   group by property_brand, brand_country,device.category,d.weekbegindate, d.weekenddate )      \r\n",
        "#   ,t4 as (select * from t1 \r\n",
        "#   union all\r\n",
        "#   select * from t2 \r\n",
        "#   union all\r\n",
        "#   select * from t3\r\n",
        "#   union all \r\n",
        "#   select * from t5\r\n",
        "#   union all\r\n",
        "#   select * from t6)\r\n",
        "#   SELECT property_brand, brand_country,concat(device_category,'_',metric) as metric,cnt\r\n",
        "# ,weekbegindate,weekenddate FROM t4 where  device_category != 'smart tv'\"\"\")\r\n",
        "\r\n",
        "# df_stats = df_stats.groupBy('property_brand', 'brand_country','weekbegindate','weekenddate')\\\r\n",
        "# .pivot('metric')\\\r\n",
        "# .agg(F.first('cnt').alias('cnt'))\r\n",
        "\r\n",
        "# #df_stats.createOrReplaceTempView('ga4_weekly_stats')\r\n",
        "# display(df_stats)\r\n",
        "# df_stats=  df_stats.repartition('weekenddate','property_brand','brand_country')\\\r\n",
        "#     .write.format(\"delta\")\\\r\n",
        "#     .mode(\"overwrite\")\\\r\n",
        "#     .option(\"path\",f\"{gold_adls_path}GA4/weekly_stats\")\\\r\n",
        "#     .option(\"mergeSchema\", \"true\")\\\r\n",
        "#     .partitionBy('weekenddate','property_brand','brand_country')\\\r\n",
        "#     .saveAsTable('lakedb_gold.ga4_weekly_stats')\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "select weekenddate,count(*)\r\n",
        " from lakedb_gold.ga4_weekly_stats\r\n",
        " group by weekenddate\r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}