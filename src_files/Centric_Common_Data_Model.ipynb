{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Common Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "account_name = raw_adls_path.split('@')[1].split('.')[0]\n",
        "account_fqdn  = account_name + \".dfs.core.windows.net\"\n",
        "print(account_name)\n",
        "print(account_fqdn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Read Initial Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Container names\n",
        "gold_container = gold_adls_path.split('@')[0].split('//')[1]  # 'gold'\n",
        "bronze_container = bronze_adls_path.split('@')[0].split('//')[1]  # 'gold'\n",
        "raw_container = 'raw'\n",
        "\n",
        "# Parquet file paths for raw Centric views (Dev_Sample)\n",
        "ed_season_folder = '/Centric/ED_SEASON.parquet'\n",
        "ed_style_folder = '/Centric/ED_STYLE.parquet'\n",
        "ed_colorway_folder = '/Centric/ED_COLORWAY.parquet'\n",
        "ed_supplier_folder = '/Centric/ED_SUPPLIER.parquet'\n",
        "ed_factory_folder = '/Centric/ED_FACTORY.parquet'\n",
        "ed_product_source_folder = '/Centric/ED_PRODUCT_SOURCE.parquet'\n",
        "ed_user_folder = '/Centric/ED_USER.parquet'\n",
        "ed_product_size_folder = '/Centric/ED_PRODUCT_SIZE.parquet'\n",
        "ed_lookup_item_folder = '/Centric/ED_LOOKUP_ITEM.parquet'\n",
        "ed_apparel_bom_folder = '/Centric/ED_APPAREL_BOM.parquet'\n",
        "ed_apparel_bom_revision_folder = '/Centric/ED_APPAREL_BOM_REVISION.parquet'\n",
        "ed_sample_folder = '/Centric/ED_SAMPLE.parquet'\n",
        "\n",
        "\n",
        "#Additional Folders for Sales Sample\n",
        "\n",
        "ed_purchased_order_folder = \"/Centric/ED_PURCHASED_ORDER.parquet\"\n",
        "ed_shipment_folder = \"/Centric/ED_SHIPMENT.parquet\"\n",
        "en_shipment_folder = \"/Centric/EN_SHIPMENT.parquet\"\n",
        "ed_shipment_terms_folder = \"/Centric/ED_SHIPMENT_TERMS.parquet\"\n",
        "ed_lookup_item_folder = \"/Centric/ED_LOOKUP_ITEM.parquet\"\n",
        "ed_color_product_source_folder = \"/Centric/ED_COLOR_PRODUCT_SOURCE.parquet\"\n",
        "ed_order_folder = \"/Centric/ED_ORDER.parquet\"\n",
        "\n",
        "#joining Tables\n",
        "ed_PURCHASED_ORDER_PRODUCT_folder = \"/Centric/ED_PURCHASED_ORDER_PRODUCT.parquet\"\n",
        "ed_PURCHASED_ORDER_COLOR_folder = \"/Centric/ED_PURCHASED_ORDER_COLOR.parquet\"\n",
        "\n",
        "\n",
        "#additional folders for Colorway\n",
        "\n",
        "ed_color_specification_folder = \"/Centric/ED_COLOR_SPECIFICATION.parquet\"\n",
        "ed_category_1_folder          = \"/Centric/ED_CATEGORY_1.parquet\"\n",
        "ed_supplier_item_folder       = \"/Centric/ED_SUPPLIER_ITEM.parquet\"\n",
        "er_supplier_item_revision_folder = \"/Centric/ER_SUPPLIER_ITEM_REVISION.parquet\"\n",
        "ed_product_sales_region_folder = \"/Centric/ED_PRODUCT_SALES_REGION.parquet\"\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {},
      "source": [
        "# ED_SEASON\n",
        "ed_season_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_season_folder}\"\n",
        "print(f\"Reading data from: {ed_season_path}\")\n",
        "df_ed_season = spark.read.format(\"parquet\").load(ed_season_path)\n",
        "\n",
        "# ED_STYLE\n",
        "ed_style_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_style_folder}\"\n",
        "print(f\"Reading data from: {ed_style_path}\")\n",
        "df_ed_style = spark.read.format(\"parquet\").load(ed_style_path)\n",
        "\n",
        "# ED_COLORWAY\n",
        "ed_colorway_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_colorway_folder}\"\n",
        "print(f\"Reading data from: {ed_colorway_path}\")\n",
        "df_ed_colorway = spark.read.format(\"parquet\").load(ed_colorway_path)\n",
        "\n",
        "# ED_SUPPLIER\n",
        "ed_supplier_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_supplier_folder}\"\n",
        "print(f\"Reading data from: {ed_supplier_path}\")\n",
        "df_ed_supplier = spark.read.format(\"parquet\").load(ed_supplier_path)\n",
        "\n",
        "# ED_FACTORY\n",
        "ed_factory_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_factory_folder}\"\n",
        "print(f\"Reading data from: {ed_factory_path}\")\n",
        "df_ed_factory = spark.read.format(\"parquet\").load(ed_factory_path)\n",
        "\n",
        "# ED_PRODUCT_SOURCE\n",
        "ed_product_source_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_product_source_folder}\"\n",
        "print(f\"Reading data from: {ed_product_source_path}\")\n",
        "df_ed_product_source = spark.read.format(\"parquet\").load(ed_product_source_path)\n",
        "\n",
        "# ED_USER\n",
        "ed_user_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_user_folder}\"\n",
        "print(f\"Reading data from: {ed_user_path}\")\n",
        "df_ed_user = spark.read.format(\"parquet\").load(ed_user_path)\n",
        "\n",
        "# ED_PRODUCT_SIZE\n",
        "ed_product_size_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_product_size_folder}\"\n",
        "print(f\"Reading data from: {ed_product_size_path}\")\n",
        "df_ed_product_size = spark.read.format(\"parquet\").load(ed_product_size_path)\n",
        "\n",
        "# ED_LOOKUP_ITEM (Region & shipping lookup)\n",
        "ed_lookup_item_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_lookup_item_folder}\"\n",
        "print(f\"Reading data from: {ed_lookup_item_path}\")\n",
        "df_ed_lookup_item = spark.read.format(\"parquet\").load(ed_lookup_item_path)\n",
        "\n",
        "# ED_APPAREL_BOM\n",
        "ed_apparel_bom_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_apparel_bom_folder}\"\n",
        "print(f\"Reading data from: {ed_apparel_bom_path}\")\n",
        "df_ed_apparel_bom = spark.read.format(\"parquet\").load(ed_apparel_bom_path)\n",
        "\n",
        "# ED_APPAREL_BOM_REVISION\n",
        "ed_apparel_bom_revision_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_apparel_bom_revision_folder}\"\n",
        "print(f\"Reading data from: {ed_apparel_bom_revision_path}\")\n",
        "df_ed_apparel_bom_revision = spark.read.format(\"parquet\").load(ed_apparel_bom_revision_path)\n",
        "\n",
        "# ED_SAMPLE (used for Dev Samples)\n",
        "ed_sample_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_sample_folder}\"\n",
        "print(f\"Reading data from: {ed_sample_path}\")\n",
        "df_ed_sample = spark.read.format(\"parquet\").load(ed_sample_path)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "# ED_PURCHASED_ORDER\n",
        "ed_purchased_order_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_purchased_order_folder}\"\n",
        "print(f\"Reading data from: {ed_purchased_order_path}\")\n",
        "df_ed_purchased_order = spark.read.format(\"parquet\").load(ed_purchased_order_path)\n",
        "\n",
        "# ED_SHIPMENT\n",
        "ed_shipment_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_shipment_folder}\"\n",
        "print(f\"Reading data from: {ed_shipment_path}\")\n",
        "df_ed_shipment = spark.read.format(\"parquet\").load(ed_shipment_path)\n",
        "\n",
        "# EN_SHIPMENT\n",
        "en_shipment_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{en_shipment_folder}\"\n",
        "print(f\"Reading data from: {en_shipment_path}\")\n",
        "df_en_shipment = spark.read.format(\"parquet\").load(en_shipment_path)\n",
        "\n",
        "# ED_SHIPMENT_TERMS\n",
        "ed_shipment_terms_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_shipment_terms_folder}\"\n",
        "print(f\"Reading data from: {ed_shipment_terms_path}\")\n",
        "df_ed_shipment_terms = spark.read.format(\"parquet\").load(ed_shipment_terms_path)\n",
        "\n",
        "# ED_ORDER\n",
        "ed_order_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_order_folder}\"\n",
        "print(f\"Reading data from: {ed_order_path}\")\n",
        "df_ed_order = spark.read.format(\"parquet\").load(ed_order_path)\n",
        "\n",
        "\n",
        "\n",
        "# ED_COLOR_PRODUCT_SOURCE\n",
        "ed_color_product_source_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_color_product_source_folder}\"\n",
        "print(f\"Reading data from: {ed_color_product_source_path}\")\n",
        "df_ed_color_product_source = spark.read.format(\"parquet\").load(ed_color_product_source_path)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "# PURCHASED_ORDER_PRODUCT\n",
        "ed_PURCHASED_ORDER_PRODUCT_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_PURCHASED_ORDER_PRODUCT_folder}\"\n",
        "print(f\"Reading data from: {ed_PURCHASED_ORDER_PRODUCT_path}\")\n",
        "df_ed_PURCHASED_ORDER_PRODUCT = spark.read.format(\"parquet\").load(ed_PURCHASED_ORDER_PRODUCT_path)\n",
        "\n",
        "# PURCHASED_ORDER_COLOR\n",
        "ed_PURCHASED_ORDER_COLOR_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_PURCHASED_ORDER_COLOR_folder}\"\n",
        "print(f\"Reading data from: {ed_PURCHASED_ORDER_COLOR_path}\")\n",
        "df_ed_PURCHASED_ORDER_COLOR = spark.read.format(\"parquet\").load(ed_PURCHASED_ORDER_COLOR_path)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Tables for Colorway"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {},
      "source": [
        "# ---------- ED_COLOR_SPECIFICATION ----------\n",
        "ed_color_specification_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_color_specification_folder}\"\n",
        "print(f\"Reading data from: {ed_color_specification_path}\")\n",
        "df_ed_color_specification = spark.read.format(\"parquet\").load(ed_color_specification_path)\n",
        "\n",
        "# ---------- ED_CATEGORY_1 ----------\n",
        "ed_category_1_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_category_1_folder}\"\n",
        "print(f\"Reading data from: {ed_category_1_path}\")\n",
        "df_ed_category_1 = spark.read.format(\"parquet\").load(ed_category_1_path)\n",
        "\n",
        "# ---------- ED_SUPPLIER_ITEM ----------\n",
        "ed_supplier_item_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_supplier_item_folder}\"\n",
        "print(f\"Reading data from: {ed_supplier_item_path}\")\n",
        "df_ed_supplier_item = spark.read.format(\"parquet\").load(ed_supplier_item_path)\n",
        "\n",
        "# ---------- ER_SUPPLIER_ITEM_REVISION ----------\n",
        "er_supplier_item_revision_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{er_supplier_item_revision_folder}\"\n",
        "print(f\"Reading data from: {er_supplier_item_revision_path}\")\n",
        "df_er_supplier_item_revision = spark.read.format(\"parquet\").load(er_supplier_item_revision_path)\n",
        "\n",
        "# ---------- ED_PRODUCT_SALES_REGION ----------\n",
        "ed_product_sales_region_path = f\"abfss://{raw_container}@{account_name}.dfs.core.windows.net{ed_product_sales_region_folder}\"\n",
        "print(f\"Reading data from: {ed_product_sales_region_path}\")\n",
        "df_ed_product_sales_region = spark.read.format(\"parquet\").load(ed_product_sales_region_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Dimensions \n",
        "for Sales Sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Region"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "# Region/Lookup Dimension (for shipping & region details)\n",
        "dim_region_df = df_ed_lookup_item.select(\n",
        "    F.col(\"ID\").alias(\"RegionId\"),\n",
        "    F.col(\"WVW_DESTINATION_SHIP_TO_ADDRESS\").alias(\"ShipToAddress\"),\n",
        "    F.col(\"WVW_NOTIFY_PARTYAND_CONSIGNEE\").alias(\"NotifyPartyConsignee\"),\n",
        "    F.col(\"WVW_DISTRIBUTOR_NAMEAND_CONTACT\").alias(\"DistributorContact\"),\n",
        "    F.col(\"WVW_SHIPPING_METHOD_COURIEROR_FF\").alias(\"ShippingMethod\"),\n",
        "    F.col(\"WVW_SHIPPING_DETAILS_COURIER_AC_NOOR_FF_DETAILS\").alias(\"ShippingDetails\"),\n",
        "    F.col(\"WVW_HALF_PAIR_INDICATOR\").alias(\"HalfPairIndicator\")\n",
        ").distinct()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Lookup Item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "# Region/Lookup Dimension (for shipping & region details)\n",
        "dim_lookup_item_df = df_ed_lookup_item.select(\n",
        "    F.col(\"ID\").alias(\"RegionId\"),\n",
        "    F.col(\"ID\").alias(\"GenderId\"),\n",
        "    F.col(\"NODE_NAME\").alias(\"GenderText\"),\n",
        "    F.col(\"WVW_DESTINATION_SHIP_TO_ADDRESS\").alias(\"ShipToAddress\"),\n",
        "    F.col(\"WVW_NOTIFY_PARTYAND_CONSIGNEE\").alias(\"NotifyPartyConsignee\"),\n",
        "    F.col(\"WVW_DISTRIBUTOR_NAMEAND_CONTACT\").alias(\"DistributorContact\"),\n",
        "    F.col(\"WVW_SHIPPING_METHOD_COURIEROR_FF\").alias(\"ShippingMethod\"),\n",
        "    F.col(\"WVW_SHIPPING_DETAILS_COURIER_AC_NOOR_FF_DETAILS\").alias(\"ShippingDetails\"),\n",
        "    F.col(\"WVW_HALF_PAIR_INDICATOR\").alias(\"HalfPairIndicator\")\n",
        ").distinct()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Apparel BOM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "dim_apparel_bom = df_ed_apparel_bom.select(\n",
        "    F.col(\"NODE_NAME\").alias(\"BOMName\"),\n",
        "    F.col(\"WVW_BOM_STATUS\").alias(\"BOMStatus\"),\n",
        "    F.col(\"THE_CREATED_AT\").alias(\"BOMCreatedDate\"),\n",
        "    F.col(\"THE_CREATED_BY\").alias(\"BOMCreatedBy\"),\n",
        "    F.col(\"THE_PARENT_ID\").alias(\"apparel_bom_THE_PARENT_ID\")  # unchanged / keep as-is\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Apparel BOM Revsion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "dim_apparel_bom_revision = df_ed_apparel_bom_revision.select(\n",
        "    F.col(\"ID\").alias(\"BOMRevisionID\"),\n",
        "    F.col(\"THE_PARENT_ID\").alias(\"BOMID\"),\n",
        "    F.col(\"NODE_NAME\").alias(\"BOMRevisionName\"),\n",
        "    F.col(\"PHASE\").alias(\"BOMPhase\"),\n",
        "    F.col(\"STATE_001\").alias(\"BOMState\"),\n",
        "    F.col(\"STATE_CHANGE_TIME\").alias(\"BOMStateModifiedDate\"),\n",
        "    F.col(\"STATE_CHANGE_USER\").alias(\"BOMStateModifiedBy\"),\n",
        "    F.col(\"WVW_BOM_STAGE\").alias(\"BOMStage\"),\n",
        "    F.col(\"WVW_MATERIAL_SPEC_COMPLETE\").alias(\"MaterialSpecComplete\"),\n",
        "    F.col(\"WVW_COLOR_SPEC_COMPLETE\").alias(\"ColorSpecComplete\"),\n",
        "    F.col(\"IS_CURRENT\").alias(\"IsCurrentRevision\"),\n",
        "    F.col(\"REVISION_COMMENT\").alias(\"RevisionComment\"),\n",
        "    F.col(\"EFFECTIVE_START\").alias(\"EffectiveStartDate\"),\n",
        "    F.col(\"EFFECTIVE_END\").alias(\"EffectiveEndDate\"),\n",
        "    F.col(\"THE_CREATED_AT\").alias(\"apparel_bom_revision_CreatedDate\"),\n",
        "    F.col(\"THE_CREATED_BY\").alias(\"apparel_bom_revision_CreatedBy\"),\n",
        "    F.col(\"MODIFIED_AT\").alias(\"apparel_bom_revision_ModifiedDate\"),\n",
        "    F.col(\"MODIFIED_BY\").alias(\"apparel_bom_revision_ModifiedBy\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Supplier Item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Supplier Item Revision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Purchase Order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_ed_purchased_order.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "dim_purchase_order_df = df_ed_purchased_order.select(\n",
        "    F.col(\"WVW_SEASON\").alias(\"Season\"),\n",
        "    F.col(\"PO\").alias(\"PO\"),  # unchanged\n",
        "    F.col(\"WVW_PO_SAMPLE_TYPE\").alias(\"PO_Sample_Type\"),\n",
        "    F.col(\"PO_SUPPLIER\").alias(\"PO_Supplier\"),\n",
        "    F.col(\"PO_Factory\").alias(\"PO_Factory\"),\n",
        "    F.col(\"Node_Name\").alias(\"Supplier_PO\"),\n",
        "    F.col(\"STATE_001\").alias(\"PO_State\"),\n",
        "    F.col(\"STATE_CHANGE_TIME\").alias(\"PO_State_Changed_On\"))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### clean up columns State and PO_Sample_Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Columns to transform\n",
        "cols_to_clean = [\n",
        "    \"PO_Sample_Type\",\n",
        "    \"PO_State\"\n",
        "    \n",
        "]\n",
        "\n",
        "# Apply substring_index to each column\n",
        "for col in cols_to_clean:\n",
        "    dim_purchase_order_df = dim_purchase_order_df.withColumn(col, F.substring_index(F.col(col), \":\", -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Shipment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "2025-01-08 Bring in ID column for Shipment. We canuse it to get the size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "# Build the DataFrame with aliased columns\n",
        "dim_ed_shipment_df = df_ed_shipment.select(\n",
        "    F.col('ID').alias('Shipment_ID'),\n",
        "    F.col('WVW_PURCHASED_ORDER').alias('PO'), \n",
        "    F.col('WVW_SAMPLE_STATUS').alias('Shipment_Status'),\n",
        "    F.col('WVW_INITIAL_ORDER_QTY').alias('Initial_Shipment_Order_Qty'),\n",
        "    F.col('WVW_SMS_ORDER_QTY').alias('Shipment_Order_Qty'),\n",
        "    F.col('WVW_ETD').alias('shipment_ETD'),\n",
        "    F.col('WVW_SHIPPING_METHOD').alias('Shipping_Method'),\n",
        "    F.col('WVW_TRACKING_NUMBER').alias('Airway_Bill'),\n",
        "    F.col('WVW_DELAY_REASON_LIST').alias('Shipment_Delay_Reason'),\n",
        "    F.col('WVW_DELAY_REASON').alias('Shipment_Delay_Reason_Comment'),\n",
        "    F.col('THE_CREATED_AT').alias('shipment_Created_At'),\n",
        "    F.col('THE_PARENT_ID').alias('shipment_PARENT_ID'),\n",
        "    F.col('MODIFIED_AT').alias('shipment_MODIFIED_AT'),\n",
        "    F.col('MODIFIED_BY').alias('shipment_MODIFIED_BY'),\n",
        "    F.col('THE_CREATED_BY').alias('shipment_CREATED_BY'), \n",
        "    #F.col('NODE_NAME').alias('Shipment'),\n",
        "    F.col('WVW_DELAY').alias('Shipment_Delay'),\n",
        "    F.col('ACTUAL_QUANTITY').alias('shipment_ACTUAL_QUANTITY'),\n",
        "    F.col('ORDER_001').alias('shipment_OrderID')\n",
        "    #\n",
        "\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### dim_ed_shipment_df clean up columns \n",
        "Shipment_Status and Shipment_Delay_Reason"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Columns to transform\n",
        "cols_to_clean = [\n",
        "    \"Shipment_Status\",\n",
        "    \"Shipment_Delay_Reason\"\n",
        "    \n",
        "]\n",
        "\n",
        "# Apply substring_index to each column\n",
        "for col in cols_to_clean:\n",
        "    dim_ed_shipment_df = dim_ed_shipment_df.withColumn(col, F.substring_index(F.col(col), \":\", -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Shipment Terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "dim_shipment_terms_df = df_ed_shipment_terms.select(\n",
        "    F.col('ID').alias('Shipment_Terms_ID'), \n",
        "    F.col('REQUESTED_SHIP_DATE').alias('shipment_terms_REQUESTED_SHIP_DATE'),\n",
        "    F.col('WVW_REGION').alias('shipment_terms_Region_ID'),\n",
        "    F.col('Node_Name').alias('shipment_terms_Shipment')\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Join Shipment Terms\n",
        "In Shipment We need to bring in one column from shipment terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "s = dim_ed_shipment_df.alias(\"s\")\n",
        "t = dim_shipment_terms_df.alias(\"t\")\n",
        "\n",
        "# Choose the join type you need: \"inner\", \"left\", \"right\", \"full\"\n",
        "# Using left join here so you keep all shipments even if no terms match\n",
        "joined_df = s.join(\n",
        "    t,\n",
        "    col(\"s.shipment_PARENT_ID\") == col(\"t.Shipment_Terms_ID\"),\n",
        "    \"left\"\n",
        ")\n",
        "\n",
        "# All columns from both sides\n",
        "joined_all_cols = joined_df.select(\"s.*\", \"t.*\")\n",
        "\n",
        "dim_ed_shipment_df = joined_all_cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## EN_Shipment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "dim_en_shipment_df = df_en_shipment.filter(col(\"ATTR_ID\")==\"QuantityPerSize\").select(\n",
        "    col(\"ID\").alias(\"EN_ShipmentID\"),\n",
        "    col(\"MAP_KEY\").alias(\"Size\"),\n",
        "    col(\"NUM_VALUE\").alias(\"QuantityPerSize\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "## Product Source\n",
        "#dim_ed_product_source_df = df_ed_product_source.select()\n",
        "# We need less columns for the first report\n",
        "'''\n",
        "dim_product_source_df = df_ed_product_source.select(\n",
        "\n",
        "    col(\"ID\").alias(\"ProductSourceID\"),\n",
        "\n",
        "    col(\"NODE_NAME\").alias(\"Supplier\"),\n",
        "    col(\"WVW_SOURCING_DEVELOPER\").alias(\"SourcingDeveloper\"),\n",
        "    col(\"WVW_BUY_READY_PRODUCT\").alias(\"ProductSourceBuyReady\"),\n",
        "    col(\"WVW_PROD_BUY_RDY_SEASON\").alias(\"ProductBuyReadySeason\"),\n",
        "\n",
        "    col(\"WVW_FINAL_TEARDOWN_MEETING\").alias(\"FinalTearDownMeetingCompleted\"),\n",
        "    col(\"WVW_FINAL_TD_MEETING_DATE\").alias(\"FinalTearDownMeetingCompletedSetDate\"),\n",
        "    col(\"WVW_FINAL_TEARDOWN_MEETING_APP_BY\").alias(\"FinalTeardownMeetingCompletedApprovedBy\"),\n",
        "    col(\"WVW_FINAL_TEARDOWN_MEETING_USER\").alias(\"FinalTeardownMeetingCompletedSetBy\"),\n",
        "\n",
        "    col(\"WVW_T_1_MOD_SIZ_FIT_SENT\").alias(\"T1ModelSizeFitSent\"),\n",
        "    col(\"WVW_T_1_MOD_SIZ_FIT_SENT_USER\").alias(\"T1ModelSizeFitSentSetBy\"),\n",
        "    col(\"WVW_T_1_MOD_SIZ_FIT_SENT_DATE\").alias(\"T1ModelSizeFitSentSetDate\"),\n",
        "    col(\"WVW_T_1_MOD_SIZ_FIT_SENT_APP_BY\").alias(\"T1ModelSizeFitSentApprovedBy\"),\n",
        "\n",
        "    col(\"WVW_T_1_MODEL_SIZE_FIT_APP_PATTERN_LEV\").alias(\"T1ModelSizeFitApprovedAtPatternLevel\"),\n",
        "    col(\"WVW_T_1_MODEL_SIZE_FIT_APP_PATTERN_LEVEL_APP_BY\").alias(\"T1ModelSizeFitApprovedAtPatternLevelApprovedBy\"),\n",
        "    col(\"WVW_T_1_MODEL_SIZE_FIT_APP_PATTERN_LEVEL_USER\").alias(\"T1ModelSizeFitApprovedAtPatternLevelSetBy\"),\n",
        "    col(\"WVW_T_1_MODEL_SIZE_FIT_APP_PATTERN_LEV_DATE\").alias(\"T1ModelSizeFitApprovedAtPatternLevelSetDate\"),\n",
        "\n",
        "    col(\"WVW_ADOPT_COMPLETE\").alias(\"AdoptionCompleted\"),\n",
        "    col(\"WVW_ADOPT_COMPLETE_APP_BY\").alias(\"AdoptionCompletedApprovedBy\"),\n",
        "    col(\"WVW_ADOPT_COMPLETE_USER\").alias(\"AdoptionCompletedSetBy\"),\n",
        "    col(\"WVW_ADOPT_COMPLETE_DATE\").alias(\"AdoptionCompletedSetDate\"),\n",
        "\n",
        "    col(\"WVW_PTA_REL\").alias(\"PTAReleased\"),\n",
        "    col(\"WVW_PTA_REL_APP_BY\").alias(\"PTAReleasedApprovedBy\"),\n",
        "    col(\"WVW_PTA_REL_USER\").alias(\"PTAReleasedSetBy\"),\n",
        "    col(\"WVW_PTA_REL_DATE\").alias(\"PTAReleasedSetDate\"),\n",
        "\n",
        "    col(\"WVW_T_2_SIZE_WIDTH_TRIAL_SENT\").alias(\"T2SizeAndWidthTrialsSent\"),\n",
        "    col(\"WVW_T_2_SIZE_WIDTH_TRIAL_SENT_APP_BY\").alias(\"T2SizeAndWidthTrialsSentApprovedBy\"),\n",
        "    col(\"WVW_T_2_SIZE_WIDTH_TRIALS_APP_USER\").alias(\"T2SizeAndWidthTrialsSentSetBy\"),\n",
        "    col(\"WVW_T_2_SIZE_WIDTH_TRIAL_SENT_DATE\").alias(\"T2SizeAndWidthTrialsSentSetDate\"),\n",
        "\n",
        "    col(\"WVW_T_2_SIZE_WIDTH_TRIALS_APPR\").alias(\"T2SizeAndWidthTrialsApproval\"),\n",
        "    col(\"WVW_T_2_SIZE_WIDTH_TRIALS_APP_BY\").alias(\"T2SizeAndWidthTrialsApprovalApprovedBy\"),\n",
        "    col(\"WVW_T_2_SIZE_WIDTH_TRIALS_APP_USER\").alias(\"T2SizeAndWidthTrialsApprovalSetBy\"),\n",
        "    col(\"WVW_T_2_SIZE_WIDTH_TRIALS_APPR_DATE\").alias(\"T2SizeAndWidthTrialsApprovalSetDate\"),\n",
        "\n",
        "    col(\"WVW_PROD_MOLD_OPEN_OPT\").alias(\"ProductionMoldOpenedOptional\"),\n",
        "    col(\"WVW_PROD_MOLD_OPEN_OPT_APP_BY\").alias(\"ProductionMoldOpenedOptionalApprovedBy\"),\n",
        "    col(\"WVW_PROD_MOLD_OPEN_OPT_USER\").alias(\"ProductionMoldOpenedOptionalSetBy\"),\n",
        "    col(\"WVW_PROD_MOLD_OPEN_OPT_DATE\").alias(\"ProductionMoldOpenedOptionalSetDate\"),\n",
        "\n",
        "    col(\"WVW_FULL_SIZ_PROD_TOOL_COM\").alias(\"FullSizeOfProductionToolingCompleted\"),\n",
        "    col(\"WVW_FULL_SIZ_PROD_TOOL_COM_APP_BY\").alias(\"FullSizeOfProductionToolingCompletedApprovedBy\"),\n",
        "    col(\"WVW_FULL_SIZ_PROD_TOOL_COM_USER\").alias(\"FullSizeOfProductionToolingCompletedSetBy\"),\n",
        "    col(\"WVW_FULL_SIZ_PROD_TOOL_COM_DATE\").alias(\"FullSizeOfProductionToolingCompletedSetDate\"),\n",
        "\n",
        "    col(\"WVW_T_3_TRIALS_SENT_OPT\").alias(\"T3TrialsSentOptional\"),\n",
        "    col(\"WVW_T_3_TRIALS_SENT_OPT_APP_BY\").alias(\"T3TrialsSentOptionalApprovedBy\"),\n",
        "    col(\"WVW_T_3_TRIALS_SENT_OPTIONAL_USER\").alias(\"T3TrialsSentOptionalSetBy\"),\n",
        "    col(\"WVW_T_3_TRIALS_SENT_OPT_DATE\").alias(\"T3TrialsSentOptionalSetDate\"),\n",
        "\n",
        "    col(\"WVW_T_3_TRIALS_APPROVED_OPT\").alias(\"T3TrialsApprovedOptional\"),\n",
        "    col(\"WVW_T_3_TRIALS_APPROVED_OPT_APP_BY\").alias(\"T3TrialsApprovedOptionalApprovedBy\"),\n",
        "    col(\"WVW_T_3_TRIALS_APPROVED_OPTIONAL_USER\").alias(\"T3TrialsApprovedOptionalSetBy\"),\n",
        "    col(\"WVW_T_3_TRIALS_APPROVED_OPT_DATE\").alias(\"T3TrialsApprovedOptionalSetDate\"),\n",
        "\n",
        "    col(\"WVW_GORE_TEX_APP\").alias(\"GoreTexTestPassOptional\"),\n",
        "    col(\"WVW_GORE_TEX_APP_DATE\").alias(\"GoreTexTestPassOptionalSetDate\"),\n",
        "    col(\"WVW_GORE_TEX_APPROVED_BY\").alias(\"GoreTexTestPassApprovedBy\"),\n",
        "    col(\"WVW_GORE_TEX_APPROVED_USER\").alias(\"GoreTexTestPassSetBy\"),\n",
        "\n",
        "    col(\"WVW_CAT_INC_APP\").alias(\"CATIncApproval\"),\n",
        "    col(\"WVW_CAT_INC_APPRO_APP_BY\").alias(\"CATIncApprovalApprovedBy\"),\n",
        "    col(\"WVW_CAT_INC_APP_USER\").alias(\"CATIncApprovalSetBy\"),\n",
        "    col(\"WVW_CAT_INC_APP_DATE\").alias(\"CATIncApprovalSetDate\"),\n",
        "\n",
        "    col(\"WVW_32_WHOLE_SHOE_TEST\").alias(\"32WholeShoeTesting\"),\n",
        "    col(\"WVW_32_WHOLE_SHOE_TEST_APPROVED_BY\").alias(\"32WholeShoeTestingApprovedBy\"),\n",
        "    col(\"WVW_32_WHOLE_SHOE_TESTING_USER\").alias(\"32WholeShoeTestingSetBy\"),\n",
        "    col(\"WVW_32_WHOLE_SHOE_TEST_DATE\").alias(\"32WholeShoeTestingSetDate\"),\n",
        "\n",
        "    col(\"WVW_T_4_MEASUREMENT_GRADING_CHAR\").alias(\"T4MeasurementGradingChart\"),\n",
        "    col(\"WVW_T_4_MEASURE_GRADING_CHAR_APP_BY\").alias(\"T4MeasurementGradingChartApprovedBy\"),\n",
        "    col(\"WVW_T_4_MEASUREMENT_GRADING_CHART_USER\").alias(\"T4MeasurementGradingChartSetBy\"),\n",
        "    col(\"WVW_T_4_MEASURE_GRADING_CHART_DATE\").alias(\"T4MeasurementGradingChartSetDate\"),\n",
        "\n",
        "    col(\"WVW_SOP_DOCUMENTS_COMP\").alias(\"SOPDocumentsCompleted\"),\n",
        "    col(\"WVW_SOP_DOCUMENTS_COMP_APP_BY\").alias(\"SOPDocumentsCompletedApprovedBy\"),\n",
        "    col(\"WVW_SOP_DOCUMENTS_COMPLETED_USER\").alias(\"SOPDocumentsCompletedSetBy\"),\n",
        "    col(\"WVW_SOP_DOCUMENTS_COMP_DATE\").alias(\"SOPDocumentsCompletedSetDate\"),\n",
        "\n",
        "    col(\"WVW_T_4_TRIAL_RUN\").alias(\"T4TrialsRun\"),\n",
        "    col(\"WVW_T_4_TRIAL_RUN_APP_BY\").alias(\"T4TrialsRunApprovedBy\"),\n",
        "    col(\"WVW_T_4_TRIALS_RUN_USER\").alias(\"T4TrialsRunSetBy\"),\n",
        "    col(\"WVW_T_4_TRIAL_RUN_DATE\").alias(\"T4TrialsRunSetDate\"),\n",
        "\n",
        "    col(\"WVW_T_4_TRIALS_SENT_OPT\").alias(\"T4TrialsSentOptional\"),\n",
        "    col(\"WVW_T_4_TRIALS_SENT_OPT_APP_BY\").alias(\"T4TrialsSentOptionalApprovedBy\"),\n",
        "    col(\"WVW_T_4_TRIALS_SENT_OPT_USER\").alias(\"T4TrialsSentOptionalSetBy\"),\n",
        "    col(\"WVW_T_4_TRIALS_SENT_OPT_DATE\").alias(\"T4TrialsSentOptionalSetDate\"),\n",
        "\n",
        "    col(\"WVW_T_4_APP\").alias(\"T4Approved\"),\n",
        "    col(\"WVW_T_4_APP_BY\").alias(\"T4ApprovedBy\"),\n",
        "    col(\"WVW_T_4_APPROVED_USER\").alias(\"T4ApprovedSetBy\"),\n",
        "    col(\"WVW_T_4_APP_DATE\").alias(\"T4ApprovedSetDate\"),\n",
        "\n",
        "    col(\"WVW_MANU_EXF_DATE\").alias(\"ManufacturingEXFDate\"),\n",
        "\n",
        "    col(\"THE_CREATED_AT\").alias(\"ProductSourceCreated\"),\n",
        "    col(\"THE_CREATED_BY\").alias(\"ProductSourceCreatedBy\"),\n",
        "    col(\"MODIFIED_AT\").alias(\"ProductSourceModified\"),\n",
        "    col(\"MODIFIED_BY\").alias(\"ProductSourceModifiedBy\"))\n",
        "    '''\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Color Product Source \n",
        "df_ed_color_product_source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {},
      "source": [
        "dim_ed_color_product_source_df = (\n",
        "    df_ed_color_product_source\n",
        "    .select(\n",
        "        # ------------------------------------------------------------------\n",
        "        # Existing columns \n",
        "        # ------------------------------------------------------------------\n",
        "        F.col(\"ID\").alias(\"ColorProductSourceId\"),\n",
        "        F.col(\"THE_PARENT_ID\").alias(\"ParentColorwayId\"),\n",
        "        F.col(\"COLORWAY\").alias(\"Colorway\"),\n",
        "        F.col(\"ACTIVE\").alias(\"Active\"),\n",
        "        F.col(\"WVW_UNIQUE_ID\").alias(\"UniqueID\"),\n",
        "        F.col(\"NODE_NAME\").alias(\"ColorwayNodeName\"),\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # Buy Ready / Planning\n",
        "        # ------------------------------------------------------------------\n",
        "        F.col(\"WVW_BUY_READY_DUE_DATE\").alias(\"BuyReadyDueDate\"),\n",
        "        F.col(\"WVW_TARGET_SPEC_LOCK_DATE\").alias(\"TargetSpecLockDate\"),\n",
        "        F.col(\"WVW_BUY_RDY_APPR_DATE\").alias(\"BuyReadyApprovedDate\"),\n",
        "        F.col(\"WVW_WBS_BUY_READY\").alias(\"WBSBuyReady\"),\n",
        "        F.col(\"WVW_HISTORICAL_BUY_READY\").alias(\"HistoricalBuyReady\"),\n",
        "        F.col(\"WVW_ORIGINAL_BUY_READY_SEASON\").alias(\"OriginalBuyReadySeason\"),\n",
        "        F.col(\"WVW_COLOR_BUY_RDY_SEASON\").alias(\"ColorBuyReadySeason\"),\n",
        "        F.col(\"WVW_BUY_READY_COMMENT\").alias(\"BuyReadyComment\"),\n",
        "        F.col(\"WVW_EST_BUY_REA_COM_DATE\").alias(\"EstimatedBuyReadyCompletionDate\"),\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # Final Spec Lock\n",
        "        # ------------------------------------------------------------------\n",
        "        F.col(\"WVW_FIN_SPEC_LOCKED_UPL\").alias(\"FinalSpecLocked\"),\n",
        "        F.col(\"WVW_FIN_SPEC_LOCKED_UPL_USER\").alias(\"FinalSpecLockedBy\"),\n",
        "        F.col(\"WVW_FIN_SPEC_LOCKED_UPL_DATE\").alias(\"FinalSpecLockedDate\"),\n",
        "        F.col(\"WVW_FIN_SPEC_LOCK_UPL_APP_BY\").alias(\"FinalSpecLockedApprovedBy\"),\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # Material Master / SWAT\n",
        "        # ------------------------------------------------------------------\n",
        "        F.col(\"WVW_MAT_MAST_SWAT_APP\").alias(\"MaterialMasterSWATApproved\"),\n",
        "        F.col(\"WVW_MAT_MAST_SWAT_APP_USER\").alias(\"MaterialMasterSWATApprovedBy\"),\n",
        "        F.col(\"WVW_MAT_MAST_SWAT_APP_DATE\").alias(\"MaterialMasterSWATApprovedDate\"),\n",
        "        F.col(\"WVW_MAT_MAST_SWAT_APP_APP_BY\").alias(\"MaterialMasterSWATApprovedApprovedBy\"),\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # Final Cost Lock\n",
        "        # ------------------------------------------------------------------\n",
        "        F.col(\"WVW_FIN_COST_LOCK_CBD_UPL\").alias(\"FinalCostLockApproved\"),\n",
        "        F.col(\"WVW_FIN_COST_LOCK_CBD_UPL_USER\").alias(\"FinalCostLockApprovedBy\"),\n",
        "        F.col(\"WVW_FIN_COST_LOCK_CBD_UPL_DATE\").alias(\"FinalCostLockApprovedDate\"),\n",
        "        F.col(\"WVW_FIN_COS_LOCK_CBD_UPL_APP_BY\").alias(\"FinalCostLockApprovedApprovedBy\"),\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # Confirmation Sample\n",
        "        # ------------------------------------------------------------------\n",
        "        F.col(\"WVW_CONF_SAMP_SENT\").alias(\"ConfirmationSampleSent\"),\n",
        "        F.col(\"WVW_CONF_SAMP_SENT_USER\").alias(\"ConfirmationSampleSentBy\"),\n",
        "        F.col(\"WVW_CONF_SAMP_SENT_DATE\").alias(\"ConfirmationSampleSentDate\"),\n",
        "        F.col(\"WVW_CONF_SAMP_SEN_APP_BY\").alias(\"ConfirmationSampleSentApprovedBy\"),\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # CFM\n",
        "        # ------------------------------------------------------------------\n",
        "        F.col(\"WVW_USCFM_RCVD\").alias(\"USCFMReceived\"),\n",
        "        F.col(\"WVW_US_DEV_CFM_APR\").alias(\"USDeveloperCFMApproved\"),\n",
        "        F.col(\"WVW_CFM_DEV_NOTES\").alias(\"CFMDeveloperNotes\"),\n",
        "        F.col(\"WVW_US_COLOR_CFM_APR\").alias(\"USColorCFMApproved\"),\n",
        "        F.col(\"WVW_CFM_COLOR_NOTES\").alias(\"CFMColorNotes\"),\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # SWAT / Testing / RS\n",
        "        # ------------------------------------------------------------------\n",
        "        F.col(\"WVW_SWAT_REF_FOL_COMP\").alias(\"SWATReferenceFollowupCompleted\"),\n",
        "        F.col(\"WVW_SWAT_REF_FOL_COMP_USER\").alias(\"SWATReferenceFollowupCompletedBy\"),\n",
        "        F.col(\"WVW_SWAT_REF_FOL_COMP_DATE\").alias(\"SWATReferenceFollowupCompletedDate\"),\n",
        "        F.col(\"WVW_SWAT_REF_FOL_COM_APP_BY\").alias(\"SWATReferenceFollowupCompletedApprovedBy\"),\n",
        "\n",
        "        F.col(\"WVW_31_PHY_TEST_COMP\").alias(\"PhysicalTest31Completed\"),\n",
        "        F.col(\"WVW_31_PHY_TEST_COMP_USER\").alias(\"PhysicalTest31CompletedBy\"),\n",
        "        F.col(\"WVW_31_PHY_TEST_COMP_DATE\").alias(\"PhysicalTest31CompletedDate\"),\n",
        "        F.col(\"WVW_31_PHY_TEST_COMP_APP_BY\").alias(\"PhysicalTest31CompletedApprovedBy\"),\n",
        "\n",
        "        F.col(\"WVW_FULL_RS\").alias(\"FullRS\"),\n",
        "        F.col(\"WVW_FULL_RS_USER\").alias(\"FullRSBy\"),\n",
        "        F.col(\"WVW_FULL_RS_DATE\").alias(\"FullRSDate\"),\n",
        "        F.col(\"WVW_FULL_RS_APP_BY\").alias(\"FullRSApprovedBy\"),\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # Tech Pack / Sample Lifecycle\n",
        "        # ------------------------------------------------------------------\n",
        "        F.col(\"WVW_INITIAL_COLORWAY_TECH_PACK_RELEASED\").alias(\"InitialColorwayTechPackReleased\"),\n",
        "        F.col(\"WVW_INITIAL_COLORWAY_TECH_PACK_RELEASED_USER\").alias(\"InitialColorwayTechPackReleasedBy\"),\n",
        "        F.col(\"WVW_INITIAL_COLORWAY_TECH_PACK_RELEASED_DATE\").alias(\"InitialColorwayTechPackReleasedDate\"),\n",
        "        F.col(\"WVW_INITIAL_COLORWAY_TECH_PACK_RELEASED_APP_BY\").alias(\"InitialColorwayTechPackReleasedApprovedBy\"),\n",
        "\n",
        "        F.col(\"WVW_INITIAL_COLORWAY_SAMPLE_COMPLETEDAND_SENT\").alias(\"InitialColorwaySampleCompletedAndSent\"),\n",
        "        F.col(\"WVW_INITIAL_COLORWAY_SAMPLE_COMP_SENT_USER\").alias(\"InitialColorwaySampleCompletedAndSentBy\"),\n",
        "        F.col(\"WVW_INITIAL_COLORWAY_SAMPLE_COMPLETED_SENT_DATE\").alias(\"InitialColorwaySampleCompletedAndSentDate\"),\n",
        "        F.col(\"WVW_INITIAL_COLORWAY_SAMPLE_COMP_SENT_APP_BY\").alias(\"InitialColorwaySampleCompletedAndSentApprovedBy\"),\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # Retail / PO / Launch\n",
        "        # ------------------------------------------------------------------\n",
        "        F.col(\"WVW_PO_ISSUED_TARGET_DATE\").alias(\"POIssuedTargetDate\"),\n",
        "        F.col(\"WVW_PO_ISSUED_ACTUAL_DATE\").alias(\"POIssuedActualDate\"),\n",
        "        F.col(\"WVW_LAUNCH_XF_DATE\").alias(\"LaunchXFDate\"),\n",
        "        F.col(\"WVW_RETAIL_DATE_CANADA\").alias(\"RetailDateCanada\"),\n",
        "        F.col(\"WVW_RETAIL_DATE_UK\").alias(\"RetailDateUK\"),\n",
        "        F.col(\"WVW_RETAIL_DATE_US\").alias(\"RetailDateUS\"),\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # Audit\n",
        "        # ------------------------------------------------------------------\n",
        "        F.col(\"THE_CREATED_AT\").alias(\"CreatedAt\"),\n",
        "        F.col(\"THE_CREATED_BY\").alias(\"CreatedBy\"),\n",
        "        F.col(\"MODIFIED_AT\").alias(\"ModifiedAt\"),\n",
        "        F.col(\"MODIFIED_BY\").alias(\"ModifiedBy\"),\n",
        "    )\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "#display(df_ed_order)\n",
        "\n",
        "dim_ed_order_df = df_ed_order.select('ID',\n",
        "                                  'PO','WVW_PO_SUPPLIER_VALIDATION','PO_COLOR','PO_PRODUCT')\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Season"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# ---------------------------\n",
        "# DIMENSION TABLES (extended)\n",
        "# ---------------------------\n",
        "\n",
        "# Season Dimension (unchanged)\n",
        "dim_season_df = df_ed_season.select(\n",
        "    F.col(\"ID\").alias(\"SeasonId\"),\n",
        "    F.col(\"NODE_NAME\").alias(\"SeasonName\"),\n",
        "    F.col(\"WVW_SEASON_TYPE\").alias(\"SeasonType\"),\n",
        "    F.col(\"WVW_YEAR\").alias(\"Year\")\n",
        ").distinct()\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Colorway"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Colorway Dimension (extended)\n",
        "dim_colorway_df = (\n",
        "    df_ed_colorway.select(\n",
        "        F.col(\"ID\").alias(\"ColorwayId\"),\n",
        "        F.col(\"THE_PARENT_ID\").alias(\"colorway_ParentID\"),\n",
        "        F.col(\"PARENT_SEASON\").alias(\"colorway_SeasonId\"),\n",
        "        F.col(\"STYLE_001\").alias(\"colorway_StyleId\"),\n",
        "\n",
        "        # existing fields\n",
        "        F.col(\"WVW_SEASONAL_COLORWAY_CODE\").alias(\"colorway_SeasonalColorwayCode\"),\n",
        "        F.col(\"WVW_STOCK_NUMBER\").alias(\"colorway_StockNumber\"),\n",
        "        F.col(\"WVW_COLORWAY_WORKING_NAME\").alias(\"colorway_DevelopmentColorwayName\"),\n",
        "        F.col(\"WVW_STATUS\").alias(\"ColorwayStatus\"),\n",
        "        F.coalesce(F.col(\"WVW_ACTIVE_USER_SELECT\").alias(\"colorway_WVW_ACTIVE_USER_SELECT\"), \n",
        "        F.col(\"ACTIVE\")).alias(\"colorway_ActiveFlag\"),\n",
        "\n",
        "        # new fields\n",
        "        F.col(\"WVW_PRODUCT_TYPE_CLASS\").alias(\"colorway_ProductTypeClassification\"),\n",
        "        F.col(\"WVW_PRODUCT_LINE\").alias(\"colorway_ProductLineSeasonless\"),\n",
        "\n",
        "        # Fields for Sales Sample\n",
        "        F.col(\"WVW_DESIGNER\").alias(\"colorway_Designer\"),\n",
        "        F.col(\"WVW_DEVELOPER\").alias(\"colorway_Developer\"),\n",
        "        F.col(\"NODE_NAME\").alias(\"colorway_Development_Colorway_Name\"),\n",
        "        F.col(\"WVW_SEASONAL_COLORWAY_CODE\").alias(\"colorway_Seasonal_Colorway_Code\"),\n",
        "        F.col(\"WVW_PRODUCT_TYPE_CLASS\").alias(\"colorway_Product_Type_classification\"),\n",
        "        \n",
        "        F.col(\"WVW_PRODUCT_LINE\").alias(\"colorway_Product_Line_Seasonless\"),\n",
        "        #ACTIVE\n",
        "        F.col(\"ACTIVE\").alias(\"CW_Active\"),\n",
        "\n",
        "        F.col(\"WVW_DEVELOPMENT_TYPE\").alias(\"colorway_DevelopmentType\"),\n",
        "        F.col(\"WVW_COLORWAY_CODE\").alias(\"ColorwayCode\"),\n",
        "        F.col(\"COLOR_SPECIFICATION\").alias(\"colorway_ColorSpecification\"),\n",
        "\n",
        "        F.col(\"WVW_LAUNCH_MONTH\").alias(\"colorway_LaunchMonth\"),\n",
        "        F.col(\"WVW_LAUNCH_PRIORITY_LEVEL\").alias(\"colorway_LaunchPriorityLevel\"),\n",
        "        F.col(\"WVW_SAP_PATTERN_NAME\").alias(\"colorway_SAPPatternName\"),\n",
        "\n",
        "        F.col(\"WVW_FRANCHISE\").alias(\"colorway_Franchise\"),\n",
        "        F.col(\"WVW_SUB_FRANCHISE\").alias(\"colorway_SubFranchise\"),\n",
        "        F.col(\"WVW_CONSUMER_TERRITORY\").alias(\"colorway_ConsumerTerritory\"),\n",
        "        F.col(\"WVW_MATERIAL_DESCRIPTION\").alias(\"colorway_MaterialDescription\"),\n",
        "        F.col(\"WVW_STORY_TYPE\").alias(\"colorway_StoryType\"),\n",
        "        F.col(\"WVW_3_D_DEVELOPER\").alias(\"colorway_Developer3D\"),\n",
        "\n",
        "        F.col(\"INTRO_DATE\").alias(\"colorway_IntroDate\"),\n",
        "        F.col(\"PRODUCTION_MIN\").alias(\"colorway_ProductionMin\"),\n",
        "        F.col(\"WVW_ORIGINAL_ENDOF_LIFE\").alias(\"colorway_OriginalEndOfLife\"),\n",
        "        F.col(\"WVW_REVISED_ENDOF_LIFE\").alias(\"colorway_RevisedEndOfLife\"),\n",
        "        F.col(\"WVW_LIFESTYLE_PHASE\").alias(\"colorway_LifestylePhase\"),\n",
        "        F.col(\"WVW_PRODUCT_TIER\").alias(\"colorway_ProductTier\"),\n",
        "\n",
        "        F.col(\"VIP_ATTRIBUTE_DISTRIBUTION_C\").alias(\"colorway_vip_Attribute_DistributionC\"),\n",
        "        F.col(\"WVW_CUSTOMER\").alias(\"colorway_Customer\"),\n",
        "        F.col(\"WVW_ADD_BACK_CUSTOMER\").alias(\"colorway_AddBackCustomer\"),\n",
        "        F.col(\"WVW_ADD_BACK_FLAG\").alias(\"colorway_AddBackFlag\"),\n",
        "\n",
        "        F.col(\"WVW_PROTO_COLOR_NOTES\").alias(\"colorway_ProtoColorNotes\"),\n",
        "        F.col(\"WVW_ISR_COLOR_NOTES\").alias(\"colorway_ISRColorNotes\"),\n",
        "        F.col(\"WVW_LDM_COLOR_NOTES\").alias(\"colorway_LDMColorNotes\"),\n",
        "        F.col(\"WVW_CFM_COLOR_NOTES\").alias(\"colorway_CFMColorNotes\"),\n",
        "        F.col(\"WVW_PICTOGRAM_LABEL_ID\").alias(\"colorway_PictogramLabelID\"),\n",
        "\n",
        "        F.col(\"CARRY_OVER\").alias(\"colorway_CarryOver\"),\n",
        "        F.col(\"THE_CREATED_AT\").alias(\"colorway_CreatedAt\"),\n",
        "        F.col(\"THE_CREATED_BY\").alias(\"colorway_CreatedBy\"),\n",
        "        F.col(\"MODIFIED_AT\").alias(\"colorway_ModifiedAt\"),\n",
        "        F.col(\"MODIFIED_BY\").alias(\"colorway_ModifiedBy\"),\n",
        "\n",
        "\n",
        "\n",
        "    )\n",
        "    .distinct()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Style Dimension (extended)\n",
        "dim_style_df = (\n",
        "    df_ed_style.select(\n",
        "        F.col(\"ID\").alias(\"StyleId\"),\n",
        "        F.col(\"PARENT_SEASON\").alias(\"style_SeasonId\"),\n",
        "\n",
        "        \n",
        "        F.col(\"WVW_BRAND\").alias(\"style_Brand_Composit\"),\n",
        "        F.col(\"WVW_SEASONAL_STYLE_CODE\").alias(\"style_SeasonalStyleCode\"),\n",
        "        F.col(\"WVW_STYLE_NAME\").alias(\"style_StyleName\"),\n",
        "        F.col(\"NODE_NAME\").alias(\"style_Style\"),\n",
        "        F.col(\"WVW_STYLE_NAME\").alias(\"Style_Working_Name\"),\n",
        "        F.col(\"WVW_STATUS\").alias(\"StyleStatus\"),\n",
        "        F.col(\"WVW_GENDER\").alias(\"style_Gender\"),\n",
        "        F.col(\"WVW_DIVISION_HIERARCHY\").alias(\"style_Division_Composit\"),\n",
        "        F.col(\"CATEGORY_1\").alias(\"style_Brand\"),\n",
        "        F.col(\"COLLECTION\").alias(\"style_Division\"),\n",
        "\n",
        "        \n",
        "        F.col(\"WVW_PATTERN_NAME\").alias(\"style_FinalPatternNameSeasonless\"),\n",
        "        F.col(\"ACTIVE\").alias(\"style_PAActive\"),\n",
        "        F.col(\"WVW_DESIGNER\").alias(\"style_Designer\"),\n",
        "        F.col(\"WVW_DEVELOPER\").alias(\"style_Developer\")\n",
        "    )\n",
        "    .distinct()\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Transformations Style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import split, col\n",
        "\n",
        "# Split the Brand column by ':'\n",
        "split_col = split(col(\"style_Brand_Composit\"), \":\")\n",
        "\n",
        "# Transform the DataFrame\n",
        "dim_style_df_transformed = (\n",
        "    dim_style_df\n",
        "    .withColumn(\"style_BrandID\", split_col.getItem(1))       # Extract the second element (index 1)\n",
        "    .withColumn(\"style_BrandText\", split_col.getItem(2))     # Extract the third element (index 2)\n",
        "    #.withColumn(\"Brand\", split_col.getItem(2))         # Replace Brand with just the text\n",
        ")\n",
        "dim_style_df_transformed = dim_style_df_transformed.drop(\"Brand\")\n",
        "\n",
        "# Split the Division\n",
        "\n",
        "dim_style_df_transformed = (\n",
        "    dim_style_df_transformed\n",
        "    .withColumn(\"style_DivisionCode\", F.element_at(F.split(F.col(\"style_Division_Composit\"), \":\"), 2))\n",
        "    .withColumn(\"style_DivisionText\", F.element_at(F.split(F.col(\"style_Division_Composit\"), \":\"), 3))\n",
        "    .drop(\"style_Division_Composit\")  # remove the raw column if you only want the derived fields\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "dim_style_df_transformed = (\n",
        "    dim_style_df_transformed\n",
        "        .withColumn(\"StyleStatus\", F.substring_index(F.col(\"StyleStatus\"), \":\", -1))\n",
        "        \n",
        ")\n",
        "\n",
        "dim_style_df = dim_style_df_transformed "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Add Gender Text to Style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "style = dim_style_df.alias(\"s\")\n",
        "lookup = dim_lookup_item_df.select(\"GenderId\", \"GenderText\").alias(\"l\")\n",
        "\n",
        "# Left join to preserve all style rows; add only GenderText from lookup\n",
        "result_df = (\n",
        "    style\n",
        "    .join(lookup, col(\"s.style_Gender\") == col(\"l.GenderId\"), \"left\")\n",
        "    .select(\"s.*\", col(\"l.GenderText\"))\n",
        ")\n",
        "dim_style_df = result_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Join Colorway to Style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "cw_alias  = \"cw\"\n",
        "sty_alias = \"sty\"\n",
        "\n",
        "cw  = dim_colorway_df.alias(cw_alias)\n",
        "sty = dim_style_df.alias(sty_alias)\n",
        "\n",
        "# Join Colorway to Style\n",
        "# Assumption: Colorway has ParentID pointing to StyleId (same mapping you used before via cps.ParentID)\n",
        "joined_cw_sty = (\n",
        "    cw.join(\n",
        "        sty,\n",
        "        col(f\"{cw_alias}.colorway_ParentID\") == col(f\"{sty_alias}.StyleId\"),\n",
        "        how=\"left\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {},
      "source": [
        "'''\n",
        "#Old Code\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "cw_alias  = \"cw\"\n",
        "sty_alias = \"sty\"\n",
        "\n",
        "cw  = dim_colorway_df.alias(cw_alias)\n",
        "sty = dim_style_df.alias(sty_alias)\n",
        "\n",
        "# Join Colorway to Style\n",
        "# Assumption: Colorway has ParentID pointing to StyleId (same mapping you used before via cps.ParentID)\n",
        "joined_cw_sty = (\n",
        "    cw.join(\n",
        "        sty,\n",
        "        col(f\"{cw_alias}.ParentID\") == col(f\"{sty_alias}.StyleId\"),\n",
        "        how=\"inner\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Columns to take from Style (same list you already use)\n",
        "style_cols = [\n",
        "    \"SeasonalStyleCode\",\n",
        "    \"StyleName\",\n",
        "    \"Style\",\n",
        "    \"Style_Working_Name\",\n",
        "    \"StyleStatus\",\n",
        "    \"Gender\",\n",
        "    \"BrandID\",\n",
        "    \"BrandText\",\n",
        "    \"Division\",\n",
        "    \"FinalPatternNameSeasonless\",\n",
        "    \"PAActive\",\n",
        "    \"Designer\",\n",
        "    \"Developer\",\n",
        "    \"GenderText\",\n",
        "]\n",
        "\n",
        "cw_cols  = dim_colorway_df.columns\n",
        "sty_cols = style_cols\n",
        "\n",
        "def norm(c: str) -> str:\n",
        "    return c.lower()\n",
        "\n",
        "# If any style col name collides (case-insensitive) with a colorway col, prefix style col with \"sty_\"\n",
        "cw_norm_set = {norm(c) for c in cw_cols}\n",
        "\n",
        "select_exprs = []\n",
        "\n",
        "# Keep all colorway columns as-is\n",
        "select_exprs += [F.col(f\"{cw_alias}.{c}\").alias(c) for c in cw_cols]\n",
        "\n",
        "# Add style columns, prefix only if collision\n",
        "select_exprs += [\n",
        "    F.col(f\"{sty_alias}.{c}\").alias(f\"{sty_alias}_{c}\") if norm(c) in cw_norm_set\n",
        "    else F.col(f\"{sty_alias}.{c}\").alias(c)\n",
        "    for c in sty_cols\n",
        "]\n",
        "\n",
        "df_colorway_style = joined_cw_sty.select(*select_exprs)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {},
      "source": [
        "## old code Join Color Product Source & Colorway\n",
        "# We will no longer join Coloway and color product source\n",
        "'''\n",
        "#old instructions\n",
        "cw.THE_PARENT_ID = style.ID\n",
        "\n",
        "from\traw.Centric_ED_COLORWAY\t\ta\n",
        "\tLEFT OUTER JOIN raw.Centric_ED_COLOR_PRODUCT_SOURCE\tb\n",
        "\t\ton\ta.ID = b.COLORWAY\n",
        "\tLEFT OUTER JOIN raw.Centric_ED_PRODUCT_SOURCE\tc\n",
        "\ton\tb.THE_PARENT_ID = c.ID\n",
        "\n",
        "\n",
        "Colorway parent id joins to Style.ID\n",
        "\n",
        "Centric_ED_COLORWAY ID joins to Centric_ED_COLOR_PRODUCT_SOURCE on ID to colorway\n",
        "\n",
        "Color product source parent id joins to product source id\n",
        "bring product source\n",
        "'''\n",
        "\n",
        "#### Join Colorway and ColorProduct Source\n",
        "'''\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Aliases\n",
        "ed_alias = \"ColorProductSource_\"   # cps\n",
        "cw_alias = \"cw\"\n",
        "\n",
        "cps = dim_ed_color_product_source_df.alias(ed_alias)\n",
        "cw  = dim_colorway_df.alias(cw_alias)\n",
        "\n",
        "joined_cps_cw = cps.join(\n",
        "    cw,\n",
        "    F.col(f\"{ed_alias}.Colorway\") == F.col(f\"{cw_alias}.ColorwayId\"),\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "cps_cols = dim_ed_color_product_source_df.columns\n",
        "cw_cols  = dim_colorway_df.columns\n",
        "\n",
        "def norm(c: str) -> str:\n",
        "    return c.lower()  # case-insensitive comparison\n",
        "\n",
        "# Build a set of normalized left column names\n",
        "cps_norm_set = {norm(c) for c in cps_cols}\n",
        "\n",
        "select_exprs = []\n",
        "\n",
        "# Keep all cps columns as-is\n",
        "select_exprs += [F.col(f\"{ed_alias}.{c}\").alias(c) for c in cps_cols]\n",
        "\n",
        "# For cw: if normalized name collides with any cps normalized name, prefix it\n",
        "select_exprs += [\n",
        "    F.col(f\"{cw_alias}.{c}\").alias(f\"{cw_alias}_{c}\") if norm(c) in cps_norm_set\n",
        "    else F.col(f\"{cw_alias}.{c}\").alias(c)\n",
        "    for c in cw_cols\n",
        "]\n",
        "\n",
        "df_colorway_colorproductsource = joined_cps_cw.select(*select_exprs)\n",
        "'''\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "'''\n",
        "\n",
        "#old code\n",
        "#Join Style\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Join\n",
        "joined_df = (\n",
        "    df_colorway_colorproductsource.alias(\"cps\")\n",
        "    .join(\n",
        "        dim_style_df.alias(\"sty\"),\n",
        "        col(\"cps.ParentID\") == col(\"sty.StyleId\"),\n",
        "        \"inner\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Columns to select from dim_style_df\n",
        "style_cols = [\n",
        "    #\"Brand_Composit\",\n",
        "    \"SeasonalStyleCode\",\n",
        "    \"StyleName\",\n",
        "    \"Style\",\n",
        "    \"Style_Working_Name\",\n",
        "    \"StyleStatus\",\n",
        "    \"Gender\",\n",
        "    #\"Division_Composit\",\n",
        "    \"BrandID\",\n",
        "    \"BrandText\",\n",
        "    \"Division\",\n",
        "    \"FinalPatternNameSeasonless\",\n",
        "    \"PAActive\",\n",
        "    \"Designer\",\n",
        "    \"Developer\",\n",
        "    \"GenderText\",\n",
        "]\n",
        "\n",
        "# Build the select list:\n",
        "#  - all columns from df_colorway_colorproductsource (using its alias)\n",
        "#  - specific columns from dim_style_df (using its alias)\n",
        "selected_cols = (\n",
        "    [col(f\"cps.{c}\") for c in df_colorway_colorproductsource.columns] +\n",
        "    [col(f\"sty.{c}\") for c in style_cols]\n",
        ")\n",
        "\n",
        "combined_ColorProductSource_Style_df = joined_df.select(*selected_cols)\n",
        "'''\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Size Dimension (new)\n",
        "dim_size_df = (\n",
        "    df_ed_product_size.select(\n",
        "        F.col(\"ID\").alias(\"SizeId\"),\n",
        "        F.col(\"SIZE_CODE\").alias(\"SizeCode\"),\n",
        "        F.col(\"US_LABEL\").alias(\"USLabel\"),\n",
        "        F.col(\"WVW_WIDTH_ALPHA\").alias(\"WidthAlpha\"),\n",
        "        F.col(\"DIMENSION_1_SIZE\").alias(\"Dimension1\"),\n",
        "        F.col(\"DIMENSION_2_SIZE\").alias(\"Dimension2\")\n",
        "    )\n",
        "    .distinct()\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Supplier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Supplier Dimension (existing)\n",
        "dim_supplier_df = (\n",
        "    df_ed_supplier.select(\n",
        "        F.col(\"ID\").alias(\"SupplierId\"),\n",
        "        F.col(\"WVW_C_8_SUPPLIER_ID\").alias(\"SupplierCode\"),\n",
        "        F.col(\"NODE_NAME\").alias(\"SupplierName\"),\n",
        "        F.col(\"COUNTRY\").alias(\"CountryId\"),\n",
        "        F.col(\"ADDRESS\").alias(\"SupplierAddress\")\n",
        "    )\n",
        "    .distinct()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Factory Dimension (existing)\n",
        "dim_factory_df = (\n",
        "    df_ed_factory.select(\n",
        "        F.col(\"ID\").alias(\"FactoryId\"),\n",
        "        F.col(\"WVW_C_8_FACTORY_ID\").alias(\"FactoryCode\"),\n",
        "        F.col(\"NODE_NAME\").alias(\"FactoryName\"),\n",
        "        F.col(\"COUNTRY\").alias(\"CountryId\"),\n",
        "        F.col(\"ADDRESS\").alias(\"FactoryAddress\"),\n",
        "        #Add column for dev sample\n",
        "        F.col(\"SUPPLIER_NUMBER\").alias(\"Factory_ID\")\n",
        "        #\n",
        "    )\n",
        "    .distinct()\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Product Source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "# Product Source Dimension (existing)\n",
        "dim_product_source_df = (\n",
        "    df_ed_product_source.select(\n",
        "        F.col(\"ID\").alias(\"ProductSourceId\"),\n",
        "        F.col(\"THE_PARENT_ID\").alias(\"StyleId\"),\n",
        "        F.col(\"SUPPLIER\").alias(\"SupplierId\"),\n",
        "        F.col(\"SUPPLIER_FACTORY\").alias(\"FactoryId\"),\n",
        "        F.col(\"WVW_SOURCING_DEVELOPER\").alias(\"SourcingDeveloperId\"),\n",
        "        F.col(\"ACTIVE\").alias(\"ActiveFlag\")\n",
        "    )\n",
        "    .distinct()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Sample Dimension (new)\n",
        "dim_sample_df = (\n",
        "    df_ed_sample.select(\n",
        "        F.col(\"ID\").alias(\"SampleId\"),\n",
        "        F.col(\"THE_PARENT_ID\").alias(\"StyleId\"),\n",
        "        F.col(\"NODE_NAME\").alias(\"SampleName\"),\n",
        "        F.col(\"WVW_SAMPLE_STAGE\").alias(\"SampleStage\"),\n",
        "        F.col(\"ITERATION\").alias(\"Iteration\"),\n",
        "        F.col(\"SAMPLE_STATUS\").alias(\"SampleStatus\"),\n",
        "        F.col(\"SAMPLE_TYPE\").alias(\"SampleType\"),\n",
        "\n",
        "        F.col(\"SAMPLE_RECEIVED_DATE\").alias(\"SampleReceivedDate\"),\n",
        "        F.col(\"SAMPLE_REVIEW_DATE\").alias(\"SampleReviewDate\"),\n",
        "        F.col(\"SAMPLE_NOTES\").alias(\"SampleNotes\"),\n",
        "\n",
        "        F.col(\"WVW_TECH_PACK_RECEIVED\").alias(\"TechPackReceivedDate\"),\n",
        "        F.col(\"WVW_TRACKING_NUMBER\").alias(\"SampleTrackingNumber\"),\n",
        "        F.col(\"WVW_ADDRESS\").alias(\"SampleAddress\"),\n",
        "        F.col(\"WVW_SELECTED_BOM\").alias(\"SampleBOM\"),\n",
        "        F.col(\"PRODUCT_COLOR\").alias(\"ProductColor\"),\n",
        "        F.col(\"wvw_ETD\").alias(\"ETD\")\n",
        "    )\n",
        "    .distinct()\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {},
      "source": [
        "# User Dimension\n",
        "dim_user_df = (\n",
        "    df_ed_user.select(\n",
        "        F.col(\"ID\").alias(\"UserId\"),\n",
        "        F.col(\"NODE_NAME\").alias(\"UserName\")\n",
        "    )\n",
        "    .distinct()\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Data Clean up for Dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Color Way Dimension\n",
        "Now colorway is joined in the combined df, so we should be cleaning that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Columns to transform\n",
        "cols_to_clean = [\n",
        "    \"colorway_ProductTypeClassification\",\n",
        "    \"colorway_ProductLineSeasonless\",\n",
        "    \"ColorwayStatus\"\n",
        "]\n",
        "\n",
        "# Apply substring_index to each column\n",
        "for col in cols_to_clean:\n",
        "    dim_colorway_df = dim_colorway_df.withColumn(col, F.substring_index(F.col(col), \":\", -1))\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Season Dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Columns to transform\n",
        "cols_to_clean = [\n",
        "    \"SeasonType\",\n",
        "    \"Year\"\n",
        "]\n",
        "\n",
        "# Apply substring_index to each column (take the part after the last \":\")\n",
        "for col in cols_to_clean:\n",
        "    dim_season_df = dim_season_df.withColumn(col, F.trim(F.substring_index(F.col(col), \":\", -1)))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "metadata": {},
      "source": [
        "user_designer_df  = dim_user_df.alias(\"ud\")\n",
        "user_developer_df = dim_user_df.alias(\"uv\")\n",
        "user_sourcing_df  = dim_user_df.alias(\"us\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Sample Dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Columns to clean\n",
        "cols_to_clean = [\n",
        "    \"SampleStage\",\n",
        "    \"SampleStatus\",\n",
        "    \"SampleType\"\n",
        "]\n",
        "\n",
        "# Apply substring_index to each column (take the part after the last \":\")\n",
        "for col in cols_to_clean:\n",
        "    dim_sample_df = dim_sample_df.withColumn(col, F.trim(F.substring_index(F.col(col), \":\", -1)))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Get PURCHASED_ORDER_PRODUCT & PURCHASED_ORDER_COLOR\n",
        "df_ed_PURCHASED_ORDER_PRODUCT <br>\n",
        "df_ed_PURCHASED_ORDER_COLOR\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## PURCHASED ORDER PRODUCT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "dim_PURCHASED_ORDER_PRODUCT_df = (\n",
        "    df_ed_PURCHASED_ORDER_PRODUCT\n",
        "        .dropDuplicates([\"PO\", \"PO_PRODUCT\",\"PRODUCT\",\"SIZE_RANGE\"])\n",
        "        .select(\n",
        "            \"PO\",\n",
        "            \"PO_PRODUCT\",\"PRODUCT\",\"SIZE_RANGE\"\n",
        "        )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Purchase Order Color"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "dim_PURCHASED_ORDER_COLOR_df = (\n",
        "    df_ed_PURCHASED_ORDER_COLOR\n",
        "        .dropDuplicates([\"PO\", \"PO_COLOR\",\"PRODUCT_COLOR\"])\n",
        "        .select(\n",
        "            \"PO\",\n",
        "            \"PO_COLOR\",\n",
        "            \"PRODUCT_COLOR\"\n",
        "\n",
        "        )\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# commented out PO  left join PO_Color  left join PO_Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "outputs": [],
      "metadata": {},
      "source": [
        "dim_purchase_order_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [],
      "metadata": {},
      "source": [
        "dim_PURCHASED_ORDER_COLOR_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [],
      "metadata": {},
      "source": [
        "dim_PURCHASED_ORDER_PRODUCT_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "outputs": [],
      "metadata": {},
      "source": [
        "po  = dim_purchase_order_df.alias(\"po\")\n",
        "po.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(po.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "outputs": [],
      "metadata": {},
      "source": [
        "poc = dim_PURCHASED_ORDER_COLOR_df.alias(\"poc\")\n",
        "poc.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "outputs": [],
      "metadata": {},
      "source": [
        "poc.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "#lets not join in Purchase Order color and Purchase Order Product\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "po  = dim_purchase_order_df.alias(\"po\")\n",
        "poc = dim_PURCHASED_ORDER_COLOR_df.alias(\"poc\")      # has PO, PO_COLOR, PRODUCT_COLOR\n",
        "#pop = dim_PURCHASED_ORDER_PRODUCT_df.alias(\"pop\")    # has PO, PO_PRODUCT, PRODUCT, SIZE_RANGE\n",
        "\n",
        "# 1) Start with PO, join in PO_COLOR on PO, then join in PO_PRODUCT on PO\n",
        "po_with_color = (\n",
        "    po\n",
        "    .join(poc, on=F.col(\"po.PO\") == F.col(\"poc.PO\"), how=\"left\") #This Join condition may be incorrect as it is infation number of rows\n",
        "    #.join(pop, on=F.col(\"po.PO\") == F.col(\"pop.PO\"), how=\"left\")\n",
        "    .select(\n",
        "        *[F.col(f\"po.{c}\").alias(c) for c in dim_purchase_order_df.columns],\n",
        "        F.col(\"poc.PO_COLOR\").alias(\"PO_COLOR\"),\n",
        "        F.col(\"poc.PRODUCT_COLOR\").alias(\"PRODUCT_COLOR\"),\n",
        "        #F.col(\"pop.PO_PRODUCT\").alias(\"PO_PRODUCT\"),\n",
        "        #F.col(\"pop.PRODUCT\").alias(\"PRODUCT\"),\n",
        "        #F.col(\"pop.SIZE_RANGE\").alias(\"SIZE_RANGE\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"po_with_color count:\", po_with_color.count())\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "outputs": [],
      "metadata": {},
      "source": [
        "po_with_color.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "display(po_with_color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Left join in Order (on PO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "po = dim_purchase_order_df.alias(\"po\")\n",
        "o  = dim_ed_order_df.alias(\"o\")\n",
        "\n",
        "po_with_order = (\n",
        "    po\n",
        "    .join(o, on=F.col(\"po.PO\") == F.col(\"o.PO\"), how=\"left\")\n",
        "    .select(\n",
        "        *[F.col(f\"po.{c}\").alias(c) for c in dim_purchase_order_df.columns],  # all PO cols\n",
        "        F.col(\"o.PO_COLOR\").alias(\"Order_PO_COLOR\"),\n",
        "        F.col(\"o.PO_PRODUCT\").alias(\"Order_PO_PRODUCT\"),\n",
        "        F.col(\"o.WVW_PO_SUPPLIER_VALIDATION\").alias(\"WVW_PO_SUPPLIER_VALIDATION\"),\n",
        "        F.col(\"o.ID\").alias(\"OrderID\")\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"po_with_order count:\", po_with_order.count())\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Left join in Shipments (on OrderID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(s.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "outputs": [],
      "metadata": {},
      "source": [
        "'''\n",
        "#Duplicate Columns\n",
        "po_order_shipment_df = (\n",
        "    po_with_order.alias(\"base\")\n",
        "    .join(s, on=F.col(\"base.OrderID\") == F.col(\"s.OrderID\"), how=\"left\")\n",
        "    .select(\n",
        "        \"base.*\",\n",
        "        F.col(\"s.*\")   # brings all shipment columns (note: this can duplicate column names if base already has same names)\n",
        "    )\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "base_df = po_with_order\n",
        "ship_df = dim_ed_shipment_df\n",
        "\n",
        "base_alias = \"base\"\n",
        "ship_alias = \"s\"\n",
        "\n",
        "# Aliases\n",
        "base = base_df.alias(base_alias)\n",
        "s    = ship_df.alias(ship_alias)\n",
        "\n",
        "# Column name sets (case-sensitive; Spark column names are case-sensitive)\n",
        "base_cols = base_df.columns\n",
        "ship_cols = ship_df.columns\n",
        "\n",
        "# Only bring shipment columns that are NOT already present in base\n",
        "new_ship_cols = [c for c in ship_cols if c not in base_cols]\n",
        "\n",
        "po_order_shipment_df = (\n",
        "    base\n",
        "    .join(s, on=F.col(f\"{base_alias}.OrderID\") == F.col(f\"{ship_alias}.OrderID\"), how=\"left\")\n",
        "    .select(\n",
        "        # all base columns\n",
        "        *[F.col(f\"{base_alias}.{c}\").alias(c) for c in base_cols],\n",
        "        # only non-duplicate shipment columns\n",
        "        *[F.col(f\"{ship_alias}.{c}\").alias(c) for c in new_ship_cols]\n",
        "    )\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [],
      "metadata": {},
      "source": [
        "po_order_shipment_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "outputs": [],
      "metadata": {},
      "source": [
        "po_order_shipment_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "display(po_order_shipment_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "'''\n",
        "#Old join\n",
        "### Join Shipment & Order\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "s = dim_ed_shipment_df.alias(\"s\")\n",
        "o = dim_ed_order_df.alias(\"o\")\n",
        "\n",
        "dim_ed_shipment_order_df = (\n",
        "    s.join(o, F.col(\"s.OrderID\") == F.col(\"o.ID\"), \"left\")\n",
        "     .select(\n",
        "         \"s.*\",  # all columns from shipment\n",
        "         F.col(\"o.PO_COLOR\"),\n",
        "         F.col(\"o.PO_PRODUCT\"),\n",
        "         F.col(\"o.WVW_PO_SUPPLIER_VALIDATION\")\n",
        "     )\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "#old join\n",
        "### Join Purchase Order\n",
        "''' \n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "so = dim_ed_shipment_order_df.alias(\"so\") \n",
        "po = dim_purchase_order_df.alias(\"po\")     \n",
        "\n",
        "\n",
        "\n",
        "# join on PO\n",
        "combined_PO_order_shipment_df = (\n",
        "    so.join(po, F.col(\"so.PO\") == F.col(\"po.PO\"), \"left\")\n",
        "      .select(\n",
        "          \"so.*\",                        # all columns from dim_ed_shipment_order_df\n",
        "          F.col(\"po.Season\"),\n",
        "          F.col(\"po.PO_Sample_Type\"),\n",
        "          F.col(\"po.Supplier\"),\n",
        "          F.col(\"po.Factory\"),\n",
        "          F.col(\"po.Supplier_PO\"),\n",
        "          F.col(\"po.State\"),\n",
        "          F.col(\"po.State_Changed_On\")\n",
        "      )\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [],
      "metadata": {},
      "source": [
        "#old Join\n",
        "## Join PURCHASED_ORDER_COLOR\n",
        "#join combined_PO_order_shipment_df with PO_Color on PO_Color\n",
        "#combined_PO_order_shipment_po_color_df = combined_PO_order_shipment_df\n",
        "\n",
        "# Inner join on PO_COLOR\n",
        "\n",
        "# Alias dataframes for clarity\n",
        "'''\n",
        "c = combined_PO_order_shipment_df.alias(\"c\")\n",
        "d = dim_PURCHASED_ORDER_COLOR_df.select(\"PO_COLOR\", \"PRODUCT_COLOR\").alias(\"d\")\n",
        "\n",
        "# Inner join on PO_COLOR and select all columns from 'c' plus only PRODUCT_COLOR from 'd'\n",
        "merged_df = (\n",
        "    c.join(d, on=\"PO_COLOR\", how=\"left\")\n",
        "     .select(\"c.*\", \"d.PRODUCT_COLOR\")\n",
        ")\n",
        "\n",
        "# Print counts\n",
        "print(\"combined_PO_order_shipment_df count:\", combined_PO_order_shipment_df.count())\n",
        "print(\"dim_PURCHASED_ORDER_COLOR_df count:\", dim_PURCHASED_ORDER_COLOR_df.count())\n",
        "print(\"merged_df count:\", merged_df)\n",
        "\n",
        "\n",
        "combined_PO_order_shipment_POColor_df = merged_df\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [],
      "metadata": {},
      "source": [
        "#old join\n",
        "## Join Purchase_Order_Product\n",
        "'''\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Aliases for readability\n",
        "left_df  = combined_PO_order_shipment_POColor_df.alias(\"left\")\n",
        "right_df = dim_PURCHASED_ORDER_PRODUCT_df.alias(\"right\")\n",
        "\n",
        "# Perform inner join on PO_PRODUCT\n",
        "combined_PO_order_shipment_POColor_POProduct_df = (\n",
        "    left_df.join(right_df, on=F.col(\"left.PO_PRODUCT\") == F.col(\"right.PO_PRODUCT\"), how=\"left\")\n",
        "            # Select all columns from the left df\n",
        "            .select(\"left.*\", F.col(\"right.PRODUCT\"), F.col(\"right.SIZE_RANGE\"))\n",
        ")\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 510,
      "outputs": [],
      "metadata": {},
      "source": [
        "#combined_PO_order_shipment_POColor_POProduct_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Column Name Cleanup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Clean the columns of Combined PO, Order and Sales\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "import re\n",
        "\n",
        "# Acronyms to preserve as UPPERCASE (customize as needed)\n",
        "ACRONYMS = {\"PO\", \"ETD\", \"ID\"}\n",
        "\n",
        "def remove_wvw(text: str, case_sensitive: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    Remove the substring 'WVW' from text.\n",
        "    By default, removal is case-insensitive.\n",
        "    \"\"\"\n",
        "    if case_sensitive:\n",
        "        return text.replace(\"WVW\", \"\")\n",
        "    else:\n",
        "        # Case-insensitive removal of 'WVW'\n",
        "        return re.sub(r\"(?i)WVW\", \"\", text)\n",
        "\n",
        "def to_pascal_after_cleanup(col_name: str) -> str:\n",
        "    \"\"\"\n",
        "    1) Remove 'WVW' substring.\n",
        "    2) Convert underscore-separated tokens to PascalCase.\n",
        "       Preserve acronyms in ACRONYMS (UPPERCASE).\n",
        "    \"\"\"\n",
        "    # Step 1: remove WVW\n",
        "    cleaned = remove_wvw(col_name, case_sensitive=False)\n",
        "\n",
        "    # Step 2: split by underscore and filter empty tokens\n",
        "    parts = [p for p in cleaned.split(\"_\") if p != \"\"]\n",
        "    converted = []\n",
        "    for p in parts:\n",
        "        token_upper = p.upper()\n",
        "        if token_upper in ACRONYMS:\n",
        "            converted.append(token_upper)            # preserve acronyms (PO, ETD, ID)\n",
        "        elif p.isupper() and len(p) <= 3:\n",
        "            converted.append(p)                      # preserve short all-caps tokens\n",
        "        else:\n",
        "            converted.append(p[:1].upper() + p[1:].lower())  # PascalCase\n",
        "    return \"\".join(converted)\n",
        "\n",
        "def rename_df_columns_remove_wvw(df):\n",
        "    \"\"\"\n",
        "    Rename all columns:\n",
        "      - remove 'WVW'\n",
        "      - convert to PascalCase\n",
        "      - avoid name collisions by appending numeric suffixes\n",
        "    \"\"\"\n",
        "    old_cols = df.columns\n",
        "    new_cols = []\n",
        "    used = set()\n",
        "    for c in old_cols:\n",
        "        base = to_pascal_after_cleanup(c)\n",
        "        new_name = base or c  # fallback if result becomes empty\n",
        "        idx = 2\n",
        "        while new_name in used:\n",
        "            new_name = f\"{base}{idx}\"\n",
        "            idx += 1\n",
        "        new_cols.append((c, new_name))\n",
        "        used.add(new_name)\n",
        "\n",
        "    for old, new in new_cols:\n",
        "        if old != new:\n",
        "            df = df.withColumnRenamed(old, new)\n",
        "    return df\n",
        "\n",
        "# Apply to your combined dataframe\n",
        "#combined_clean_df = rename_df_columns_remove_wvw(combined_PO_order_shipment_df)\n",
        "combined_clean_df = rename_df_columns_remove_wvw(combined_PO_order_shipment_POColor_POProduct_df)\n",
        "\n",
        "# Verify\n",
        "print(combined_clean_df.columns)\n",
        "\n",
        "combined_PO_order_shipment_POColor_POProduct_df = combined_clean_df\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "#Changing type of date columns\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "combined_PO_order_shipment_POColor_POProduct_df = (\n",
        "    combined_PO_order_shipment_POColor_POProduct_df\n",
        "    .withColumn(\"CreatedAt\",      F.to_timestamp(\"CreatedAt\"))\n",
        "    .withColumn(\"ModifiedAT\",     F.to_timestamp(\"ModifiedAT\"))\n",
        "    .withColumn(\"StateChangedOn\", F.to_timestamp(\"StateChangedOn\"))\n",
        ")\n",
        "#combined_PO_order_shipment_POColor_POProduct_df.printSchema()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 513,
      "outputs": [],
      "metadata": {},
      "source": [
        "#combined_PO_order_shipment_POColor_POProduct_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Color Product Source\n",
        "Column Name Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "outputs": [],
      "metadata": {},
      "source": [
        "clean_color_product_source_df = rename_df_columns_remove_wvw(dim_ed_color_product_source_df)\n",
        "dim_ed_color_product_source_df = clean_color_product_source_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## combined_ColorProductSource_Style_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 515,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "#combined_ColorProductSource_Style_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Clean column Names\n",
        "combined_ColorProductSource_Style_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "outputs": [],
      "metadata": {},
      "source": [
        "cleaned_combined_ColorProductSource_Style_df = rename_df_columns_remove_wvw(combined_ColorProductSource_Style_df)\n",
        "#cleaned_combined_ColorProductSource_Style_df.columns\n",
        "combined_ColorProductSource_Style_df = cleaned_combined_ColorProductSource_Style_df\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 517,
      "outputs": [],
      "metadata": {},
      "source": [
        "'''\n",
        "#This is not needed we have another working code below.\n",
        "from collections import defaultdict\n",
        "\n",
        "def drop_duplicate_columns_keep_first(df):\n",
        "    \"\"\"\n",
        "    Drops duplicate column names in a Spark DF by keeping the first occurrence.\n",
        "    Works even when Spark throws AMBIGUOUS_REFERENCE due to duplicate names.\n",
        "    \"\"\"\n",
        "    orig = df.columns  # may contain duplicates\n",
        "    counts = defaultdict(int)\n",
        "\n",
        "    # 1) create unique names for ALL columns (by position)\n",
        "    unique_names = []\n",
        "    for c in orig:\n",
        "        counts[c] += 1\n",
        "        if counts[c] == 1:\n",
        "            unique_names.append(c)\n",
        "        else:\n",
        "            unique_names.append(f\"{c}__dup{counts[c]-1}\")  # e.g. col__dup1, col__dup2\n",
        "\n",
        "    df_unique = df.toDF(*unique_names)\n",
        "\n",
        "    # 2) keep only the first occurrence of each original column name\n",
        "    seen = set()\n",
        "    keep_unique_names = []\n",
        "    keep_final_names = []\n",
        "    for o, u in zip(orig, unique_names):\n",
        "        if o not in seen:\n",
        "            seen.add(o)\n",
        "            keep_unique_names.append(u)   # select by unique name (no ambiguity)\n",
        "            keep_final_names.append(o)    # rename back to original\n",
        "\n",
        "    return df_unique.select(*keep_unique_names).toDF(*keep_final_names)\n",
        "\n",
        "\n",
        "# usage\n",
        "combined_ColorProductSource_Style_df = drop_duplicate_columns_keep_first(combined_ColorProductSource_Style_df)\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Join two Combined views"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 518,
      "outputs": [],
      "metadata": {},
      "source": [
        "#combined_PO_order_shipment_df Left Outer join on\n",
        "#combined_ColorProductSource_Style_df.[ColorwayId] = combined_PO_order_shipment_df.[ProductColor]\n",
        "#combined_ColorProductSource_Style_df\n",
        "#Do not repeat column names\n",
        "# result dataframe combined_shipment_product_df\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "outputs": [],
      "metadata": {},
      "source": [
        "combined_ColorProductSource_Style_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "outputs": [],
      "metadata": {},
      "source": [
        "combined_PO_order_shipment_POColor_POProduct_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def dedupe_within_df_case_insensitive(df, alt_prefix=\"alt\"):\n",
        "    \"\"\"\n",
        "    Deduplicate columns that collide case-insensitively *within the same dataframe*.\n",
        "    Keeps the first occurrence as-is.\n",
        "    Subsequent duplicates are renamed to: <original>_<alt_prefix><n>\n",
        "    Uses toDF(*new_names) so it is safe even when duplicates already exist.\n",
        "    \"\"\"\n",
        "    seen = {}  # lower_name -> count\n",
        "    new_names = []\n",
        "\n",
        "    for c in df.columns:\n",
        "        key = c.lower()\n",
        "        seen[key] = seen.get(key, 0) + 1\n",
        "        if seen[key] == 1:\n",
        "            new_names.append(c)\n",
        "        else:\n",
        "            # alt1 for the 2nd occurrence, alt2 for 3rd, etc.\n",
        "            alt_n = seen[key] - 1\n",
        "            new_names.append(f\"{c}_{alt_prefix}{alt_n}\")\n",
        "\n",
        "    return df.toDF(*new_names)\n",
        "\n",
        "def prefix_cross_df_duplicates(cols_left, cols_right, right_prefix=\"po_\"):\n",
        "    \"\"\"\n",
        "    If a right-side column name collides case-insensitively with any left-side column name,\n",
        "    return a mapping dict for right columns: {original_right_name: output_name}\n",
        "    where output_name is prefixed.\n",
        "    \"\"\"\n",
        "    left_set = set([c.lower() for c in cols_left])\n",
        "\n",
        "    mapping = {}\n",
        "    for c in cols_right:\n",
        "        if c.lower() in left_set:\n",
        "            mapping[c] = f\"{right_prefix}{c}\"\n",
        "        else:\n",
        "            mapping[c] = c\n",
        "    return mapping\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 522,
      "outputs": [],
      "metadata": {},
      "source": [
        "'''\n",
        "# 1) Dedupe within each DF (fixes AMBIGUOUS_REFERENCE inside DF)\n",
        "cps_clean = dedupe_within_df_case_insensitive(combined_ColorProductSource_Style_df, alt_prefix=\"alt\")\n",
        "po_clean  = dedupe_within_df_case_insensitive(combined_PO_order_shipment_POColor_POProduct_df, alt_prefix=\"alt\")\n",
        "\n",
        "# 2) Join (LEFT OUTER) per instruction\n",
        "cps_alias = \"cps\"\n",
        "po_alias  = \"po\"\n",
        "\n",
        "cps = cps_clean.alias(cps_alias)\n",
        "po  = po_clean.alias(po_alias)\n",
        "\n",
        "joined = cps.join(\n",
        "    po,\n",
        "    F.col(f\"{cps_alias}.ColorwayId\") == F.col(f\"{po_alias}.ProductColor\"),\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# 3) Build output columns:\n",
        "#    - All CPS columns as-is\n",
        "#    - All PO columns, but prefix only those that would duplicate CPS columns\n",
        "cps_cols = cps_clean.columns\n",
        "po_cols  = po_clean.columns\n",
        "\n",
        "po_name_map = prefix_cross_df_duplicates(cps_cols, po_cols, right_prefix=\"po_\")\n",
        "\n",
        "select_exprs = []\n",
        "\n",
        "# CPS first\n",
        "select_exprs += [F.col(f\"{cps_alias}.{c}\").alias(c) for c in cps_cols]\n",
        "\n",
        "# PO second (all columns, with prefix if needed)\n",
        "select_exprs += [F.col(f\"{po_alias}.{c}\").alias(po_name_map[c]) for c in po_cols]\n",
        "\n",
        "combined_shipment_product_df = joined.select(*select_exprs)\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# 1) Dedupe within each DF (unchanged)\n",
        "cps_clean = dedupe_within_df_case_insensitive(\n",
        "    combined_ColorProductSource_Style_df,\n",
        "    alt_prefix=\"alt\"\n",
        ")\n",
        "po_clean = dedupe_within_df_case_insensitive(\n",
        "    combined_PO_order_shipment_POColor_POProduct_df,\n",
        "    alt_prefix=\"alt\"\n",
        ")\n",
        "\n",
        "# 2) Swap join sides\n",
        "cps_alias = \"cps\"\n",
        "po_alias  = \"po\"\n",
        "\n",
        "cps = cps_clean.alias(cps_alias)\n",
        "po  = po_clean.alias(po_alias)\n",
        "\n",
        "#  PO_shipment is now LEFT, CPS is RIGHT\n",
        "joined = po.join(\n",
        "    cps,\n",
        "    F.col(f\"{po_alias}.ProductColor\") == F.col(f\"{cps_alias}.ColorwayId\"),\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# 3) Build output columns (INTENTIONALLY UNCHANGED)\n",
        "cps_cols = cps_clean.columns\n",
        "po_cols  = po_clean.columns\n",
        "\n",
        "# Prefix PO columns only if they collide with CPS\n",
        "po_name_map = prefix_cross_df_duplicates(\n",
        "    cps_cols,\n",
        "    po_cols,\n",
        "    right_prefix=\"po_\"\n",
        ")\n",
        "\n",
        "select_exprs = []\n",
        "\n",
        "# CPS columns FIRST (same names as before)\n",
        "select_exprs += [\n",
        "    F.col(f\"{cps_alias}.{c}\").alias(c)\n",
        "    for c in cps_cols\n",
        "]\n",
        "\n",
        "# PO columns SECOND (same prefix behavior as before)\n",
        "select_exprs += [\n",
        "    F.col(f\"{po_alias}.{c}\").alias(po_name_map[c])\n",
        "    for c in po_cols\n",
        "]\n",
        "\n",
        "combined_shipment_product_df = joined.select(*select_exprs)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "display(combined_shipment_product_df.limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Write Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Folder variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# gold_adls_path is assumed to be already defined, e.g.\n",
        "# gold_adls_path = \"abfss://gold@youraccount.dfs.core.windows.net/gold\"\n",
        "\n",
        "gold_container = gold_adls_path.split('@')[0].split('//')[1]   # e.g. 'gold'\n",
        "\n",
        "CDM_Root_Folder = \"/Centric/CDM\"\n",
        "\n",
        "# One folder per dimension / fact\n",
        "dim_season_folder          = CDM_Root_Folder + \"/dim_season\"\n",
        "dim_style_folder           = CDM_Root_Folder + \"/dim_style\"\n",
        "dim_colorway_folder        = CDM_Root_Folder + \"/dim_colorway\"\n",
        "dim_supplier_folder        = CDM_Root_Folder + \"/dim_supplier\"\n",
        "dim_factory_folder         = CDM_Root_Folder + \"/dim_factory\"\n",
        "dim_product_source_folder  = CDM_Root_Folder + \"/dim_product_source\"\n",
        "dim_size_folder            = CDM_Root_Folder + \"/dim_size\"\n",
        "dim_sample_folder          = CDM_Root_Folder + \"/dim_sample\"\n",
        "dim_user_folder            = CDM_Root_Folder + \"/dim_user\"\n",
        "\n",
        "dim_purchase_order_folder = CDM_Root_Folder + \"/dim_purchase_order\"\n",
        "dim_shipment_folder =       CDM_Root_Folder + \"/dim_shipment\"\n",
        "dim_en_shipment_folder = CDM_Root_Folder + \"/dim_en_shipment\"\n",
        "dim_color_product_source_folder = CDM_Root_Folder + \"/dim_color_product_source\"\n",
        "\n",
        "dim_order_folder = CDM_Root_Folder + \"/dim_order\"\n",
        "dim_lookup_item_folder = CDM_Root_Folder + \"/dim_lookup_item\"\n",
        "\n",
        "dim_PURCHASED_ORDER_PRODUCT_folder = CDM_Root_Folder + \"/dim_PURCHASED_ORDER_PRODUCT\"\n",
        "dim_PURCHASED_ORDER_COLOR_folder = CDM_Root_Folder + \"/dim_PURCHASED_ORDER_COLOR\"\n",
        "\n",
        "#Combined PO, Order, Shipment\n",
        "dim_combined_PO_Order_Shipment_Folder = CDM_Root_Folder + \"/dim_combined_PO_Order_Shipment\"\n",
        "\n",
        "\n",
        "fact_dev_samples_folder    = CDM_Root_Folder + \"/fact_dev_samples\"\n",
        "\n",
        "combined_ColorProductSource_Style_folder = CDM_Root_Folder + \"/combined_ColorProductSource_Style\"\n",
        "combined_L2_shipment_product_folder = CDM_Root_Folder + \"/combined_L2_shipment_product_folder\"\n",
        "\n",
        "\n",
        "def cdm_path(folder: str) -> str:\n",
        "    \"\"\"\n",
        "    Build full ABFSS path under the gold container for a given folder.\n",
        "    \"\"\"\n",
        "    return f\"abfss://{gold_container}@{account_fqdn}{folder}\"\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Combined Level 2 - Shipment Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "outputs": [],
      "metadata": {},
      "source": [
        "(combined_shipment_product_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .option(\"mergeschema\",\"true\")\n",
        "    .save(cdm_path(combined_L2_shipment_product_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Colorway"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 527,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_colorway_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .option(\"mergeschema\",\"true\")\n",
        "    .save(cdm_path(dim_colorway_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Combined ColorProductSource & Style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 528,
      "outputs": [],
      "metadata": {},
      "source": [
        "#combined_ColorProductSource_Style_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 529,
      "outputs": [],
      "metadata": {},
      "source": [
        "combined_ColorProductSource_Style_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 530,
      "outputs": [],
      "metadata": {},
      "source": [
        "'''\n",
        "(combined_ColorProductSource_Style_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(combined_ColorProductSource_Style_folder)))\n",
        "'''\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Style\n",
        "Write Style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 531,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_style_df_transformed\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .option(\"mergeSchema\", \"true\")\n",
        "    .save(cdm_path(dim_style_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Product Source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 532,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_product_source_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .option(\"mergeschema\",\"true\")\n",
        "    .save(cdm_path(dim_product_source_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "(dim_style_df_transformed\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .option(\"mergeSchema\", \"true\")\n",
        "    .save(cdm_path(dim_style_folder)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 533,
      "outputs": [],
      "metadata": {},
      "source": [
        "(combined_PO_order_shipment_POColor_POProduct_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .option(\"mergeSchema\", \"true\")\n",
        "    .save(cdm_path(dim_combined_PO_Order_Shipment_Folder)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 534,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_PURCHASED_ORDER_PRODUCT_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_PURCHASED_ORDER_PRODUCT_folder)))\n",
        "\n",
        "(dim_PURCHASED_ORDER_COLOR_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_PURCHASED_ORDER_COLOR_folder)))\n",
        "\n",
        "    \n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 535,
      "outputs": [],
      "metadata": {},
      "source": [
        "# DIMENSIONS\n",
        "(dim_season_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_season_folder)))\n",
        "\n",
        "\n",
        "\n",
        "(dim_supplier_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_supplier_folder)))\n",
        "\n",
        "(dim_factory_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .option(\"mergeschema\",\"true\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_factory_folder)))\n",
        "\n",
        "\n",
        "\n",
        "(dim_size_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_size_folder)))\n",
        "\n",
        "(dim_sample_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_sample_folder)))\n",
        "\n",
        "(dim_user_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_user_folder)))\n",
        "\n",
        "# FACT\n",
        "'''\n",
        "(fact_dev_samples_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(fact_dev_samples_folder)))\n",
        "'''\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 536,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_purchase_order_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_purchase_order_folder)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 537,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(dim_purchase_order_folder)\n",
        "print(dim_shipment_folder)\n",
        "print(dim_color_product_source_folder)\n",
        "print(dim_order_folder)\n",
        "print(dim_lookup_item_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Shipment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 538,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "(dim_ed_shipment_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_shipment_folder))\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## EN_Shipment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_en_shipment_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_en_shipment_folder))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## color product source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 539,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_ed_color_product_source_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .option(\"mergeschema\",\"true\")\n",
        "    .save(cdm_path(dim_color_product_source_folder)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 540,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_ed_order_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(cdm_path(dim_order_folder)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 541,
      "outputs": [],
      "metadata": {},
      "source": [
        "(dim_lookup_item_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .option(\"mergeschema\",\"true\")\n",
        "    .save(cdm_path(dim_lookup_item_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# CSV Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#po_with_color_product\n",
        "po_order_shipment_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "outputs": [],
      "metadata": {},
      "source": [
        "po_order_shipment_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "outputs": [],
      "metadata": {},
      "source": [
        "po_order_shipment_df_folder = (\n",
        "    CDM_Root_Folder + \"/po_order_shipment_df_folder\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "(\n",
        "    po_order_shipment_df\n",
        "        .coalesce(1)                     # single CSV file\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .option(\"header\", \"true\")\n",
        "        .csv(cdm_path(po_order_shipment_df_folder))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 543,
      "outputs": [],
      "metadata": {},
      "source": [
        "combined_shipment_product_df_read = (\n",
        "    spark.read\n",
        "         .format(\"delta\")\n",
        "         .load(cdm_path(combined_L2_shipment_product_folder))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 544,
      "outputs": [],
      "metadata": {},
      "source": [
        "combined_L2_shipment_product_csvfolder = (\n",
        "    CDM_Root_Folder + \"/combined_L2_shipment_product_csvfolder\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 545,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "(\n",
        "    combined_shipment_product_df_read\n",
        "        .coalesce(1)                     # single CSV file\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .option(\"header\", \"true\")\n",
        "        .csv(cdm_path(combined_L2_shipment_product_csvfolder))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}