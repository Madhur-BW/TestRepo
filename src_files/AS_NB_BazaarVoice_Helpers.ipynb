{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "from delta import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType, LongType\n",
        "from datetime import datetime\n",
        "import json\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from notebookutils import mssparkutils\n",
        "# SFTP\n",
        "import paramiko\n",
        "import gzip\n",
        "import io\n",
        "from pyspark.sql.functions import col, when, array, explode, expr\n",
        "import re\n",
        "from delta.tables import DeltaTable\n",
        "from pyspark.sql.utils import AnalysisException"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def get_json_keys(schema, prefix):\n",
        "    \"\"\"Recursively fetches all the keys from a complex JSON schema, including nested structures and arrays of structs\"\"\"\n",
        "    keys = []\n",
        "    for field in schema.fields:\n",
        "        if isinstance(field.dataType, StructType):\n",
        "            if prefix:\n",
        "                new_prefix = f\"{prefix}.{field.name}\"\n",
        "            else:\n",
        "                new_prefix = field.name\n",
        "            keys += get_json_keys(field.dataType, new_prefix)\n",
        "        elif isinstance(field.dataType, ArrayType) and isinstance(field.dataType.elementType, StructType):\n",
        "            if prefix:\n",
        "                new_prefix = f\"{prefix}.{field.name}\"\n",
        "            else:\n",
        "                new_prefix = field.name\n",
        "            keys += get_json_keys(field.dataType.elementType, new_prefix)\n",
        "        else:\n",
        "            if prefix:\n",
        "                keys.append(f\"{prefix}.{field.name}\")\n",
        "            else:\n",
        "                keys.append(field.name)\n",
        "    # Return a list of strings representing the path to each key in the JSON object\n",
        "    return keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# ???\n",
        "\n",
        "# define the dynamic schema\n",
        "dynamic_schema = spark.read.json(mParticle_raw_df.rdd.map(lambda row: row.value)).schema\n",
        "\n",
        "# List of keys that can be used to access specific fields in the JSON data\\r\\n\",\n",
        "mParticle_cols = get_json_keys(dynamic_schema, \"json_construct\")\n",
        "\n",
        "# Convert JSON strings to structured data\\r\\n\",\n",
        "mParticle_df = mParticle_raw_df.withColumn(\"json_construct\", from_json(col(\"value\"), dynamic_schema)).select(\"json_construct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Read a regular file (RatingValues is Struct) and an array file (RatingValues is Array(Struct)).\n",
        "regular_file_path = f\"abfss://raw@azwwwnonproddevadapadls.blob.core.windows.net/BazaarVoice/date=20250307/bv_sweatybetty_incremental_standard_client_feed_20250305.xml.gz\"\n",
        "array_file_path = \"abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/BazaarVoice/date=20250308/bv_sweatybetty_incremental_standard_client_feed_20250306.xml.gz\"\n",
        "\n",
        "# Get the Product elements\n",
        "print(f\"Reading XML from file {regular_file_path}...\")\n",
        "df_raw_product_regular = spark.read.format(\"xml\") \\\n",
        "    .option(\"rowTag\", \"Product\") \\\n",
        "    .load(regular_file_path)\n",
        "\n",
        "# Get the Product elements\n",
        "print(f\"Reading XML from file {array_file_path}...\")\n",
        "df_raw_product_array = spark.read.format(\"xml\") \\\n",
        "    .option(\"rowTag\", \"Product\") \\\n",
        "    .load(array_file_path)\n",
        "\n",
        "# Reviews **********************************************************************\n",
        "# Explode the reviews and filter to only get the Product attributes we need\n",
        "#df_reviews = df_raw_product.select(\"_id\", \"_disabled\", \"_removed\", explode(col(\"Reviews.Review\")).alias(\"product_reviews\"))\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Explode the reviews \n",
        "df_product_regular_exploded = df_raw_product_regular.select(\"_id\", \"_disabled\", \"_removed\", explode(col(\"Reviews.Review\")).alias(\"product_reviews\"))\n",
        "df_product_array_exploded = df_raw_product_array.select(\"_id\", \"_disabled\", \"_removed\", explode(col(\"Reviews.Review\")).alias(\"product_reviews\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Now we can explore normalizing the schema.\n",
        "df_product_regular_exploded.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_product_array_exploded.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {},
      "source": [
        "# df_final_regular = df_product_regular_exploded.select(\n",
        "#     #col(\"*\"),  # Keep all other columns\n",
        "#     when(\n",
        "#         col(\"product_reviews.RatingValues.RatingValue\").isNotNull() & (size(col(\"product_reviews.RatingValues.RatingValue\")) == 0),\n",
        "#         array(col(\"product_reviews.RatingValues.RatingValue\"))\n",
        "#     ).otherwise(\n",
        "#         col(\"product_reviews.RatingValues.RatingValue\")\n",
        "#     ).alias(\"rating_values\")\n",
        "# )\n",
        "\n",
        "# df_final_regular = df_product_regular_exploded.selectExpr(\n",
        "#     \"*\", \n",
        "#     \"\"\"CASE \n",
        "#         WHEN typeof(product_reviews.RatingValues.RatingValue) = 'struct' \n",
        "#         THEN array(product_reviews.RatingValues.RatingValue) \n",
        "#         ELSE product_reviews.RatingValues.RatingValue \n",
        "#        END AS rating_values\"\"\"\n",
        "# )\n",
        "\n",
        "df_regular_normalized = df_product_regular_exploded.select(\n",
        "    to_json(col(\"product_reviews.RatingValues.RatingValue\")).alias(\"rating_values\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Explore the application of schema to XML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create an XML document with some elements that could be an array or a struct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {},
      "source": [
        "# This doc has the key characteristics we need to addres:\n",
        "# 1) sub-elements that could be inferred as a single element struct on an array of structs\n",
        "# 2) sub-elements that have variable structure (sem-structured)\n",
        "xml_doc = \"\"\"<Products>\n",
        "    <Product id=\"P1001\" status=\"active\">\n",
        "        <Name>Wireless Mouse</Name>\n",
        "        <Price currency=\"USD\">29.99</Price>\n",
        "        <Categories>\n",
        "            <Category>Electronics</Category>\n",
        "            <Category>Computer Accessories</Category>\n",
        "        </Categories>\n",
        "        <Tags>\n",
        "            <Tag>wireless</Tag>\n",
        "            <Tag>mouse</Tag>\n",
        "        </Tags>\n",
        "        <Specifications>\n",
        "            <Width>6.2</Width>\n",
        "            <Height>3.4</Height>\n",
        "            <Depth>1.5</Depth>\n",
        "            <Weight unit=\"grams\">85</Weight>\n",
        "        </Specifications>\n",
        "        <Manufacturer name=\"TechCo\" country=\"USA\" />\n",
        "    </Product>\n",
        "    <Product id=\"P1002\" status=\"inactive\">\n",
        "        <Name>USB-C Hub</Name>\n",
        "        <Price currency=\"USD\">49.95</Price>\n",
        "        <Categories>\n",
        "            <Category>Computer Accessories</Category>\n",
        "        </Categories>\n",
        "        <Tags>\n",
        "            <Tag>usb-c</Tag>\n",
        "        </Tags>\n",
        "        <Specifications>\n",
        "            <Ports>4</Ports>\n",
        "            <SupportedOS>\n",
        "                <OS>Windows</OS>\n",
        "                <OS>MacOS</OS>\n",
        "                <OS>Linux</OS>\n",
        "            </SupportedOS>\n",
        "        </Specifications>\n",
        "        <Manufacturer name=\"GizmoCorp\" country=\"China\" />\n",
        "    </Product>\n",
        "</Products>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {},
      "source": [
        "# write the XML to a temp file\n",
        "from notebookutils import mssparkutils\n",
        "\n",
        "abfs_path = \"abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/temp/peter_test.xml\"\n",
        "\n",
        "# Write the XML string to the ABFS path\n",
        "mssparkutils.fs.put(abfs_path, xml_doc, overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Schema for it - generated.\n",
        "# This handles variable schema by including all the variants in the possible StructFields\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "product_schema = StructType([\n",
        "    StructField(\"@id\", StringType(), True),\n",
        "    StructField(\"@status\", StringType(), True),\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Price\", StructType([\n",
        "        StructField(\"@currency\", StringType(), True),\n",
        "        StructField(\"_VALUE\", StringType(), True)\n",
        "    ]), True),\n",
        "    StructField(\"Categories\", StructType([\n",
        "        StructField(\"Category\", ArrayType(StringType()), True)\n",
        "    ]), True),\n",
        "    StructField(\"Tags\", StructType([\n",
        "        StructField(\"Tag\", ArrayType(StringType()), True)\n",
        "    ]), True),\n",
        "    StructField(\"Specifications\", StructType([\n",
        "        StructField(\"Width\", StringType(), True),\n",
        "        StructField(\"Height\", StringType(), True),\n",
        "        StructField(\"Depth\", StringType(), True),\n",
        "        StructField(\"Weight\", StructType([\n",
        "            StructField(\"@unit\", StringType(), True),\n",
        "            StructField(\"_VALUE\", StringType(), True)\n",
        "        ]), True),\n",
        "        StructField(\"Ports\", StringType(), True),\n",
        "        StructField(\"SupportedOS\", StructType([\n",
        "            StructField(\"OS\", ArrayType(StringType()), True)\n",
        "        ]), True)\n",
        "    ]), True),\n",
        "    StructField(\"Manufacturer\", StructType([\n",
        "        StructField(\"@name\", StringType(), True),\n",
        "        StructField(\"@country\", StringType(), True)\n",
        "    ]), True)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Read the XML doc using the schema\n",
        "df = (\n",
        "    spark.read.format(\"xml\")\n",
        "    .option(\"rowTag\", \"Product\")\n",
        "    .option(\"attributePrefix\", \"@\")\n",
        "    .schema(product_schema)\n",
        "    .load(abfs_path)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Now work with it\n",
        "df.printSchema()\n",
        "df.show(truncate=False) # FAIL: Input path does not exist: abfss://azwwwnonproddevadapsyn01@azwwwnonproddevadapadls.dfs.core.windows.net/tmp/tmp_wbr6opt.xml\n",
        "# Why fail when we call show() rather than fail when we read it above? Bizarre.\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Another simpler XML test - just to test the array(struct) vs struct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "xml_doc = \"\"\"<Products>\n",
        "    <Product id=\"P1001\" status=\"active\">\n",
        "        <Name>Wireless Mouse</Name>\n",
        "        <Price currency=\"USD\">29.99</Price>\n",
        "        <Categories>\n",
        "            <Category>Electronics</Category>\n",
        "            <Category>Computer Accessories</Category>\n",
        "        </Categories>\n",
        "    </Product>\n",
        "    <Product id=\"P1002\" status=\"inactive\">\n",
        "        <Name>USB-C Hub</Name>\n",
        "        <Price currency=\"USD\">49.95</Price>\n",
        "        <Categories>\n",
        "            <Category>Computer Accessories</Category>\n",
        "        </Categories>\n",
        "    </Product>\n",
        "</Products>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "# write the XML to a temp file\n",
        "from notebookutils import mssparkutils\n",
        "\n",
        "abfs_path = \"abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/temp/peter_test2.xml\"\n",
        "\n",
        "# Write the XML string to the ABFS path\n",
        "mssparkutils.fs.put(abfs_path, xml_doc, overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {},
      "source": [
        "# define our schema\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "product_schema = StructType([\n",
        "    StructField(\"@id\", StringType(), True),\n",
        "    StructField(\"@status\", StringType(), True),\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Price\", StructType([\n",
        "        StructField(\"@currency\", StringType(), True),\n",
        "        StructField(\"_VALUE\", StringType(), True)\n",
        "    ]), True),\n",
        "    StructField(\"Categories\", StructType([\n",
        "        StructField(\"Category\", ArrayType(StringType()), True)\n",
        "    ]), True),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_simple_products = (\n",
        "    spark\n",
        "    .read\n",
        "    .format(\"xml\")\n",
        "    .option(\"rowTag\", \"Product\")\n",
        "    .option(\"attributePrefix\", \"@\")\n",
        "    .schema(product_schema)\n",
        "    .load(abfs_path)\n",
        ")\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_simple_products.printSchema()\n",
        "df_simple_products.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# REMOVE THIS CODE FROM HERE DOWN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Look at Tylers bronze AccountList\n",
        "abfss_path = 'abfss://bronze@azwwwnonproddevadapsyn01@azwwwnonproddevadapadls.dfs.core.windows.net/GL/AccountList'\n",
        "df_account_list = spark.read.format(\"delta\").load(abfss_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Define a regex for valid integers\n",
        "valid_int = \"^[0-9]+$\"\n",
        "\n",
        "# Good rows: both columns are valid integers\n",
        "# df_valid = df_account_list.filter(\n",
        "#     col(\"CorporateSortOrder\").rlike(valid_int) & \n",
        "#     col(\"RetailSortOrder\").rlike(valid_int)\n",
        "# )\n",
        "\n",
        "# Bad rows: either column is invalid\n",
        "df_invalid = df_account_list.filter(\n",
        "    ~col(\"CorporateSortOrder\").rlike(valid_int) |\n",
        "    ~col(\"RetailSortOrder\").rlike(valid_int)\n",
        ")\n",
        "\n",
        "df_invalid.select(\"Key\", \"CorporateSortOrder\", \"RetailSortOrder\").show(truncate=False)"
      ]
    }
  ]
}