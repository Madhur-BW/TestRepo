{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azure.storage.blob import BlobServiceClient\n",
        "from notebookutils import mssparkutils\n",
        "import urllib.parse\n",
        "import re\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import col, to_date, coalesce, regexp_replace, date_add, current_date, last_day, expr, lit, unbase64,col, hex, split, lead, when, min as min_, max, translate, desc\n",
        "\n",
        "from datetime import timedelta, datetime\n",
        "from pyspark.sql import functions as F, Window\n",
        "from delta.tables import DeltaTable\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### includes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Set Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# SharePoint API Details\n",
        "tenant_id = \"deace5d6-717b-4f79-ab12-6357206c0c36\"\n",
        "\n",
        "match = re.search(r'@([^.]+)\\.dfs\\.core\\.windows\\.net', raw_adls_path)\n",
        "storage_account = match.group(1) if match else None\n",
        "print(f\"storage_account: {storage_account}\")\n",
        "\n",
        "if \"prodprd\" in storage_account:\n",
        "    environment = 'production'\n",
        "else:\n",
        "    environment = 'development'\n",
        "\n",
        "print(f\"environment: {environment}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Set Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "start_date = '2025-06-01'\n",
        "full_refresh_table = 0\n",
        "##full_refresh_export = 1\n",
        "filter_date = (datetime.strptime(start_date, \"%Y-%m-%d\") - timedelta(days=40)).strftime(\"%Y-%m-%d\")\n",
        "print(f\"filter_date: {filter_date}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Load Delta Table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Get Source Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "source_path = f\"abfss://gold@{storage_account}.dfs.core.windows.net/CustomerSegmentation/CustomerSegmentationHistory/period=weekly/\"\n",
        "target_path = f\"abfss://gold@{storage_account}.dfs.core.windows.net/CustomerSegmentation/mparticle_egress/\"\n",
        "\n",
        "segmentation_df = spark.read.format(\"delta\").load(source_path).filter(col(\"Segmentation_date\") >= filter_date )\n",
        "print(f\"source_path: {source_path}\")\n",
        "\n",
        "segmentation_df.createOrReplaceTempView(\"segmentation\")\n",
        "##sales_df.show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Incremental Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "max_date = None\n",
        "if DeltaTable.isDeltaTable(spark, target_path):\n",
        "    target_df = spark.read.format(\"delta\").load(target_path)\n",
        "    max_date = target_df.agg(max(\"Segmentation_date\")).collect()[0][0]\n",
        "\n",
        "\n",
        "if max_date is not None and full_refresh_table !=1 :\n",
        "    extract_date = max_date\n",
        "else:\n",
        "    extract_date = start_date\n",
        "\n",
        "print(f\"start_date: {start_date}\")\n",
        "print(f\"max_date: {max_date}\")\n",
        "print(f\"extract_date: {extract_date}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "editable": false,
        "run_control": {
          "frozen": true
        }
      },
      "source": [
        "segmentation_df = spark.sql(\"\"\"\n",
        "        with cte as \n",
        "        (   select *, \n",
        "                lag(customermetricshashdiff) over(partition by MparticleUserKey order by Segmentation_Date) as Prev\n",
        "            from source \n",
        "        )\n",
        "        select *\n",
        "        from cte where customermetricshashdiff <> coalesce(Prev,'x')\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "mparticle_egress = spark.sql(f\"\"\"\n",
        "with cust as \n",
        "(\n",
        "SELECT *\n",
        "FROM (\n",
        "    SELECT\n",
        "        SEGMENTATION_DATE,\n",
        "        PERIOD,\n",
        "        customermetricshashdiff,\n",
        "        LAG(customermetricshashdiff) OVER (\n",
        "            PARTITION BY PERIOD, MparticleUserKey\n",
        "            ORDER BY SEGMENTATION_DATE\n",
        "        ) AS PREV,\n",
        "        *\n",
        "    FROM segmentation    \n",
        "    where Segmentation_Date >= date('{extract_date}')\n",
        ") tmp\n",
        "WHERE (tmp.customermetricshashdiff IS NULL AND tmp.PREV IS NOT NULL)\n",
        "   OR (tmp.customermetricshashdiff IS NOT NULL AND tmp.PREV IS NULL)\n",
        "   OR (tmp.customermetricshashdiff != tmp.PREV)\n",
        "ORDER BY SEGMENTATION_DATE DESC\n",
        ")\n",
        "select \n",
        "    Segmentation_date,\n",
        "    MparticleUserKey as mpid,\n",
        "    DerivedCustomerType as CUSTOMER_TYPE,\n",
        "    lifecycle_stage as LIFECYCLE_STAGE,\n",
        "    NewCustomerAcquiredFPorSale as NEW_CUSTOMER_ACQUIRED_FULL_PRICE_OR_SALE,\n",
        "    CustomerLifeTimeValueGBP as CUSTOMER_LIFETIME_VALUE,\n",
        "    NetLifeTimeValueGBP as NET_LIFETIME_VALUE,\n",
        "    TotalOrdercount as TOTAL_ORDER_COUNT,\n",
        "    AverageOrderValueGBP as AVERAGE_ORDER_VALUE,\n",
        "    average_order_items as AVERAGE_ORDER_ITEMS,\n",
        "    PercetageStoreOrders as PERCENTAGE_STORE_ORDERS,\n",
        "    PercentageDiscountItems as PERCENTAGE_DISCOUNT_ITEMS,\n",
        "    totalreturnamountGBP as TOTAL_RETURN_AMOUNT,\n",
        "    FirstOrderSource as FIRST_ORDER_SOURCE,\n",
        "    FirstOrderDate as FIRST_ORDER_DATE,\n",
        "    FirstordervalueGBP as FIRST_ORDER_VALUE,\n",
        "    FirstOrderItems as FIRST_ORDER_ITEMS,\n",
        "    LastOrderSource as LAST_ORDER_SOURCE,\n",
        "    LastOrderDate as LAST_ORDER_DATE,\n",
        "    Lastordervalue as LAST_ORDER_VALUE,\n",
        "    LastOrderItems as LAST_ORDER_ITEMS,\n",
        "    FavouriteSource as FAVOURITE_SOURCE,\n",
        "    FavouriteCategory as FAVOURITE_CATEGORY,\n",
        "    FavouriteSubCategory as FAVOURITE_SUBCATEGORY,\n",
        "    FavouriteColor as FAVOURITE_COLOUR,\n",
        "    FavouriteColourType as FAVOURITE_COLOUR_TYPE,\n",
        "    FavouriteHero as FAVOURITE_HERO,\n",
        "    FavouriteLeggingSize as FAVOURITE_LEGGING_SIZE,\n",
        "    FavouriteStore as FAVOURITE_STORE,\n",
        "    CUSTOMER_FIRST_ORDER_PRICE_TYPE_V2,\n",
        "    CUSTOMER_12M_ROLLING_PRICETYPE,\n",
        "    '{environment}' as environment\n",
        "from cust \n",
        "\"\"\")\n",
        "##mparticle_egress.show(15,truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Write into Delta table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "mparticle_egress.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").partitionBy(\"Segmentation_date\").save(target_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Exit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "mssparkutils.notebook.exit(\"0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Miscellaneous ad hoc code cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "mparticle_egress.createOrReplaceTempView(\"mparticle_egress\")\n",
        "result_df = spark.sql(\"\"\"\n",
        "        select \n",
        "            Segmentation_date,\n",
        "            count(*)\n",
        "        from mparticle_egress\n",
        "        group by Segmentation_date\n",
        "        order by Segmentation_date desc\n",
        "\"\"\")\n",
        "#result_df.show(40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "result_df = spark.sql(\"\"\"\n",
        "        select instr('{storage_account}','proddev')\n",
        "\"\"\")\n",
        "result_df.show(40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "if \"noproddev\" in storage_account:\n",
        "    print(\"Substring exists\")\n",
        "else:\n",
        "    print(\"Substring does not exist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Export Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from datetime import datetime\n",
        "timestamp_str = datetime.now().strftime('%Y%m%d%H%M%S')  # â†’ e.g. '20250711120802'\n",
        "\n",
        "file_name = f\"Synapse_Profile_Updates_{timestamp_str}-eventless.csv\"\n",
        "print(f\"file_name: {file_name}\")\n",
        "export_path = f\"abfss://export@{storage_account}.dfs.core.windows.net/mParticle/{file_name}\"\n",
        "print(f\"export_path: {export_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Generate CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "mparticle_egress.coalesce(1).write.option(\"header\", \"true\").csv(export_path)"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}