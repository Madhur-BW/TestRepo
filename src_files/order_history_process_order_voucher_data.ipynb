{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Revision History\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Change_date         revision_number     change_description                           author\n",
        "# 08/15/2023          1                   initial check-in                             Kranthi\n",
        "# 01/06/2024          2                   OrderFlags tag fix                           Kranthi \n",
        "# 02/15/2024          3                   replace structured_adls with lakedb_gold     kranthi  \n",
        "#03/12/2024           4                   add upc to unique combination                kranthi \n",
        "#03/25/2024           5                   add shippigstateshort, subtotal columns      kranthi\n",
        "#04/23/2024           6                   display subsidy only for one line            kranthi   \n",
        "#05/08/2024           7                   if a single order, then extracting voucher code should handled kranthi\n",
        "#05/15/2024           8                   add retries for 20 mins                       kranthi\n",
        "#06/17/2024           9                   show all order history                        Kranthi \n",
        "#06/21/2024           10                  do not move the files to archive folder- do a full load daily       Kranthi \n",
        "#07/22/2024           11                  change from delta to parquet                 josh hintz\n",
        "#08/01/2024           12                  change criteria against ZSOM_D12 selection   josh hintz\n",
        "#08/12/2024           13                  addition of ZSOMVOURF (voucher return amt)   josh hintz\n",
        "#08/14/2024           14                  add credit card refund and calculation       josh hintz\n",
        "#08/20/2024           15                  apply refunds only on return records         josh hintz\n",
        "#08/26/2024           16                  merge all to line 1, zero out aggr sums on returns josh hintz\n",
        "#08/28/2024           17                  update line items on exchange-return lines   josh hintz\n",
        "#09/12/2024           18                  change from YTAXAMT1 to ZCNTYTAX             josh hintz\n",
        "#09/23/2024           19                  order date to est, return and exchange initialized dates   josh hintz\n",
        "# 11/18/2024          20                  Added API status filter for Blocked in dev/test   Kettnech\n",
        "#11/19/2024           21                  Adjust env variable in API call for dev/test  josh hintz\n",
        "#11/19/2024           22                  Revert spark pool to spkmed03, change to spkmed34 later   josh hintz\n",
        "#2/3/2025             23                  Remove API call and replace with Voucher delta table  josh hintz\n",
        "#2/3/2025             24                  Fix spark34 failure                           josh hintz "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import from_json, col, concat, lit,array,explode,first,when,expr\n",
        "from pyspark.sql.functions import date_format\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark import HiveContext\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import Row, functions as F\n",
        "from pyspark.sql.window import Window\n",
        "import concurrent.futures\n",
        "import datetime\n",
        "import re\n",
        "from dateutil import tz\n",
        "from time import sleep\n",
        "from pyspark.sql.functions import input_file_name,regexp_extract\n",
        "from pyspark.sql.functions import from_utc_timestamp\n",
        "import xml.etree.ElementTree as ET\n",
        "spark.conf.set(\"spark.sql.parquet.mergeSchema\", True)   \n",
        "spark.conf.set(\"spark.hadoop.parquet.enable.summary-metadata\", True)\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
        "spark.sql(\"SET spark.databricks.delta.schema.autoMerge.enabled = true\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print(mssparkutils.env.getWorkspaceName())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "%run /utils/merge_data_notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /b2b2c/config/b2b2c_config_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Move the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\n",
        "\n",
        "def move_files_from_sftp(file_extension,raw_b2b2cfolder):\n",
        "  print(\"attempting move_files_from_sftp\")\n",
        "  v_file_cnt =0\n",
        "  print(\"file_extension::\",file_extension,\"raw_folder::\",raw_b2b2cfolder)  \n",
        "  file_lst=[c.path for c in mssparkutils.fs.ls(sftp_path) if re.search(f\"\\{file_extension}$\",c.path) is not None]\n",
        "  print(\"file_lst in SFTP::\",file_lst)\n",
        "  mssparkutils.fs.ls(sftp_path)\n",
        "  for j in file_lst :\n",
        "    file_name = j.split(\"/\")[-1].split('.')[0]\n",
        "    if  env_var == 'azwwwprodprdadapsyn01' and 'prod' in file_name :\n",
        "      print(\"PROD SFTP file_name::\",file_name, ' moving ', file_name, ' to raw_path' )\n",
        "      v_file_cnt = v_file_cnt+1\n",
        "      mssparkutils.fs.mv(j,f\"{raw_path}{raw_b2b2cfolder}/{file_name}_{dt}{current_cst_time}{file_extension}\",True)\n",
        "    elif (env_var == 'azwwwnonproddevadapsyn01' or env_var == 'azwwwnonprodtestadapsyn01') and ('staging' in file_name or 'development' in file_name) :\n",
        "      print(\"TEST SFTP file_name::\",file_name, ' moving ', file_name, ' to raw_path' )\n",
        "      v_file_cnt = v_file_cnt+1\n",
        "      mssparkutils.fs.mv(j,f\"{raw_path}{raw_b2b2cfolder}/{file_name}_{dt}{current_cst_time}{file_extension}\",True)\n",
        "\n",
        "  return v_file_cnt    \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## First Step - move the files from SFTP to raw folder "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from notebookutils import mssparkutils\n",
        "spark.sql(\"SET spark.databricks.delta.schema.autoMerge.enabled = true\")\n",
        "retry_attempt = 0\n",
        "v_xml_file_cnt = 0\n",
        "#print(\"env_var::\", env_var)\n",
        "\n",
        "#dt = (datetime.datetime.now()).strftime('%Y-%m-%d')\n",
        "dt = (datetime.datetime.now()).strftime('%Y-%m-%d')\n",
        "dt_time = (datetime.datetime.now()).strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "print(\"date of load::\", dt)\n",
        "dt_date_type = datetime.datetime.strptime(dt, '%Y-%m-%d')\n",
        "\n",
        "dt_datetime_type = datetime.datetime.strptime(dt_time, '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "print(\"time of load UTC::\", dt_datetime_type)\n",
        "\n",
        "\n",
        "from_zone = tz.gettz('UTC')\n",
        "to_zone = tz.gettz('America/Chicago')\n",
        "dt_time = (datetime.datetime.now()).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "\n",
        "utc = datetime.datetime.strptime(dt_time, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "print(\"utc::\",utc)\n",
        "utc = utc.replace(tzinfo=from_zone)\n",
        "cst = utc.astimezone(to_zone)\n",
        "print(\"utc2::\",utc)\n",
        "print(\"cst1::\",cst)\n",
        "print(\"cst2::\",cst.strftime('%H:%M:%S').replace(':',''))\n",
        "current_cst_time = cst.strftime('%H:%M:%S').replace(':','')\n",
        "print(\"current_cst_time\",current_cst_time)\n",
        "\n",
        "#v_csv_file_cnt = move_files_from_sftp('.csv','dpcce')\n",
        "while (retry_attempt < 4) :  \n",
        "  v_xml_file_cnt = move_files_from_sftp('.xml','sfcc')\n",
        "  print('v_xml_file_cnt::',v_xml_file_cnt,'retry_attempt::', retry_attempt)\n",
        "  retry_attempt = retry_attempt +1\n",
        "  if v_xml_file_cnt == 0:\n",
        "    time.sleep(300)\n",
        "  else:\n",
        "    break  \n",
        "\n",
        "#print(\"v_csv_file_cnt::\",v_csv_file_cnt)\n",
        "\n",
        "print(\"v_xml_file_cnt::\",v_xml_file_cnt)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Exit the notebook if no XML files are found in the path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import sys\n",
        "\n",
        "#file_lst_cnt = mssparkutils.fs.ls(f\"{raw_path}sfcc/\")\n",
        "file_lst_cnt = [j for j in mssparkutils.fs.ls(f\"{raw_path}sfcc/\") if j.isDir== False]\n",
        "print(\"xml file count::\",len(file_lst_cnt))\n",
        "if len(file_lst_cnt)<1:\n",
        "  mssparkutils.notebook.exit(\"None\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## read the XML file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# df = spark.read.format('xml').option(\"rootTag\", \"orders\").option(\"rowtag\",\"order\").load(\"abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/b2b2c/sfcc/Order-20231220_2023-12-21063658.xml\")\n",
        "# dfcustom_attributes = df.select(col(\"_order-no\").alias(\"order_no\"),col(\"order-date\").alias(\"order_date\"),\"custom-attributes.*\")\n",
        "# display(dfcustom_attributes)\n",
        "# dfcustom_attributes = dfcustom_attributes.select(\"order_no\",\"order_date\",explode('custom-attribute').alias(\"col1\"))\n",
        "# dfcustom_attributes = dfcustom_attributes.select(\"order_no\",\"order_date\",\"col1.*\")\n",
        "# display(dfcustom_attributes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#df_xml = spark.read.format('xml').option(\"rootTag\", \"orders\").option(\"rowtag\",\"order\").load(\"abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/b2b2c/sfcc/Order-20231220_2023-12-21120844.xml\")\n",
        "#malformed\n",
        "# df_xml = spark.read.format('xml').option(\"rootTag\", \"orders\")\\\n",
        "# .option(\"rowtag\",\"order\")\\\n",
        "# .option(\"nullValue\",\"\")\\\n",
        "# .option(\"inferSchema\", 'false')\\\n",
        "# .option(\"mode\",\"PERMISSIVE\")\\\n",
        "# .load(\"abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/b2b2c/sfcc/Order-20231220_2023-12-21063658.xml\")\n",
        "#df_xml = spark.read.format('xml').option(\"rootTag\", \"orders\").option(\"rowtag\",\"order\").load(\"abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/b2b2c/sfcc/short_xml_test.xml\")\n",
        "df_xml = spark.read.format('xml').option(\"rootTag\", \"orders\").option(\"rowtag\",\"order\").load(f\"{raw_path}sfcc/\")\n",
        "display(df_xml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# display(df_xml.where(\"`_order-no` IN ('T1080039867','T1080039868')\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get Customer details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "\n",
        "dfCustomer = df_xml.select(\"customer.*\")  \n",
        "dfCustomer = dfCustomer.select(\"customer-email\",\"customer-name\",\"customer-no\",\"billing-address.*\") \n",
        "display(dfCustomer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## get payment details only for subsidy data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "subsidy_payment_schema = StructType([\n",
        "    StructField(\"order_no\", StringType(), True),\n",
        "    #StructField(\"order_date\", TimestampType(), True),\n",
        "    StructField(\"is_redeemed\", StringType(), True),\n",
        "    StructField(\"voucher_amount\", IntegerType(), True),\n",
        "    StructField('voucher_code',StringType(), True)\n",
        "                      ])\n",
        "dummy_data = [('a','a',0,'a')]  \n",
        "stringified_array_schema = ArrayType(StructType([StructField(\"_VALUE\",StringType(),True)\n",
        ",StructField(\"_attribute_id\",StringType(),True)]))                   \n",
        "\n",
        "try:\n",
        "    dfPayment = df_xml.select(col(\"_order-no\").alias(\"order_no\"),col(\"order-date\").alias(\"order_date\"),\"payments.*\")\n",
        "   #dfPayment = dfPayment.select(col(\"_order-no\").alias(\"order_no\"),explode('payment').alias(\"payment\"))\n",
        "   #dfPayment = dfPayment.select('*','payment.custom-attributes.*')\n",
        "   #display(dfPayment)\n",
        "    dfPayment = dfPayment.select('order_no','order_date',explode(\"payment.custom-method\").alias('payment_custom_attr'))\n",
        "    dfPayment = dfPayment.select('order_no','order_date','payment_custom_attr.custom-attributes.custom-attribute')\n",
        "    dfPayment = dfPayment.select('order_no','order_date',explode('custom-attribute').alias('payment_custom_attr'))\n",
        "    dfPayment = dfPayment.groupBy(\"order_no\",\"order_date\").pivot(\"payment_custom_attr._attribute-id\").agg(first(\"payment_custom_attr._VALUE\"))\n",
        "    dfPayment = dfPayment.selectExpr('order_no'\n",
        "    ,'order_date'\n",
        "    ,'isRedeemed as is_redeemed'\n",
        "    , 'isSubsidy as is_subsidy'\n",
        "    ,'voucherAmount as voucher_amount'\n",
        "    ,'voucherCode as voucher_code')\n",
        "    display(dfPayment)    \n",
        "except Exception as e:\n",
        "    if  \"cannot resolve 'explode(\" in str(e):\n",
        "      print('payment is of type STRUCT, cannot apply explode()')  \n",
        "      dfPayment = dfPayment.select('order_no','order_date','payment.*')\n",
        "      dfPayment = dfPayment.select('order_no','order_date',\"custom-method.custom-attributes.*\")\n",
        "      dfPayment = dfPayment.select('order_no','order_date',\"custom-attribute\")\n",
        "      dfPayment = dfPayment.select('order_no','order_date',explode('custom-attribute').alias('custom-attribute'))     \n",
        "      dfPayment = dfPayment.groupBy(\"order_no\",\"order_date\").pivot(\"custom-attribute._attribute-id\").agg(first(\"custom-attribute._VALUE\"))\n",
        "      dfPayment = dfPayment.selectExpr('order_no'\n",
        "                                    , 'order_date'\n",
        "                                    , 'isRedeemed as is_redeemed'\n",
        "                                    , 'isSubsidy as is_subsidy'\n",
        "                                    , 'voucherAmount as voucher_amount'\n",
        "                                    , 'voucherCode as voucher_code')\n",
        "  \n",
        "    else:    \n",
        "      print(\"no voucher/custom-method payment present::\") \n",
        "      dfPayment =   spark.createDataFrame(dummy_data, subsidy_payment_schema)\n",
        "      dfPayment = dfPayment.withColumn(\"order_date\",F.current_timestamp()) \n",
        "display(dfPayment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#display(dfPayment.select('*').where(\"\"\"order_no IN ('T1080027936','T1080027937','T1080027939','T1080027940','T1080027941','T1080027942','T1080027943','T1080027951','T1080027952','T1080027953','T1080027945','T1080027946',\n",
        "#'T1080027947','T1080027954','T1080027955','T1080027957')\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## get the payment data for creditcard when partially paid by voucher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "cc_payment_schema = StructType([\n",
        "    StructField(\"order_no\", StringType(), True),     \n",
        "    StructField(\"pay_amt\", IntegerType(), True),\n",
        "    StructField('card-holder',StringType(), True),\n",
        "    StructField('card-number',StringType(), True),\n",
        "    StructField('card-token',StringType(), True),\n",
        "    StructField('card-type',StringType(), True),\n",
        "    StructField('expiration-month',StringType(), True),\n",
        "    StructField('expiration-year',StringType(), True)\n",
        "                      ])\n",
        "dummy_data = [('a',0,'a','a','a','a','a','a')]  \n",
        "try:\n",
        "    dfPaymentcc0 = df_xml.select(col(\"_order-no\").alias(\"order_no\"),col(\"order-date\").alias(\"order_date\"),\"payments.*\")\n",
        "    #display(dfPaymentcc0)\n",
        "    dfPaymentcc = dfPaymentcc0.select('order_no','order_date','payment.amount',explode(\"payment.credit-card\").alias('payment_credit_card'))\n",
        "    #display(dfPaymentcc)\n",
        "    dfPaymentcc = dfPaymentcc.selectExpr('order_no','order_date','amount as pay_amt','payment_credit_card.*')\n",
        "    display(dfPaymentcc)\n",
        "except Exception as e:\n",
        "    print(\"no credit card payment present::\") \n",
        "    dfPaymentcc =   spark.createDataFrame(dummy_data, cc_payment_schema)\n",
        "    dfPaymentcc = dfPaymentcc.withColumn(\"order_date\",F.current_timestamp()) \n",
        "    display(dfPaymentcc)\n",
        "\n",
        "# dfPaymentccattr = dfPaymentcc0.select('order_no','order_date', explode(array('payment.custom-attributes')).alias('payment_cc_custom-attr'))\n",
        "# dfPaymentccattr = dfPaymentccattr.select('order_no','order_date',explode(array('payment_cc_custom-attr.custom-attribute')).alias('payment_cc_custom-attr'))  \n",
        "# dfPaymentccattr = dfPaymentccattr.groupBy(\"order_no\",\"order_date\").pivot(\"payment_cc_custom-attr._attribute-id\").agg(first(\"payment_cc_custom-attr._VALUE\"))\n",
        "# dfPaymentcc = dfPaymentcc.join(dfPaymentccattr,['order_no','order_date']).selectExpr('order_no','order_date','cardType as cc_card_type','authAmount as cc_amount_paid').distinct()\n",
        "# display(dfPaymentcc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#display(dfPaymentcc.select('*').where(\"\"\"order_no IN ('T1080027936','T1080027937','T1080027939','T1080027940','T1080027941','T1080027942','T1080027943','T1080027951','T1080027952','T1080027953','T1080027945','T1080027946',\n",
        "#'T1080027947','T1080027954','T1080027955','T1080027957')\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# get product lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "\n",
        "try:\n",
        "    df_product_lines = df_xml.select(col(\"_order-no\").alias(\"order_no\")\n",
        "    ,col(\"order-date\").alias(\"order_date_time\")\n",
        "    ,explode(\"product-lineitems.product-lineitem\").alias('line_item'))\n",
        "    df_product_lines = df_product_lines.select(\"order_no\"\n",
        "    ,\"order_date_time\"\n",
        "    ,\"line_item.`gross-price`\"\n",
        "    ,\"line_item.`lineitem-text`\"\n",
        "    ,\"line_item.`product-id`\"\n",
        "    ,\"line_item.quantity\"\n",
        "    ,\"line_item.`product-name`\" ).select('*').distinct()\n",
        "\n",
        "    display(df_product_lines)\n",
        "except Exception as e:\n",
        "  #print('e',type(e).__name__, str(e)[0:30])  \n",
        "  if  \"\"\"Cannot resolve \"explode(\"\"\" in str(e):\n",
        "    df_product_lines = df_xml.select(col(\"_order-no\").alias(\"order_no\")\n",
        "    ,col(\"order-date\").alias(\"order_date_time\")\n",
        "    ,col(\"product-lineitems.product-lineitem\")\\\n",
        "    .alias('line_item'))\n",
        "    df_product_lines = df_product_lines.selectExpr(\"order_no\"\n",
        "    ,\"order_date_time\"\n",
        "    ,\"line_item.`gross-price`\"\n",
        "    ,\"line_item.`lineitem-text`\"\n",
        "    ,\"line_item.`product-id`\"\n",
        "    ,\"line_item.quantity\"\n",
        "    ,\"line_item.`product-name`\" ).select('*').distinct()\n",
        "    display(df_product_lines)\n",
        "  else:\n",
        "    print('Different error', str(e))\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "from pyspark.sql.functions import col, explode\n",
        "\n",
        "try:\n",
        "    df_product_lines = df_xml.select(\n",
        "        col(\"_order-no\").alias(\"order_no\"),\n",
        "        col(\"order-date\").alias(\"order_date_time\"),\n",
        "        explode(\"product-lineitems.product-lineitem\").alias(\"line_item\")\n",
        "    )\n",
        "\n",
        "    df_product_lines = df_product_lines.select(\n",
        "        \"order_no\",\n",
        "        \"order_date_time\",\n",
        "        col(\"line_item.gross-price\").alias(\"gross_price\"),\n",
        "        col(\"line_item.lineitem-text\").alias(\"lineitem_text\"),\n",
        "        col(\"line_item.product-id\").alias(\"product_id\"),\n",
        "        col(\"line_item.quantity._VALUE\").alias(\"quantity\"),  # Extracting only the value from the JSON structure\n",
        "        col(\"line_item.product-name\").alias(\"product_name\")\n",
        "    ).distinct()\n",
        "\n",
        "    display(df_product_lines)\n",
        "\n",
        "except Exception as e:\n",
        "    if  \"\"\"Cannot resolve \"explode(\"\"\" in str(e):\n",
        "        df_product_lines = df_xml.select(\n",
        "            col(\"_order-no\").alias(\"order_no\"),\n",
        "            col(\"order-date\").alias(\"order_date_time\"),\n",
        "            col(\"product-lineitems.product-lineitem\").alias(\"line_item\")\n",
        "        )\n",
        "\n",
        "        df_product_lines = df_product_lines.select(\n",
        "            \"order_no\",\n",
        "            \"order_date_time\",\n",
        "            col(\"line_item.gross-price\").alias(\"gross_price\"),\n",
        "            col(\"line_item.lineitem-text\").alias(\"lineitem_text\"),\n",
        "            col(\"line_item.product-id\").alias(\"product_id\"),\n",
        "            col(\"line_item.quantity._VALUE\").alias(\"quantity\"),  # Extracting only the value from the JSON structure\n",
        "            col(\"line_item.product-name\").alias(\"product_name\")\n",
        "        ).distinct()\n",
        "\n",
        "        display(df_product_lines)\n",
        "    else:\n",
        "        print(\"Error:\", str(e))\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#dfcustom_attributes = df_xml.select(col(\"_order-no\").alias(\"order_no\"),col(\"order-date\").alias(\"order_date\"),\"custom-attributes.*\")\n",
        "#dfcustom_attributes = dfcustom_attributes.select(\"order_no\",\"order_date\",explode('custom-attribute').alias(\"col1\"))\n",
        "#display(dfcustom_attributes.select('*'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## get custom attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "file_list = []\n",
        "custom_attributes_lod = []\n",
        "custom_attrib_schema = StructType([\n",
        "    StructField(\"order_no\", StringType(), True),\n",
        "    #StructField(\"order_date_time\", TimestampType(), True),\n",
        "    StructField(\"POLocation\", StringType(), True),\n",
        "    StructField(\"accountName\", StringType(), True),\n",
        "    StructField('blanketPO',StringType(), True),\n",
        "    StructField('brandBusinessUnit',StringType(), True),\n",
        "    StructField('costcenter',StringType(), True),\n",
        "    StructField('employeeID',StringType(), True),\n",
        "    StructField('employeeLocation',StringType(), True),\n",
        "    StructField('firstName',StringType(), True),\n",
        "    StructField('lastName',StringType(), True),\n",
        "    StructField('districtOrLocationCodeOrBranch',StringType(), True),\n",
        "    StructField('storeOrDept',StringType(), True),\n",
        "    StructField('salesCenter',StringType(), True),\n",
        "    StructField('POValue',StringType(), True),\n",
        "                      ])\n",
        "\n",
        "def parse_xml(file_name):\n",
        "    ## read the text file, convert to string and parse the string\n",
        "    file_text = spark.read.text(f\"{raw_path}sfcc/{file_name}\", wholetext=True)\n",
        "    xml_data = file_text.collect()[0][0]\n",
        "    root = ET.fromstring(xml_data)\n",
        "\n",
        "    # Get the dynamic namespace\n",
        "    namespace = {'ns': root.tag.split('}')[0][1:]}\n",
        "   \n",
        "    # Iterate over each order\n",
        "    for order in root.findall('.//ns:order', namespaces=namespace):\n",
        "        custom_attributes = {}\n",
        "        custom_attributes['order_no'] = order.get('order-no')\n",
        "        #datetime.strptime(timestamp_str, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "        #custom_attributes['order_date'] = datetime.datetime.strptime(order.find('./ns:order-date', namespaces=namespace).text,'%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "                                                                        \n",
        "        \n",
        "        # Extract parent-level custom-attributes\n",
        "        #parent_level_attributes = order.findall('./ns:custom-attributes/ns:custom-attribute', namespaces=namespace)\n",
        "        parent_level_attributes = order.findall('./ns:custom-attributes/ns:custom-attribute', namespaces=namespace)\n",
        "        # Print order details\n",
        "        #print(f\"\\nOrder Number: {order_no}\")\n",
        "        #print(f\"Order Date: {order_date}\")\n",
        "       \n",
        "        \n",
        "        # Print parent-level custom-attributes\n",
        "        for attr in parent_level_attributes:\n",
        "            attribute_id = attr.get('attribute-id')\n",
        "            value = attr.text\n",
        "            custom_attributes[attribute_id] = value\n",
        "            #print(f\"{attribute_id}: {value}\")\n",
        "        #print(custom_attributes)\n",
        "        custom_attributes_lod.append(custom_attributes)\n",
        " \n",
        "  \n",
        "\n",
        "    \n",
        "# extract file names \n",
        "file_list = [j.name for j in mssparkutils.fs.ls(f\"{raw_path}sfcc/\") if j.size>0]\n",
        " \n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        results = list(executor.map(parse_xml, file_list))\n",
        "#print(custom_attributes_lod)           \n",
        "# Create a DataFrame from the list of dictionaries\n",
        "dfcustom_attributes = spark.createDataFrame(data=custom_attributes_lod,schema=custom_attrib_schema)\n",
        "#display(dfcustom_attributes)   \n",
        "\n",
        "# dfcustom_attributes = df_xml.select(col(\"_order-no\").alias(\"order_no\"),col(\"order-date\").alias(\"order_date\"),\"custom-attributes.*\")\n",
        "# display(dfcustom_attributes)\n",
        "# #display(dfcustom_attributes.select(\"*\").where(\"order_no='T1080008309'\"))\n",
        "# dfcustom_attributes = dfcustom_attributes.select(\"order_no\",\"order_date\",explode('custom-attribute').alias(\"col1\"))\n",
        "# #dfcustom_attributes = dfcustom_attributes.selectExpr(\"_order-no\",\"explode('custom-attribute') as col1\")\n",
        "# display(dfcustom_attributes.select(\"order_no\",\"col1.*\"))\n",
        "\n",
        "# dfcustom_attributes = dfcustom_attributes.select(\"order_no\",\"order_date\",\"col1.*\")\n",
        "# #display(dfcustom_attributes.select(\"*\").where(\"order_no='T1080008309'\"))\n",
        "#dfcustom_attributes  = dfcustom_attributes.groupBy(\"order_no\",\"order_date\").pivot(\"_attribute-id\").agg(first(\"_VALUE\"))\n",
        "dfcustom_attributes = dfcustom_attributes.join(df_product_lines,[\"order_no\"],\"inner\")\n",
        "dfcustom_attributes = dfcustom_attributes.select(\"*\").distinct()\n",
        "display(dfcustom_attributes)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#display(dfcustom_attributes.select('*').where(\"\"\"order_no IN ('T1080027945','T1080027946',\n",
        "#'T1080027947','T1080027954','T1080027955','T1080027957')\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Retreive API password and today's bearer token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#import requests\n",
        "#import json\n",
        "\n",
        "#voucher_api_pwd = mssparkutils.credentials.getSecret(kv_name,'b2b2cVoucherApiPassword','ls_kv_adap' )\n",
        "#voucher_api_authorization = mssparkutils.credentials.getSecret(kv_name,'b2b2cVoucherApiAuthorization','ls_kv_adap' )\n",
        "#voucher_api_pwd = mssparkutils.credentials.getSecret(kv_name,'b2b2cVoucherApiPassword')\n",
        "# import sys  \n",
        "# from pyspark.sql import SparkSession  \n",
        "  \n",
        "# sc = SparkSession.builder.getOrCreate()  \n",
        "# token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary  \n",
        "  \n",
        "# voucher_api_pwd = token_library.getSecret(kv_name, 'b2b2cVoucherApiPassword')  \n",
        "\n",
        "#headers = { 'Content-Type': 'application/x-www-form-urlencoded' }\n",
        "\n",
        "#body = { 'grant_type': 'password',\n",
        "#          'client_id': b2b_client_id,\n",
        "#          'client_secret': b2b_client_secret,\n",
        "#          'username': b2b2c_voucher_api_user_name,\n",
        "#          'password': voucher_api_pwd\n",
        "#          }\n",
        "\n",
        "#response = requests.post(b2b2c_voucher_token_url,\n",
        "#                         headers=headers,\n",
        "#                         data=body)\n",
        "\n",
        "#voucher_api_authorization = json.loads(response.text)['access_token']\n",
        "\n",
        "#print(voucher_api_pwd)\n",
        "#print(voucher_api_authorization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Get the token to be used for calling coupon API\n",
        "## This is commented out as broken, above code blcok replaces the below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#import requests\n",
        "#import json\n",
        "\n",
        "\n",
        "\n",
        "#headers = {\n",
        "#    'Content-Type': 'application/x-www-form-urlencoded',\n",
        "#    'Authorization': voucher_api_authorization\n",
        "#}\n",
        "\n",
        "#data = {\n",
        "#    'client_id': b2b_client_id,\n",
        "#    'client_secret': b2b_client_secret,\n",
        "#    'password': voucher_api_pwd,\n",
        "#    'grant_type': 'password',\n",
        "#    'username': b2b2c_voucher_api_user_name ,\n",
        "#}\n",
        "\n",
        "#response = requests.post(b2b2c_voucher_token_url,    \n",
        "#                        headers=headers,\n",
        "#                        data=data,\n",
        "#)\n",
        "#json_resp = json.loads(response.text)\n",
        "#print(\"token::\",json_resp['access_token'])\n",
        "#print(json.load(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Write voucher raw to bronze "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "file_list = mssparkutils.fs.ls(f'{raw_adls_path}Salesforce/dpcce/Voucher__c/')\n",
        "file_path = f'{raw_adls_path}Salesforce/dpcce/Voucher__c/'\n",
        "\n",
        "for f in file_list:\n",
        "\n",
        "    if f != \"archive\":\n",
        "\n",
        "        print(\"--DPCCE Voucher--: Reading file \" + file_path + f.name)\n",
        "        dpcce = spark.read.json(file_path + f.name)\n",
        "\n",
        "        display(dpcce)\n",
        "\n",
        "        display(dpcceSql)\n",
        "\n",
        "        #Create temp view\n",
        "        dpcce.createOrReplaceTempView(\"Voucher__c\")\n",
        "\n",
        "        count = spark.sql(\"SELECT COUNT(*) AS COUNT FROM Voucher__c\")\n",
        "\n",
        "        #Create view as new dataframe osView\n",
        "        if count.collect()[0][0] > 0:\n",
        "            print(\"--DPCCE Voucher--: Processing \" + str(count.collect()[0][0]) + \" records...\")\n",
        "            dpcceView = spark.sql(dpcceSql)\n",
        "            createMergeData('Salesforce/DPCCE/Voucher__c', dpcceView, \"Voucher__c\", \"Salesforce/DPCCE\")\n",
        "        else:\n",
        "            print(\"--DPCCE Voucher--: No records found - empty file?\")\n",
        "\n",
        "        #Move files to archive folder to reduce process time if we're in production, if not leave them alone\n",
        "        mssparkutils.fs.mv(file_path + f.name, raw_adls_path + 'Salesforce/dpcce/Voucher__c_archive/' + f.name,True, overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## read voucher data returned by  API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#import requests\n",
        "#from pyspark.sql import SparkSession\n",
        "#from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DateType\n",
        "\n",
        "\n",
        "#print(voucher_api_authorization)\n",
        "\n",
        "\n",
        "#headers = {\n",
        "#    'Content-Type': 'application/json',\n",
        "    #'Authorization': f\"Bearer {json_resp['access_token']}\",\n",
        "#    'Authorization': f\"Bearer {voucher_api_authorization}\",\n",
        "    # 'Cookie': 'BrowserId=n5uowCD8Ee68SElowr2Prw; CookieConsentPolicy=0:1; LSKey-c$CookieConsentPolicy=0:1',\n",
        "#}\n",
        "\n",
        "# 20241118 KETTNECH - appears that thousands of Expired vouchers were added to dev/test recently and are overflowing the text string below.\n",
        "# Limiting in dev/test\n",
        "# Valid values appear to be: Active, Printed, Blocked, Disabled, Expired, Removed, Inactive\n",
        "# Not sure how to specify multiple select here (tried ['1','2','3',etc])\n",
        "\n",
        "#if env_var == 'azwwwnonproddevadapsyn01' or env_var == 'azwwwnonprodtestadapsyn01':\n",
        "#    json_data = {\n",
        "#        'status': 'Redeemed',\n",
        "#        'employeeid': '',\n",
        "#        'companyid': '',\n",
        "#        'emailid': '',\n",
        "#    }\n",
        "#else:\n",
        "#    json_data = {\n",
        "#        'status': '',\n",
        "#        'employeeid': '',\n",
        "#        'companyid': '',\n",
        "#        'emailid': '',\n",
        "#    }\n",
        "\n",
        "#response1 = requests.post(b2b2c_voucher_api_url,   \n",
        "#                        headers=headers,\n",
        "#                        json=json_data,\n",
        "#)\n",
        "\n",
        "#print(response1.text)\n",
        "#json_resp1 = json.loads(response1.text)\n",
        "#print(\"response status::\",json_resp1['success'])\n",
        "#if json_resp1['success'] == True:\n",
        "#    print('success response')\n",
        "#    #print(json_resp1['response'])\n",
        "#    #prepare list of tuples\n",
        "#    voucher_d = [(k, v) for k, v in json_resp1['response'].items()]\n",
        "#    voucher_data = spark.createDataFrame(voucher_d, voucher_schema)\n",
        "\n",
        "#    voucher_data = voucher_data.selectExpr('voucher_status','explode(voucher_data) as voucher_data')\n",
        "#    df_csv_voucher = voucher_data.selectExpr('voucher_data.vouchercode as voucher_code'\n",
        "#    ,'voucher_status as status'\n",
        "#    ,'voucher_data.startDate as voucher_start_dt','voucher_data.expirydate as expiry_date'\n",
        "#    ,'voucher_data.employeeid as employee_id', 'voucher_data.amount', 'voucher_data.companyid as company_id'\n",
        "#    ,'voucher_data.redemptiondate as redemption_date','row_number() over(partition by voucher_data.vouchercode order by voucher_data.expirydate desc) as rn')\n",
        "#    df_csv_write = write_to_file(df_csv_voucher,f\"{raw_path}dpcce/\")\n",
        "#    df_csv_voucher_dups = df_csv_voucher.select(\"*\").where(\"rn>1\")\n",
        "#    df_csv_voucher = df_csv_voucher.select(\"*\").where(\"rn=1\").drop(\"rn\")\n",
        "#    display(df_csv_voucher)\n",
        "#else:\n",
        "#    print(\"No API response\")    \n",
        "\n",
        "#New method reading delta table 1/27/2025 - hintzjo\n",
        "\n",
        "df_vou = DeltaTable.forPath(spark, f'{bronze_adls_path}/Salesforce/DPCCE/Voucher__c').toDF()\n",
        "df_csv_voucher = df_vou.selectExpr('Voucher_Code__c as voucher_code'\n",
        "    ,'Status__c as status'\n",
        "    ,'Start_Date__c as voucher_start_dt'\n",
        "    ,'Expiry_Date__c as expiry_date'\n",
        "    ,'Employee_Id__c as employee_id'\n",
        "    ,'Amount__c as amount'\n",
        "    ,'Company_Id__c as company_id'\n",
        "    ,'Redemption_Date__c as redemption_date'\n",
        "    ,'Partial_Redemption_Allowed__c as partial_redemption_allowed'\n",
        "    ,'Balance__c as remaining_balance'\n",
        "    ,'Redeemed_Amount__c as redeemed_amount'\n",
        "    ,'Original_Amount__c as original_amount')\n",
        "\n",
        "display(df_csv_voucher)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#display(df_csv_voucher.select('employee_id','company_id','status','voucher_start_dt','voucher_code').where(\"voucher_code like '20240228%'\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## print duplicate voucher codes from Voucher API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#display(df_csv_voucher_dups)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print(df_csv_voucher.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## get data from Fact.orders "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# from notebookutils import mssparkutils\n",
        "\n",
        "# import sys  \n",
        "# from pyspark.sql import SparkSession  \n",
        "  \n",
        "# sc = SparkSession.builder.getOrCreate()  \n",
        "# token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary  \n",
        "# #token_library.getSecret(\"kv-name\", \"secret-name\", \"linked-service\")  \n",
        "# jdbcPassword = token_library.getSecret(kv_name, \"SqlAdmin\", \"ls_kv_adap\")  \n",
        "# print(jdbcPassword)\n",
        "\n",
        "# #jdbcHostname = 'az-www-datahub-nonprod-dev-adap-sql.database.windows.net'\n",
        "# #\"azwwwnonproddevadapsyn01.sql.azuresynapse.net\"\n",
        "# #jdbc:sqlserver://azwwwnonproddevadapsyn01.sql.azuresynapse.net:1433;database=syndw01;user=sqlAdmin@azwwwnonproddevadapsyn01;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\n",
        "# jdbcPort = 1433\n",
        "\n",
        "# #az-www-datahub-prod-prd-adap-sql.database.windows.net\n",
        "\n",
        "# jdbcDatabase = \"dw\"\n",
        "\n",
        "# jdbcUsername = \"sqlAdmin\"\n",
        "\n",
        "# #jdbcPassword = mssparkutils.credentials.getSecret('az-www-dev-adap-kv', 'SqlAdmin')\n",
        "\n",
        "# jdbcDriver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
        "\n",
        "# #url = s\"jdbc:sqlserver://${database_host}:${database_port}/${database_name}\"\n",
        "\n",
        "# table = \"fact.orders\"\n",
        "\n",
        "# jdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};databaseName={jdbcDatabase}\"\n",
        "\n",
        "# df_fact_orders = spark.read.format(\"jdbc\")\\\n",
        "# .option(\"driver\", jdbcDriver)\\\n",
        "# .option(\"url\", jdbcUrl)\\\n",
        "# .option(\"dbtable\", table)\\\n",
        "# .option(\"user\", jdbcUsername)\\\n",
        "# .option(\"password\", jdbcPassword).load()\n",
        "# display(df_fact_orders.select(\"*\"))\n",
        "df_fact_orders = spark.read.format('delta').load(f'{bronze_adls_path}SAP/BW/ZSOM_D12')\n",
        "#display(df_fact_orders.where(\"`/BIC/ZSOMOSMNM` IN ('T1080039863')\"))\n",
        "df_fact_orders = df_fact_orders.selectExpr('`/BIC/ZSOMOSMNM` as order_number'\n",
        "                                ,'`/BIC/ZUPCU` as upc' # ProductCode\n",
        "                                ,'`/BIC/ZMATNUM` as stock_number' # StockNumber\n",
        "                                ,'`/BIC/YQTY` as quantity' \n",
        "                                ,'`/BIC/ZSHOEWD`  as width'\n",
        "                                ,'`/BIC/ZSHOESZ` as size'\n",
        "                                ,'`/BIC/ZSOMOSTTL` as pre_tax_total' # PreTaxTotal/pre_tax_total\n",
        "                                ,'`/BIC/ZCNTYTAX` as  item_tax' # TaxAmount1/item_tax\n",
        "                                #,'round(`/BIC/ZSOMOTTL`,3) as  order_total' #GrandTotal/order_total\n",
        "                                ,'round(`/BIC/ZSOMOSTTL`,3) + round(`/BIC/ZCNTYTAX`,3) as order_total'\n",
        "                                ,'`/BIC/ZREC_TYPE` as record_type' # RecordType/record_type\n",
        "                                ,'`ERDAT` as date_key' #OrderCreatedDate\n",
        "                                ,'`/BIC/ZSOMDLVST` as ShipToState' #ShipToRegion or ship_to_state\n",
        "                                ,'`/BIC/YAMOUNT` as sale_price' #UnitPrice\n",
        "                                ,'`/BIC/ZSOMVEMP` as voucheremployeeid' #voucheremployeeid\n",
        "                                ,'`/BIC/ZSOMVSTE` as vouchersiteid' # vouchersiteid\n",
        "                                ,'`/BIC/ZSOMVBBU` as voucherbrandbusinessunit' # VoucherBrandBusinessUnit\n",
        "                                ,'`/BIC/ZSOMLINE` as LineNumber'  \n",
        "                                ,'round(`/BIC/ZSOMVOUPY`,3) as LineItemSubsidyCharge' #LineItemSubsidyCharge\n",
        "                                ,'round(`RPA_TAT`,3) as LineItemCreditCardCharge' # TenderValue/LineItemCreditCardCharge                                \n",
        "                                ,'`/BIC/ZSOMOISST` as ship_status'\n",
        "                                ,'`/BIC/ZSOMSHPDT` as ship_date'\n",
        "                                ,'`/BIC/YSHIPCHRG` as ship_charge'\n",
        "                                ,'`/BIC/ZSOMVOURF` as voucher_refund_amt'\n",
        "                                ,'`/BIC/ZSOMRFAMT` as cc_refund_amt' \n",
        "                                ,'`/BIC/ZRPLCSKU` as replacement_upc'\n",
        "                                ,'`/BIC/ZSOMRTNDT` as return_exchange_date'\n",
        "                                ,'row_number() over(partition by `/BIC/ZSOMOSMNM`,`/BIC/ZREC_TYPE`,`/BIC/ZSOMLINE` order by  `AEDAT` DESC,`/BIC/ZAEDAT` DESC,`/BIC/ZCHGTIME` DESC) as rn') \n",
        "#display(df_fact_orders.select('*').where(\"order_number = 'T1080030340'\"))\n",
        "#display(df_fact_orders.where(\"order_number IN ('T1080030340')\"))\n",
        "df_fact_orders = df_fact_orders.where('rn=1')\n",
        "display(df_fact_orders)\n",
        "#display(df_fact_orders.where(\"order_number IN ('T1080039863')\"))\n",
        "\n",
        "#Original where clause - commented out 8/1/2024 - josh hintz\n",
        "#df_fact_order_one_upc = df_fact_orders.selectExpr('order_number','upc','LineNumber')\\\n",
        "#                        .where(\"record_type = 'O' and upc is not null and upc != 'SHIPMENTCOSTS'\")\n",
        "\n",
        "#New where clause - added 8/1/2024 - josh hintz\n",
        "df_fact_order_one_upc = df_fact_orders.selectExpr('order_number','upc','LineNumber')\\\n",
        "                        .where(\"record_type in ('O','E') and upc is not null and upc != 'SHIPMENTCOSTS'\")\n",
        "\n",
        "df_fact_order_one_upc = df_fact_order_one_upc\\\n",
        "       .selectExpr('order_number','first(`upc`) over(partition by `order_number` order by `upc`,`LineNumber`) as upc')\\\n",
        "       .distinct()                       \n",
        "#display(df_fact_order_one_upc.where(\"order_number IN ('T1080033220','T1080033222','T1080033221')\"))\n",
        "df_fact_ordersT = df_fact_orders.selectExpr('order_number', 'record_type as rec_type','LineItemSubsidyCharge','LineItemCreditCardCharge','voucher_refund_amt','cc_refund_amt')\\\n",
        ".where(\"rec_type='T' \")\n",
        "df_fact_ordersT = df_fact_ordersT.join(df_fact_order_one_upc,['order_number'],\"inner\")\n",
        "#display(df_fact_ordersT.where(\"order_number IN ('T1080045391')\")) #record_type='T' and LineItemSubsidyCharge>0 \n",
        "#display(df_fact_orders.where(\"order_number IN ('T1080045391')\"))\n",
        "#df_fact_orders.printSchema()\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## join  SFCC and DPCCE on employeeId ,accountName/company_id and voucher_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "dpcce_join_sfcc = ''\n",
        "dpcce_join_sfcc = dfcustom_attributes.alias('da').join(dfPayment.alias('dfp'),(dfcustom_attributes.order_no==dfPayment.order_no),\"left\")\n",
        "dpcce_join_sfcc = dpcce_join_sfcc.join(df_csv_voucher.alias('dc'), ((df_csv_voucher.employee_id==dfcustom_attributes.employeeID) & (F.lower(df_csv_voucher.company_id)==F.lower(dfcustom_attributes.accountName)) & (dfPayment.voucher_code ==df_csv_voucher.voucher_code)) ,\"left\" )\n",
        "dpcce_join_sfcc = dpcce_join_sfcc.selectExpr(\"da.order_no as order_no\"\n",
        ",'da.`product_id` as upc'\n",
        ",\"da.order_date_time as order_date_time\"\n",
        ",\"cast(da.order_date_time as date) as order_date\"\n",
        ",\"da.brandBusinessUnit as brand_business_unit\" \n",
        ",\"da.accountName as account_name\"\n",
        ",\"da.employeeID as employee_id\"\n",
        ",\"da.lastName as last_name\"\n",
        ",\"da.firstName as first_name\"\n",
        ",\"dc.voucher_start_dt\" \n",
        ",\"dc.voucher_code\"\n",
        ",\"dc.status as voucher_status\"\n",
        ",\"dc.partial_redemption_allowed\"\n",
        ",\"dc.remaining_balance\"\n",
        ",\"dc.redeemed_amount\"\n",
        ",\"dc.original_amount\"\n",
        ",'da.employeeLocation as b2b_customer_location'\n",
        ",'da.costCenter as cost_center'\n",
        ",\"da.districtOrLocationCodeOrBranch as b2b_location_code\" ## b2b_location_code \n",
        ",\"da.storeOrDept as b2b_store_no\" #B2B - STORE # \n",
        ",\"da.salesCenter as b2b_sales_center\" #B2B - SALES CENTER #\n",
        ",\"da.POLocation as b2b_po_location\" #B2B PO-Location\n",
        ",\"da.blanketPO as b2b_blanket_po\"\n",
        ",\"da.POValue as POValue\" #this is po_number from SFCC--pending,this field is not there yet, only applies for a PO enabled site \n",
        "##,\"round(dfp.voucher_amount,3) as line_item_subsidy_charge\"\n",
        ")\n",
        "\n",
        "#dpcce_join_sfcc = dpcce_join_sfcc.alias('sfcc1').join(dfPayment.alias('dp'),['voucher_code'],\"inner\")\n",
        "#dpcce_join_sfcc = dpcce_join_sfcc.selectExpr(\"sfcc1.*\",'dp.is_redeemed','dp.is_subsidy','dp.voucher_amount as line_item_subsidy_charge','dp.cc_amount_paid as line_item_cc_charge')\n",
        "display(dpcce_join_sfcc.select('*').distinct())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(dfcustom_attributes.select('accountName').distinct())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "df_state_codes = spark.read.format('csv')\\\n",
        "       .option(\"header\", \"true\")\\\n",
        "       .load(f\"{raw_adls_path}b2b2c/state_codes/US_state_codes.csv\")\n",
        "display(df_state_codes)       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## join with fact_orders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "dpcce_sfcc_fact_orders=''\n",
        "dpcce_sfcc_fact_orders = dpcce_join_sfcc.alias('sf')\\\n",
        ".join(df_fact_orders.alias('fo'),(dpcce_join_sfcc.order_no == df_fact_orders.order_number)&  (dpcce_join_sfcc.upc == df_fact_orders.upc) ,\"inner\")\\\n",
        ".join(df_state_codes.alias('scodes'),(df_fact_orders.ShipToState == df_state_codes.us_state),\"left\")\\\n",
        ".join(df_fact_ordersT.alias('fot'),((df_fact_ordersT.order_number == df_fact_orders.order_number)&(df_fact_orders.LineNumber == '00000000001')),\"left\")\n",
        "#.join(df_fact_ordersT.alias('fot'),((df_fact_ordersT.order_number == df_fact_orders.order_number)&(df_fact_ordersT.upc == df_fact_orders.upc)&(df_fact_orders.LineNumber == '00000000001')),\"left\")\n",
        "dpcce_sfcc_fact_orders = dpcce_sfcc_fact_orders.selectExpr('sf.*'\n",
        ",'fo.stock_number'\n",
        ",'fo.quantity as product_qty'\n",
        ",'fo.width'\n",
        ", \"case when cast(ltrim('0',fo.size) as int) >20 then cast(ltrim('0',fo.size) as int)/10 else cast(ltrim('0',fo.size) as int) end as size\"\n",
        ",'fo.pre_tax_total as line_item_price'\n",
        ",'fo.item_tax line_item_tax'\n",
        ",'fo.order_total line_item_total'\n",
        ",'fo.record_type as order_type'\n",
        ",'fo.date_key'\n",
        ",'fo.shipToState as shipping_state'\n",
        ",'fo.sale_price'\n",
        ",'fo.voucheremployeeid as voucher_emp_id'\n",
        ",'fo.vouchersiteid as voucher_site_id'\n",
        ",'fo.voucherbrandbusinessunit as voucher_bbusiness_unit'\n",
        "#,'round(nvl(fot.LineItemCreditCardCharge,0),3) - round(nvl(fot.cc_refund_amt,0),3) as line_item_credit_card_charge'\n",
        ",'round(nvl(fot.LineItemCreditCardCharge,0),3) as line_item_credit_card_charge'\n",
        "#subtracting voucher refund amt to adjust line_item_subsidy_charge\n",
        "#,'round(nvl(fot.LineItemSubsidyCharge,0),3) - round(nvl(fot.voucher_refund_amt,0),3) as line_item_subsidy_charge'\n",
        ",'round(nvl(fot.LineItemSubsidyCharge,0),3) as line_item_subsidy_charge'\n",
        "#,'case when fo.order_total > round(nvl(fot.LineItemSubsidyCharge,0),3) then round(nvl(fot.LineItemSubsidyCharge,0),3) else round(fo.order_total,3) end as balance_national_acc_owes'\n",
        "#subtracting voucher refunda mt to adjust balance_national_acc_owes\n",
        ",'round(nvl(fot.LineItemSubsidyCharge,0),3) - round(nvl(fot.voucher_refund_amt,0),3) as balance_national_acc_owes'\n",
        "#,'round(nvl(fot.LineItemSubsidyCharge,0),3) as balance_national_acc_owes'\n",
        ",'fo.ship_status as line_item_status'\n",
        ",'round(fo.quantity*fo.pre_tax_total,3) as subtotal'\n",
        ",'scodes.code as shipping_state_short'\n",
        ",'fo.size as size_original'\n",
        ",\"to_date(fo.ship_date,'yyyyMMdd') as ship_date\"\n",
        ",\"ship_charge\"\n",
        ",\"(fot.voucher_refund_amt * -1) as voucher_refund_amt\"\n",
        ",\"(fot.cc_refund_amt * - 1) as cc_refund_amt\"\n",
        ",\"fo.replacement_upc as replacement_upc\"\n",
        ",\"to_date(fo.return_exchange_date,'yyyyMMdd') as return_exchange_date\"\n",
        ",\"from_utc_timestamp(current_timestamp(),'EST5EDT') as last_update_ts\")\n",
        "dpcce_sfcc_fact_orders.createOrReplaceTempView('order_voucher_data')\n",
        "#display(dpcce_sfcc_fact_orders.select('*'))\n",
        "#display(dpcce_sfcc_fact_orders.where(\"sf.order_no IN ('T1080045391','T1080042899')\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "select * from (select * , row_number() over(partition by order_no,upc, employee_id, voucher_code,order_type order by order_date_time desc) rn \n",
        "from order_voucher_data) t where t.rn = 1;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## print duplicates from integrated(sfcc, fact.orders, voucher) data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "df_order_voucher_dups = spark.sql(f\"\"\"select * from (select * , row_number() over(partition by order_no, upc,employee_id, voucher_code,order_type order by order_date_time desc) rn \n",
        "from order_voucher_data) t where t.rn > 1\"\"\")\n",
        "display(df_order_voucher_dups)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## select one row per orderno+voucherno and empid "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#dpcce_sfcc_fact_orders = spark.sql(f\"\"\"select * from (select * , row_number() over(partition by order_no,upc, employee_id, voucher_code, order_type order by order_date_time desc) rn \n",
        "#from order_voucher_data) t where t.rn = 1\"\"\")\n",
        "#dpcce_sfcc_fact_orders = dpcce_sfcc_fact_orders.drop('rn')\n",
        "#display(dpcce_sfcc_fact_orders)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write to DataFrame out (df_out) and perform final transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#Change line item status for exchanges\n",
        "df_out = dpcce_sfcc_fact_orders.withColumn(\"line_item_status\", when(col(\"replacement_upc\").isNull(), col(\"line_item_status\")).otherwise(\"EXCHANGED\"))\n",
        "\n",
        "#Update aggregate amounts on returns\n",
        "df_out = df_out.withColumn(\"line_item_subsidy_charge\", when(col(\"order_type\") == \"O\", col(\"line_item_subsidy_charge\")).otherwise(\"0.00\").cast('decimal(12,2)'))\n",
        "df_out = df_out.withColumn(\"balance_national_acc_owes\", when(col(\"order_type\") == \"O\", col(\"balance_national_acc_owes\")).otherwise(\"0.00\").cast('decimal(12,2)'))\n",
        "df_out = df_out.withColumn(\"voucher_refund_amt\", when(col(\"order_type\") == \"O\", col(\"voucher_refund_amt\")).otherwise(\"0.00\").cast('decimal(12,2)'))\n",
        "df_out = df_out.withColumn(\"cc_refund_amt\", when(col(\"order_type\") == \"O\", col(\"cc_refund_amt\")).otherwise(\"0.00\").cast('decimal(12,2)'))\n",
        "df_out = df_out.withColumn(\"line_item_credit_card_charge\", when(col(\"order_type\") == \"O\", col(\"line_item_credit_card_charge\")).otherwise(\"0.00\").cast('decimal(12,2)'))\n",
        "df_out = df_out.withColumn(\"subtotal\",when(col(\"order_type\") == \"O\", col(\"subtotal\")).otherwise(col(\"subtotal\") * -1).cast('decimal(12,2)'))\n",
        "\n",
        "#Update line item amounts on return-exchange lines - we do not want to subtract these values as it causes false reporting - value should remain\n",
        "df_out = df_out.withColumn(\"product_qty\", when(concat(df_out[\"line_item_status\"], df_out[\"order_type\"]) == \"EXCHANGEDR\", 0).otherwise(col(\"product_qty\")))\n",
        "df_out = df_out.withColumn(\"line_item_price\", when(concat(df_out[\"line_item_status\"], df_out[\"order_type\"]) == \"EXCHANGEDR\", 0).otherwise(col(\"line_item_price\")))\n",
        "df_out = df_out.withColumn(\"line_item_total\", when(concat(df_out[\"line_item_status\"], df_out[\"order_type\"]) == \"EXCHANGEDR\", 0).otherwise(col(\"line_item_total\")))\n",
        "df_out = df_out.withColumn(\"sale_price\", when(concat(df_out[\"line_item_status\"], df_out[\"order_type\"]) == \"EXCHANGEDR\", 0).otherwise(col(\"sale_price\")))\n",
        "df_out = df_out.withColumn(\"subtotal\", when(concat(df_out[\"line_item_status\"], df_out[\"order_type\"]) == \"EXCHANGEDR\", 0).otherwise(col(\"subtotal\")))\n",
        "\n",
        "#Convert order_date_time from UTC to EST and overwrite order_date with this\n",
        "df_out = df_out.withColumn('order_date_time', F.from_utc_timestamp('order_date_time', 'America/New_York'))\n",
        "df_out = df_out.withColumn('order_date', F.date_format(col(\"order_date_time\"),\"yyyy-MM-dd\").cast(DateType()))\n",
        "\n",
        "#display(df_out.where(\"order_no IN ('T1080044631')\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write dataframes to parquet files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Delete yesterday's data and re-create new folder with current data\n",
        "if mssparkutils.fs.exists(gold_adls_path + 'b2b2c/order_history_data_full') == True:\n",
        "  mssparkutils.fs.rm(gold_adls_path + 'b2b2c/order_history_data_full',True)\n",
        "  df_out.write.parquet(gold_adls_path + 'b2b2c/order_history_data_full')\n",
        "else:\n",
        "  df_out.write.parquet(gold_adls_path + 'b2b2c/order_history_data_full')\n",
        "\n",
        "if mssparkutils.fs.exists(gold_adls_path + 'b2b2c/voucher_master_data_full') == True:\n",
        "  mssparkutils.fs.rm(gold_adls_path + 'b2b2c/voucher_master_data_full',True)\n",
        "  df_csv_voucher.write.parquet(gold_adls_path + 'b2b2c/voucher_master_data_full')\n",
        "else:\n",
        "  df_csv_voucher.write.parquet(gold_adls_path + 'b2b2c/voucher_master_data_full')\n",
        "\n",
        "#Legacy delta lake load - will maybe revisit in the future to work out de-dupes.\n",
        "#dict_tables = {\n",
        "#      \"b2b2c/order_voucher_data\": { \"source_df\": dpcce_sfcc_fact_orders\n",
        "#                 ,\"where_condition\":\"\"\"target.order_no = source.order_no \n",
        "#                  and target.upc = source.upc \n",
        "#                  and target.employee_id = source.employee_id \n",
        "#                  and target.voucher_code= source.voucher_code \n",
        "#                  and target.order_type = source.order_type\"\"\"                        \n",
        "#                 ,\"target_table\":\"lakedb_gold.b2b2c_order_history_data2\" \n",
        "#                 ,\"partition\":\"order_date\"\n",
        "#                 ,\"path\": gold_adls_path + 'b2b2c/order_history_data2'\n",
        "#                       \n",
        "#                   }\n",
        "#      ,\"b2b2c/voucher_master_data\": \n",
        "#                { \"source_df\": df_csv_voucher\n",
        "#                 ,\"where_condition\":\"target.voucher_code = source.voucher_code\"                        \n",
        "#                 ,\"target_table\":\"lakedb_gold.b2b2c_voucher_master_data2\" \n",
        "#                 ,\"partition\":\"\"\n",
        "#                 ,\"path\": gold_adls_path + 'b2b2c/voucher_master_data2'\n",
        "#                }\n",
        "#                            \n",
        "#               }\n",
        "#perform_merge(dict_tables)   \n",
        "               "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## exit the notebook - send an email about the processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#move the processed files to archive\n",
        "if  v_xml_file_cnt >=1:  \n",
        "  # for j in mssparkutils.fs.ls(f\"{raw_path}sfcc/\"):\n",
        "  #   if j.size>0:  \n",
        "  #     print(f'moving ', j.name, ' to archive' )\n",
        "  #     mssparkutils.fs.mv(f\"{raw_path}sfcc/{j.name}\", f\"{raw_path}sfcc/archive/{j.name}\",overwrite=True)\n",
        "  mssparkutils.notebook.exit(\"sfcc file\")  \n",
        "\n",
        "elif  v_xml_file_cnt <1:\n",
        "  mssparkutils.notebook.exit(\"None\")   \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# df_fact_order = spark.read.format('delta').load('abfss://bronze@azwwwnonproddevadapadls.dfs.core.windows.net/SAP/BW/ZSOM_D12')\n",
        "# (len(df_fact_order.columns))\n",
        "# cols1= ['YACTDATE', 'YAMOUNT', 'YARTICLE', 'YORGPLANT', 'YPLANT', 'YQTY', 'YREASON', 'YREGISTER', 'YRTLCOST', 'YSHIPCHRG', 'YSLSPRSN', 'YTAXAMT1', 'YTAXAMT2', 'ZAEDAT', 'ZCARRIER', 'ZCHGTIME', 'ZCOUPON', 'ZCOUPON2', 'ZCOUPON3', 'ZDEMANDLC', 'ZDISCMSG', 'ZDISCMSGP', 'ZFINALSAL', 'ZINVOICED', 'ZLOCALE', 'ZMATNUM', 'ZREC_TYPE', 'ZRPA_TCD', 'ZRTMSRP', 'ZSALETYPE', 'ZSDTRACK', 'ZSHOESZ', 'ZSHOEWD', 'ZSKU', 'ZSOMACTID', 'ZSOMBILCC', 'ZSOMBILCT', 'ZSOMBILPC', 'ZSOMBILST', 'ZSOMCSRNM', 'ZSOMDLVCC', 'ZSOMDLVCT', 'ZSOMDLVPC', 'ZSOMDLVST', 'ZSOMFSTAT', 'ZSOMFTIME', 'ZSOMGCORD', 'ZSOMGCTND', 'ZSOMHSTAT', 'ZSOMIACDT', 'ZSOMIACTM', 'ZSOMLINE', 'ZSOMODISC', 'ZSOMODLMD', 'ZSOMOISST', 'ZSOMOODT', 'ZSOMOORD', 'ZSOMOOTM', 'ZSOMOPTIN', 'ZSOMOSMNM', 'ZSOMOSTTL', 'ZSOMOTTL', 'ZSOMPDISC', 'ZSOMPYST', 'ZSOMRFST', 'ZSOMRTNDT', 'ZSOMSAPTD', 'ZSOMSAPTT', 'ZSOMSDISC', 'ZSOMSHPDT', 'ZSOMSLMD', 'ZSOMSTIME', 'ZSOMTYPE', 'ZSTOREID', 'ZUPCU', 'ZVATVAL', 'AEDAT', 'BASE_UOM', 'CALDAY', 'CALMONTH', 'CALMONTH2', 'CALWEEK', 'CALYEAR', 'CREA_TIME', 'CURRENCY', 'DOC_CURRCY', 'DOC_NUMBER', 'EMPLOYEE', 'ERDAT', 'FISCPER', 'FISCPER3', 'FISCVARNT', 'FISCYEAR', 'LOAD_DATE', 'LOC_CURRCY', 'RPA_DTC', 'RPA_REA', 'RPA_TAT', 'RPA_TCD', 'RPA_TTC', 'SR_COUNTER', 'TIME', 'ZEMPPURCH', 'ZPRODEAL', 'ZISMOBILE', 'ZISCUSTOM', 'ZCSRORD', 'ZPCIPAL', 'ZPREORDER', 'ZOUTLETF', 'ZINSTORE', 'ZSTORETYP', 'ZSOMSLSCH', 'ZISLEGACY', 'ZSOMCRDT', 'ZSOMVEMP', 'ZSOMVSTE', 'ZSOMVBBU', 'ZSOMVOUPY']\n",
        "# cols_renamed = ['ActualTransactionDate', 'UnitPrice', 'RetailProductCode', 'OriginatingStore', 'Site', 'Quantity', 'ReturnReason', 'Register', 'RetailCost', 'ShippingCharges', 'StoreSalesAssociate', 'TaxAmount1', 'TaxAmount2', 'ItemChangedDate', 'ShipmentCarrier', 'ItemChangedTime', 'ShipCouponCode', 'OrderCouponCode', 'ProductCouponCode', 'DemandLocalCurrency', 'OrderPromoMessage', 'ProductPromoMessage', 'FinalSale', 'Invoiced', 'Locale', 'StockNumber', 'RecordType', 'GiftCardPayIndicator', 'ListPrice', 'SaleType', 'TrackingNumber', 'ItemSize', 'ItemWidth', 'StockKeepingUnit', 'CustomerAccountId', 'BillToCountryCode', 'BillToCity', 'BillToPostalCode', 'BillToRegion', 'CSRWWWUserId', 'ShipToCountryCode', 'ShipToCity', 'ShipToPostalCode', 'ShipToRegion', 'FulfillmentStatus', 'FulfillOrderTime', 'GiftCardOrderIndicator', 'GiftCardTenderValue', 'OrderStatus', 'ItemAdjCreatedDate', 'ItemAdjCreatedTime', 'LineNumber', 'OrderDiscount', 'ShippingMethod', 'ItemStatus', 'OriginalOrderDate', 'OriginalOrderNumber', 'OriginalOrderTime', 'OptInFlag', 'OrderNumber', 'PreTaxTotal', 'GrandTotal', 'ProductDiscount', 'PaymentStatus', 'RefundStatus', 'ReturnDate', 'SAPTransactDate', 'SAPTransactTime', 'ShippingDiscount', 'ShipmentCreateDate', 'ShipLastModifyDate', 'ShipmentCreateTime', 'OrderType', 'CidStoreId', 'ProductCode', 'VATTax', 'OrderChangedDate', 'UnitOfMeasure', 'OrderedDate', 'CalendarYearMonth', 'CalendarMonth', 'OrderedWeek', 'CalendarYear', 'OrderCreatedTime', 'CurrencyKey', 'DocumentCurrency', 'SAPSalesOrder', 'EmployeeNumber', 'OrderCreatedDate', 'FiscalYearPeriod', 'FiscalPeriod', 'FiscalCalendar', 'FiscalYear', 'BWLoadDate', 'LocalCurrency', 'DiscountTypesFlag', 'TotalDiscount', 'TenderValue', 'MeansOfPayment', 'TransType', 'NumberOfTransactions', 'OrderedTime', 'EmployeePurchaseFlag', 'ProDealFlag', 'MobileOrderFlag', 'CustomOrderFlag', 'CsrOrderFlag', 'PciPalFlag', 'PreOrderFlag', 'OutletFlag', 'InStoreFlag', 'StoreType', 'SalesChannel', 'LegacyFlag', 'CancelReturnDate', 'VoucherEmployeeID', 'VoucherSiteID', 'VoucherBrandBusinessUnit', 'LineItemSubsidyCharge']\n",
        "# #cols_renamed_filter = [x for x in cols_renamed if x in []\n",
        "# cols_zip = list(zip(cols1,cols_renamed))\n",
        "# for j in cols_zip:\n",
        "#   print(j[0]+' as '+j[1])\n",
        "# # display(spark.sql(\"\"\"select * from parquet.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/SAP/BW/ZSOM_D12/2024/03/25/2024-03-25T17:59:31.9176378Z.parquet`\n",
        "# # where ZSOMOSMNM like 'T1080030340%'\"\"\"))  \n",
        ""
      ]
    }
  ],
  "metadata": {
    "description": "b2b2c process vouchers",
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}