{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Revision History\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Change_date         revision_number       change_description                                          author\r\n",
        "# 03/27/2023             1                  initial check-in                                            kranthi   \r\n",
        "# 04/03/2023             2                  remove 322, 324 script_ids                                  kranthi\r\n",
        "# 04/03/2023             3                  add Q1-Mapping and Q2-mapping for all scripts               kranthi\r\n",
        "# 09/19/2023             4                  extract data from Participants child node                   kranthi\r\n",
        "# 09/03/2025             5                  Removed subject_area pcs from processing\r\n",
        "#                                           New ingestion for pcs is in AS_NB_8x8_NewAPI-Ingestion      Peter Daniels (3cloud)\r\n",
        "# "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import datetime\r\n",
        "#import mssparkutils\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "import requests\r\n",
        "import json\r\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType, TimestampType, DateType\r\n",
        "import concurrent.futures\r\n",
        "import traceback\r\n",
        "import math\r\n",
        "import time\r\n",
        "import pyspark.sql.functions as F\r\n",
        "from pyspark.sql.functions import split\r\n",
        "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\",\"true\")\r\n",
        "spark.sql(\"SET spark.databricks.delta.schema.autoMerge.enabled = true\")\r\n",
        "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"true\")\r\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\r\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\r\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\r\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\",\"true\")\r\n",
        "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\",\"true\")\r\n",
        "spark.conf.set(\"spark.databricks.adaptive.autoOptimizeShuffle.enabled\",\"true\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print(mssparkutils.env.getWorkspaceName())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# import frequently used functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# import struct/config variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /contact_center_ops/config/config_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# functions to process API data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#add adls path here\r\n",
        "spark.sql(\"SET spark.databricks.delta.schema.autoMerge.enabled = true\")\r\n",
        "print(\"env_var::\", env_var)\r\n",
        "\r\n",
        "#dt = (datetime.datetime.now()).strftime('%Y-%m-%d')\r\n",
        "dt = (datetime.datetime.now()).strftime('%Y-%m-%d')\r\n",
        "dt_time = (datetime.datetime.now()).strftime('%Y-%m-%d %H:%M:%S')\r\n",
        "print(\"date of load::\", dt)\r\n",
        "dt_date_type = datetime.datetime.strptime(dt, '%Y-%m-%d')\r\n",
        "dt_folder_nm = (datetime.datetime.now()).strftime('%Y%m%d')\r\n",
        "print(\"dt_folder_nm::\",dt_folder_nm)\r\n",
        "\r\n",
        "dt_datetime_type = datetime.datetime.strptime(dt_time, '%Y-%m-%d %H:%M:%S')\r\n",
        "print(\"time of load::\", dt_datetime_type)\r\n",
        "\r\n",
        "\r\n",
        "def make_api_call(verb,headers,url,body={},params={}):\r\n",
        "    response = ''\r\n",
        "    #print(\"make_api_call::\",type(headers), verb,headers,url,\"body::\",body,\"params::\",params)\r\n",
        "    try:\r\n",
        "        if verb == 'get':\r\n",
        "            response = requests.get(url,headers=headers,params=params)\r\n",
        "        elif verb == 'post':\r\n",
        "            response = requests.post(url, json=body, headers=headers,params=params)\r\n",
        "        return response\r\n",
        "    except Exception as e:\r\n",
        "        print(\"error in make_api_call::::\",e)\r\n",
        "        time.sleep(10)\r\n",
        "        traceback.print_exc()\r\n",
        "        i = 5\r\n",
        "        while i <5:\r\n",
        "            make_api_call(verb,headers,url,body,params)\r\n",
        "            i=i+1\r\n",
        "        raise    \r\n",
        "\r\n",
        "#helps calculate the number of pages a particular script_id has\r\n",
        "def get_threads_to_be_used(url,json_data,headers):\r\n",
        "    res =  make_api_call('post',headers,url,json_data)\r\n",
        "    print(\"url::\", url,\"res::\",res)\r\n",
        "    if res is not None and res.status_code ==200:\r\n",
        "      rec_count = json.loads(res.text)['hits']\r\n",
        "      print(rec_count,' hits for script_id ',json_data['script'])\r\n",
        "      if rec_count>0:\r\n",
        "        thread_count = 1\r\n",
        "        if rec_count>500: \r\n",
        "          thread_count = math.ceil(rec_count/500)\r\n",
        "        return thread_count,rec_count\r\n",
        "      else :\r\n",
        "        return 0,0\r\n",
        "    else:\r\n",
        "      print(\"res for get_threads_to_be_used is Null::\", url, json_data,\"status_code::\", res.status_code) \r\n",
        "      return 0,0      \r\n",
        "\r\n",
        "\r\n",
        "q1_mapping = ['getdigit-d930a6f4-f5d8-44d5-8729-c3ab0c53aba4','getdigit-ea101383-8440-4019-aa39-d52f864a9371','getdigit-79fe9d37-51e9-4a69-8f72-79c3a09db853',\r\n",
        "'getdigit-0afff142-a242-42f0-910c-ae077f6d8a63','getdigit-cb1dba8c-7630-4ddc-9631-7f455c78ffb2','getdigit-ae648023-a2fd-4c6e-afa8-4e54cb3374bd'\r\n",
        ",'getdigit-f2254255-0a76-4274-a910-1d1e608deafc']\r\n",
        "q2_mapping = ['getdigit-b85fb4cc-2c33-4d52-83fb-6608ee944564','getdigit-e5843f02-ddf6-40be-8a2b-4c1499bb811f']\r\n",
        "\r\n",
        "def create_pcs_dataframe(data,hits,questionlabel,schema):\r\n",
        "  try:  \r\n",
        "    all_pcs_data = []  \r\n",
        "    for row in data:\r\n",
        "        row_dict = {}\r\n",
        "        row_keys = row.keys()\r\n",
        "        row_dict['hits']= int(hits)\r\n",
        "        q1s = 'None'\r\n",
        "        q2s = 'None'\r\n",
        "        q1l = []\r\n",
        "        q2l = []\r\n",
        "        q1l = [row[q1_keys] for q1_keys in row_keys if q1_keys in q1_mapping]\r\n",
        "        q1s= ''.join(q1l)\r\n",
        "        q2l = [row[q2_keys] for q2_keys in row_keys if q2_keys in q2_mapping]\r\n",
        "        q2s= ''.join(q2l)\r\n",
        "        #print(\"q1s::\",q1s,\"q2s::\",q2s)\r\n",
        "        row_dict['data'] = {\r\n",
        "        'callId':row['callId'],\r\n",
        "        'callDate': row['callDate'],\r\n",
        "        'callerName': row['callerName'],\r\n",
        "        'callerPhoneNumber': row['callerPhoneNumber'],\r\n",
        "        'question1': q1s,\r\n",
        "        'question2': q2s,\r\n",
        "        'totalScore': row['totalScore'],\r\n",
        "        'agentList': row['agentList'],\r\n",
        "        'queueList' : row['queueList'],\r\n",
        "        'transactionId': row['transactionId'],\r\n",
        "        'agentCallHandlingDuration':row['agentCallHandlingDuration'],\r\n",
        "        'holdDuration':row['holdDuration'],\r\n",
        "        'muteDuration':row['muteDuration'],\r\n",
        "        'timeInIVR':row['timeInIVR'],\r\n",
        "        'waitTime':row['waitTime'],\r\n",
        "        'callDuration':row['callDuration']\r\n",
        "        #'loadDate': dt_date_type \r\n",
        "         }\r\n",
        "        row_dict['QuestionLabel'] = questionlabel\r\n",
        "        row_dict['loadDate'] = dt_date_type\r\n",
        "        row_dict['loadDateTime'] = dt_datetime_type\r\n",
        "        #print(\"row_dict::\",row_dict)\r\n",
        "        all_pcs_data.append(row_dict)\r\n",
        "   # try:\r\n",
        "    df = spark.createDataFrame(data=all_pcs_data,schema=schema)\r\n",
        "    #display(df)\r\n",
        "    return df\r\n",
        "  except Exception as e:\r\n",
        "        print(\"error in cds::::\",e)\r\n",
        "        traceback.print_exc()\r\n",
        "        raise\r\n",
        "        #print(op2,'\\n\\n')\r\n",
        "    \r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "        \r\n",
        "def pcs_api_call(inp):\r\n",
        "    print(\"in pcs_api_call::\",inp)\r\n",
        "    api_call_bdy = inp[0]\r\n",
        "    verb = inp[1]\r\n",
        "    url= inp[2]\r\n",
        "    headers = inp[3]\r\n",
        "    pcs_folder = inp[4]\r\n",
        "    try:\r\n",
        "        res = make_api_call(verb,headers,url,api_call_bdy)\r\n",
        "        #requests.post(url, json=api_call_bdy, headers=headers)\r\n",
        "        r = json.loads(res.text)\r\n",
        "        if r is not None:\r\n",
        "          data = r['data']\r\n",
        "        #if len(data)!=0:\r\n",
        "        if r['hits'] >0:\r\n",
        "            #all_pcs_data.append(pcs_res)\r\n",
        "            print(\"hits before pcs\",r['hits'])  \r\n",
        "            #create_pcs_dataframe(data,r['hits'],r['questionLabel'],pcs_schema)\r\n",
        "            df = create_pcs_dataframe(data,r['hits'],r['questionLabel'],pcs_schema)\r\n",
        "            #print(df.count())\r\n",
        "            write_to_file(df,raw_adls_path+pcs_folder)\r\n",
        "    except Exception as e:\r\n",
        "        print(\"error in pcs api::::\",e)\r\n",
        "        traceback.print_exc()\r\n",
        "        raise\r\n",
        "\r\n",
        "\r\n",
        "def fetch_config(subject_area):\r\n",
        "    vals = spark.sql(f\"select * from rest_api_config where subject_area = '{subject_area}' and is_active = '1'\")\r\n",
        "    verb = vals.select('verb').head()[0]\r\n",
        "    url = vals.select('url').head()[0]\r\n",
        "    headers = vals.select('headers').head()[0]\r\n",
        "    body = vals.select('body').head()[0]\r\n",
        "    params = vals.select('params').head()[0]\r\n",
        "    folder_in_adls =  vals.select('folder_in_adls').head()[0]\r\n",
        "    return verb,url,headers,body,params, folder_in_adls\r\n",
        "    \r\n",
        "def retrieve_access_token(headers,url,verb,body={}): #,params={},params = params,\r\n",
        "    try:\r\n",
        "        print(\"in retrieve access tokens..\")\r\n",
        "        response = requests.post(url, headers=headers, data=body, verify=False) \r\n",
        "        return response\r\n",
        "    except Exception as e:\r\n",
        "        print(\"error::::\",str(e))\r\n",
        "        traceback.print_exc()\r\n",
        "\r\n",
        "def create_report(access_token,headers,body,url,verb):\r\n",
        "    try:\r\n",
        "        Previous_Date = str((datetime.datetime.today() - datetime.timedelta(days=1)).strftime ('%Y-%m-%d'))\r\n",
        "        print(\"create_report::Previous_Date::\", Previous_Date)\r\n",
        "        print(\"create_report::headers::\",eval(headers), \"url::\",url,\"body::\", eval(body) )\r\n",
        "        response2 = make_api_call(verb,eval(headers),url,eval(body))\r\n",
        "        #requests.post(url, headers=eval(headers), json=eval(body))\r\n",
        "        report_id = response2.json()['id']\r\n",
        "        report_status = response2.json()['status']\r\n",
        "        print(\"create_report::report_id::status::\",report_id,\"::=====\",report_status)\r\n",
        "        return report_id\r\n",
        "    except Exception as e:\r\n",
        "        print(\"error in create_report::::\",e)\r\n",
        "        traceback.print_exc()\r\n",
        "        \r\n",
        "\r\n",
        "\r\n",
        "def access_report_data(access_token,url,headers,report_id,is_first_page,verb,last_doc_id,hist_folder):\r\n",
        "    try:\r\n",
        "        print(\"\\nin access_report_data::access_token::\",access_token,\"url::\",url,\"headers::\",headers,\"report_id::\",report_id,\"is_first_page::\",is_first_page,\"verb::\",verb,\"last_doc_id::\",last_doc_id)\r\n",
        "        if is_first_page==True:\r\n",
        "            url = url+f'/{report_id}/data?size=1000'\r\n",
        "        else:\r\n",
        "            url = url+f'/{report_id}/data?size=1000&lastDocumentId={last_doc_id}'\r\n",
        "        print(\"\\nurl::\", url,\"headers::\",headers)\r\n",
        "        response3 = make_api_call(verb,headers,url,{},{})\r\n",
        "        print(\"----response3--------------\")\r\n",
        "        #print(\"\\naccess_report_data::response3.headers::\", response3.headers)\r\n",
        "\r\n",
        "        #print(\"\\naccess_report_data::complete response3::\", response3.json())\r\n",
        "        #print(\"response3 headers.Last-Document-ID::\",response3.headers['Last-Document-ID'])\r\n",
        "        #print(\"response3 headers.X-Total-Pages::\",response3.headers['X-Total-Pages'])\r\n",
        "        #res = response3.json()['items']\r\n",
        "        res = json.loads(response3.text)\r\n",
        "        print(\"type of res:\", type(res), \"len of res:\",len(res))\r\n",
        "        #print(\"\\n\\nreslllll::\",type(res), len(res),\"res::\" ,res)\r\n",
        "        if len(res[0]['items'])!=0:\r\n",
        "            pivot1 = []\r\n",
        "            pivot2 = {}\r\n",
        "            pivot3 = []\r\n",
        "            for k in res:\r\n",
        "              #print(\"kkkk::\",k)\r\n",
        "              pivot1 = [{j['key']:j['value'] }for j in k['items'] if j['key'] not in 'participants']\r\n",
        "              #print(\"\\n####pivot1::\", len(pivot1))\r\n",
        "           \r\n",
        "            #print(\"after pivot1\")\r\n",
        "         \r\n",
        "              pivot2 = {k: str(v) for d in pivot1 for k, v in d.items()}\r\n",
        "              #pivot3 = []\r\n",
        "             \r\n",
        "\r\n",
        "              pivot1_particip = [{j['key']:j['value'] }for j in k['items'] if j['key'] in ['participants']]\r\n",
        "              #if pivot1_particip is not None:\r\n",
        "                #print(\"\\n####pivot1_pa::\",pivot1_particip)\r\n",
        "              v9 = [] \r\n",
        "              v55 = {}\r\n",
        "              # pivot1_particip is a list of dictionaries \r\n",
        "              for d in pivot1_particip:\r\n",
        "                #print('####1pivot1_particip',d)\r\n",
        "                for k, v in d.items():\r\n",
        "                    #print('####2',k,v)\r\n",
        "                    for parv in v:\r\n",
        "                      #print('####3',parv)\r\n",
        "                      v55 = {}\r\n",
        "                      v55 = {k1:str(v1) for k1,v1 in parv.items() if k1 in ['participantAssignNumber','participantId','participantName','participantOfferAction']}\r\n",
        "                      \r\n",
        "                      if v55: ## append this dict to list\r\n",
        "                        v9.append(v55)\r\n",
        "                        #print('####appending ParticipToList',v9)\r\n",
        "              pivot2['participants'] = str(v9)  ## creating a key in dictionary and value is list of dictionary\r\n",
        "                          #print('pppcc::',pivot2['participants'])       \r\n",
        "              \r\n",
        "              #if 'participants' in pivot2.keys(): \r\n",
        "                  #print(\"####pivot2::\", pivot2['participants'])\r\n",
        "              pivot3.append(pivot2) ## list of dictionaries\r\n",
        "              #if pivot1_particip:\r\n",
        "                  #print(\"####pivot3::\",pivot3)\r\n",
        "                                      \r\n",
        "           \r\n",
        "            #print(\"pivot3 len::\",len(pivot3))\r\n",
        "            df = spark.createDataFrame(data=pivot3,schema=rpt_schema)\r\n",
        "            #print(\"rpt_schema::\",rpt_schema)\r\n",
        "            df = df.withColumn(\"loadDate\", F.lit(dt_date_type))\\\r\n",
        "                   .withColumn(\"loadDateTime\",F.lit(dt_datetime_type))\r\n",
        "            #print(\"after create df\")\r\n",
        "            #mssparkutils.notebook.exit(\"Done\") \r\n",
        "            print(\"====count of HA rows writing to file::====\",df.count())\r\n",
        "            write_to_file(df,raw_adls_path+hist_folder+'/incremental/'+dt_folder_nm)\r\n",
        "            #print(\"after create df\")\r\n",
        "        if 'Last-Document-ID' in response3.headers.keys():\r\n",
        "            print(\"=====last_doc_id in access_report_data::===== \",response3.headers['Last-Document-ID'])\r\n",
        "            return response3.headers['Last-Document-ID'] \r\n",
        "        else:\r\n",
        "            print(\"no Last-Document-ID\")\r\n",
        "            return None      \r\n",
        "    except Exception as e:\r\n",
        "        print(\"error in access_report_data::::\",str(e))\r\n",
        "        traceback.print_exc()\r\n",
        "        raise\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        " %%sql\r\n",
        " select 1\r\n",
        "-- ALTER TABLE delta.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/historical_analytics/incremental/` SET TBLPROPERTIES (\r\n",
        "--    'delta.columnMapping.mode' = 'name',\r\n",
        "--    'delta.minReaderVersion' = '2',\r\n",
        "--    'delta.minWriterVersion' = '5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# MAIN ( )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def main_call(subject_area):\r\n",
        "  try:  \r\n",
        "    if subject_area == 'pcs':\r\n",
        "        # As of 2025-08-25, this should no longer get called.  Only the second section (the elif) will.\r\n",
        "        print(\"subject_area1::\",subject_area)\r\n",
        "        #add all survey script id to be run to this list\r\n",
        "        #survey_script_id = [69,70,301,304,322,303,321,305,324]\r\n",
        "        verb,vurl_original,headers,body,params,folder = fetch_config(subject_area)\r\n",
        "        print(\"config::\", verb,vurl_original,headers,body)\r\n",
        "        headers = json.loads(headers)\r\n",
        "        body = json.loads(body)\r\n",
        "        dict_pcs = {'us': {'regions':[69,70,301,304,303,321,305], 'timezone': 'America/New_York'}\r\n",
        "            #,'uk': {'regions': [301,304,322,303,321,305,324], 'timezone':'Europe/London'}\r\n",
        "            }\r\n",
        "        api_calls_list = []\r\n",
        "        for key1, val in dict_pcs.items():\r\n",
        "          region = key1  \r\n",
        "          print(\"region:\",region, \"vurl original::\",vurl_original)\r\n",
        "          vurl = vurl_original.format(region=region)\r\n",
        "          print(\"region:\",region, \"vurl substituted::\",vurl)\r\n",
        "          script_ids = val['regions']\r\n",
        "          timezone = val['timezone']\r\n",
        "          for script_id in script_ids:\r\n",
        "              #print(\"script_id:\",script_id, \"body:\",body)\r\n",
        "              body['script']= [script_id]\r\n",
        "              body['timezone'] = timezone\r\n",
        "              print(\"script_id::\",script_id,\"url:\",vurl,\"headers:\",headers,\"body:\",body)  \r\n",
        "              page_count,hits = get_threads_to_be_used(vurl,body,headers)\r\n",
        "              if page_count >=1:\r\n",
        "                for pg in range(1,page_count+1):\r\n",
        "                    json_data2 = body.copy()\r\n",
        "                    json_data2['page']= pg\r\n",
        "                    print(\"scriptid::\",script_id, \"::page::\",pg)\r\n",
        "                    api_calls_list.append((json_data2,verb,vurl,headers,folder))\r\n",
        "        \r\n",
        "        #print('11')  \r\n",
        "        #with concurrent.futures.ThreadPoolExecutor(max_workers=len(api_calls_list)) as executor:\r\n",
        "            #executor.map(pcs_api_call,api_calls_list)\r\n",
        "        for i in api_calls_list:\r\n",
        "          pcs_api_call(i)\r\n",
        "        # pcs_flat_list = []\r\n",
        "        # #remove null lists\r\n",
        "        # all_pcs_data1 = [ele for ele in all_pcs_data if ele != []]\r\n",
        "        # pcs_flat_list = list(map(pcs_flat_list.extend, all_pcs_data1))\r\n",
        "        # print(\"pcs data::\",len(pcs_flat_list)) \r\n",
        "        # print(\"after flat list\")\r\n",
        "        #df = spark.createDataFrame(pcs_flat_list,pcs_schema)\r\n",
        "        print(\"after create pcs df\")\r\n",
        "        #display(df)\r\n",
        "        #print(df.count())\r\n",
        "        print(\"after create pcs df2\")\r\n",
        "        #write_to_file(df,'pcs/pcs') \r\n",
        "    elif subject_area == 'historical_analytics':\r\n",
        "        print(\"subject_area2::\",subject_area)\r\n",
        "        verb2,url2,headers2,body2,params,folder = fetch_config(subject_area)\r\n",
        "        access_token = \"\"\r\n",
        "        issued_at = \"\"\r\n",
        "        expires_in = \"\"\r\n",
        "        is_first_page = True\r\n",
        "        response = retrieve_access_token(json.loads(headers2),url2,verb2,json.loads(body2))\r\n",
        "        if response.status_code != 200:\r\n",
        "            raise Exception(\"{} - {}\".format(response.status_code, response.text))\r\n",
        "            time.sleep(10)\r\n",
        "        else:\r\n",
        "            response_dict = json.loads(response.text)\r\n",
        "            access_token = response_dict['access_token']\r\n",
        "            issued_at = response_dict['issued_at']\r\n",
        "            expires_in = response_dict['expires_in']\r\n",
        "            #expires_in = 1200\r\n",
        "            print(\"token expires in---\",expires_in)\r\n",
        "            token_issuetime = datetime.datetime.now()\r\n",
        "            verb3,url3,headers3,body3,params3,folder = fetch_config('create_report')\r\n",
        "            report_id = create_report(access_token,headers3,body3,url3,verb3)\r\n",
        "            verb4,url4,headers4,body4,params4,folder = fetch_config('access_report')\r\n",
        "\r\n",
        "            \r\n",
        "            if is_first_page == True:\r\n",
        "              last_doc_id = access_report_data(access_token,url4,eval(headers4),report_id,is_first_page,verb4,'',folder)\r\n",
        "              is_first_page = False\r\n",
        "              \r\n",
        "              while last_doc_id is not None:\r\n",
        "                 print(\"=========last_doc_id::======== \",last_doc_id)\r\n",
        "                 if (datetime.datetime.now()-token_issuetime).seconds >= int(expires_in)-180:\r\n",
        "                    print(\"======================*****token expired******===============================\")\r\n",
        "                    verb2,url2,headers2,body2,params,folder = fetch_config(subject_area)\r\n",
        "                    #print(\"hee::\",headers2, \"url2::\",url2,\"v2::\",verb2,\"b2::\",body2 )\r\n",
        "                    #def retrieve_access_token(headers,url,verb,body={},params={}):\r\n",
        "                    response = retrieve_access_token(json.loads(headers2),url2,verb2,json.loads(body2)) \r\n",
        "                    #print('dddffee')\r\n",
        "                    response_dict = json.loads(response.text)\r\n",
        "                    access_token = response_dict['access_token']\r\n",
        "                    issued_at = response_dict['issued_at']\r\n",
        "                    expires_in = response_dict['expires_in']\r\n",
        "                    print(\"token expires in---\",expires_in,\"access_token::\", access_token)\r\n",
        "                    token_issuetime = datetime.datetime.now()\r\n",
        "                    #verb3,url3,headers3,body3,params3,folder = fetch_config('create_report')\r\n",
        "                    #report_id = create_report(access_token,headers3,body3,url3,verb3)\r\n",
        "                    #verb4,url4,headers4,body4,params4,folder = fetch_config('access_report') \r\n",
        "                 last_doc_id = access_report_data(access_token,url4,eval(headers4),report_id,is_first_page,verb4,last_doc_id,folder)\r\n",
        "                 \r\n",
        "  except Exception as e:\r\n",
        "    print(\"error in main()::::\",str(e))\r\n",
        "    traceback.print_exc()\r\n",
        "    raise\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "         \r\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## call both PCS and Historical Analytics API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Removed pcs from here on 2025-08-25 to process with new API in a different notebook\r\n",
        "subject_areas = ['historical_analytics']#,'pcs']\r\n",
        "#subject_areas = ['historical_analytics']\r\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=len(subject_areas)) as executor:    \r\n",
        "    executor.map(main_call,subject_areas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#mssparkutils.fs.rm('abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/historical_analytics/incremental/20230921',True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# -- %%sql\r\n",
        "# -- select distinct participants\r\n",
        "# -- --participantAssignNumber,participantId,participantName,participantOfferAction\r\n",
        "# --   from delta.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/historical_analytics/incremental/20230926`\r\n",
        "# # -- where participants is not null or (participants) != '[]';"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# get_ccrt_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "queue_schema = StructType([StructField('id', StringType(), True),\r\n",
        "                           StructField('name', StringType(),True),\r\n",
        "                         \r\n",
        "                                     ]\r\n",
        "                                     \r\n",
        "                                     )\r\n",
        "                                    \r\n",
        "def get_ccrt_data(p_url,p_headers,p_verb,p_params,p_folder):\r\n",
        "    print('in get_ccrt_data::',p_folder, \"url::\",p_url)\r\n",
        "    \r\n",
        "    \r\n",
        "    print(p_url,p_headers,p_verb,{},p_params)\r\n",
        "    queue_res = make_api_call(p_verb,p_headers,p_url,{},p_params)\r\n",
        "    print(\"queue_res::\", queue_res.status_code)\r\n",
        "    if queue_res.status_code ==200 :\r\n",
        "      print(\"headers:\",queue_res.headers)\r\n",
        "      queue_row = json.loads(queue_res.text)\r\n",
        "      #print(\"queue_row::\",queue_row )\r\n",
        "      for item in queue_row: \r\n",
        "            item.pop('metrics')\r\n",
        "        #print(\"agent page 0:\",agent_res)  \r\n",
        "      all_queues.append(queue_row)  \r\n",
        "      #df = spark.createDataFrame(queue_row,queue_schema)\r\n",
        "      #write_to_file(df,p_folder,'N')\r\n",
        "      page_count = int(queue_res.headers['X-Total-Pages'])\r\n",
        "      print(\"page_count for \",p_folder,\" ::\",page_count)\r\n",
        "      if page_count >1:\r\n",
        "        for pg in range(1,page_count):\r\n",
        "          queue_params2 = p_params.copy()\r\n",
        "          queue_params2['page']= pg\r\n",
        "          queue_res = make_api_call(p_verb,p_headers, p_url,{},queue_params2)\r\n",
        "          queue_row = json.loads(queue_res.text)\r\n",
        "          #make_api_call(verb4,headers_dict,url4,{},json.loads(params4))\r\n",
        "          #all_queues.append(queue_row)\r\n",
        "          print(\"appending data to list::\",p_folder, \"::page::\",pg,\"elements in this page::\",len(queue_row))\r\n",
        "          #print(\"data in this page::\", queue_row)\r\n",
        "          #df = spark.createDataFrame(queue_row,queue_schema)\r\n",
        "          #write_to_file(df,p_folder,'N')\r\n",
        "          for item in queue_row: \r\n",
        "            item.pop('metrics')\r\n",
        "            #print(\"agent page 0:\",agent_res)  \r\n",
        "            all_queues.append(queue_row) \r\n",
        "    #return all_queues      \r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# call queue and group_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "\r\n",
        "#def ccrt_subject_processing(p_subject_area):\r\n",
        "  #        ccrt_subject = p_subject_area\r\n",
        "all_queues = [] \r\n",
        "subject_area = ['queue','group_id']\r\n",
        "#subject_area = ['queue']\r\n",
        "for ccrt_subject in subject_area:\r\n",
        "          all_queues = [] \r\n",
        "          print(\"subject_area2::\",ccrt_subject)\r\n",
        "          verb3,url3,headers3,body3,params3,p_folder = fetch_config('ccrt')\r\n",
        "          access_token = \"\"\r\n",
        "          issued_at = \"\"\r\n",
        "          expires_in = \"\"\r\n",
        "          is_first_page = True\r\n",
        "          #print(verb3,url3,headers3,body3,params3)\r\n",
        "          response = retrieve_access_token(headers=json.loads(headers3),url=url3,verb=verb3,body=json.loads(body3))\r\n",
        "          print(response)\r\n",
        "          if response.status_code != 200:\r\n",
        "              raise Exception(\"{} - {}\".format(response.status_code, response.text))\r\n",
        "              time.sleep(10)\r\n",
        "          else:\r\n",
        "              print('ccrt_subject response.status_code is 200')\r\n",
        "              \r\n",
        "              verb4,url4,headers4,body4,params4,p_folder = fetch_config(ccrt_subject)\r\n",
        "             \r\n",
        "              response_dict = json.loads(response.text)\r\n",
        "              queue_token = response_dict['access_token']\r\n",
        "              print(\"queue_token::\",queue_token)\r\n",
        "              issued_at = response_dict['issued_at']\r\n",
        "              expires_in = response_dict['expires_in']\r\n",
        "              #print(\"---\",expires_in)\r\n",
        "            \r\n",
        "              #print(\"sss::\",ccrt_subject, verb4,url4,headers4,body4,params4,p_folder)\r\n",
        "            \r\n",
        "              headers_dict = json.loads(json.dumps(eval(headers4))) \r\n",
        "              #if p_subject_area == 'agents': \r\n",
        "                #url4 = eval(url4)     \r\n",
        "              get_ccrt_data(url4,headers_dict,verb4,json.loads(params4),p_folder)\r\n",
        "              #print(\"after appending ccrt data to list, length of list::\", len(all_queues))\r\n",
        "              flat_list = []\r\n",
        "              flat_list = [item for sublist in all_queues for item in sublist]\r\n",
        "              #all_queues1 = [ele for ele in all_queues if len(ele) >0]\r\n",
        "              #single_list = list(map(flat_list.extend, all_queues))\r\n",
        "              #print(len(single_list))\r\n",
        "              #print(\"flat_list::\",flat_list)              \r\n",
        "              df = spark.createDataFrame(flat_list,queue_schema)\r\n",
        "              print(\"df count::\",df.distinct().count())\r\n",
        "              df = df.withColumn(\"lastUpdateDate\",F.lit(dt_datetime_type))  \r\n",
        "              write_to_file(df,raw_adls_path+p_folder) \r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# def agent_api_call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "all_agent_data = []\r\n",
        "\r\n",
        "def agent_api_call(inp):\r\n",
        "    print(\"in agent_api_call::\",inp)\r\n",
        "    api_params = inp[0]\r\n",
        "    verb = inp[1]\r\n",
        "    url= inp[2]\r\n",
        "    headers = inp[3]\r\n",
        "    gr_id =inp[4]\r\n",
        "    \r\n",
        "    try:\r\n",
        "        res = make_api_call(verb,headers,url,{},api_params)\r\n",
        "        #requests.post(url, json=api_call_bdy, headers=headers)\r\n",
        "        print(\"res::\",res)\r\n",
        "        agent_res = json.loads(res.text)\r\n",
        "        page_count = int(res.headers['X-Total-Pages'])\r\n",
        "       # print(\"ffff:\",r)\r\n",
        "        for item in agent_res: \r\n",
        "            item['group_id']=gr_id\r\n",
        "            item.pop('metrics')\r\n",
        "        #print(\"agent page 0:\",agent_res)  \r\n",
        "        all_agent_data.append(agent_res)        \r\n",
        "        if page_count >1:\r\n",
        "            for pg in range(1,page_count):\r\n",
        "                agent_params2 = api_params.copy()\r\n",
        "                agent_params2['page']= pg\r\n",
        "                agent_res = make_api_call(verb,headers,url,{},agent_params2)\r\n",
        "                agent_res = json.loads(agent_res.text)\r\n",
        "                # add group_id\r\n",
        "                for item in agent_res: \r\n",
        "                  item['group_id']=gr_id\r\n",
        "                  item.pop('metrics')\r\n",
        "                #print(\"agent page#:\",pg,agent_res)  \r\n",
        "                all_agent_data.append(agent_res) \r\n",
        "    except Exception as e:\r\n",
        "      print(\"error::::\",e)\r\n",
        "      traceback.print_exc()\r\n",
        "      raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# call agent api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "verb3,url3,headers3,body3,params3,p_folder = fetch_config('ccrt')\r\n",
        "access_token = \"\"\r\n",
        "issued_at = \"\"\r\n",
        "expires_in = \"\"\r\n",
        "is_first_page = True\r\n",
        "#print(verb3,url3,headers3,body3,params3)\r\n",
        "response = retrieve_access_token(headers=json.loads(headers3),url=url3,verb=verb3,body=json.loads(body3))\r\n",
        "print(response)\r\n",
        "if response.status_code != 200:\r\n",
        "    raise Exception(\"{} - {}\".format(response.status_code, response.text))\r\n",
        "    time.sleep(10)\r\n",
        "else:\r\n",
        "    print('ccrt_subject response.status_code is 200')\r\n",
        "    verb4,url4,headers4,body4,params4,agent_folder = fetch_config('agents')\r\n",
        "    \r\n",
        "    response_dict = json.loads(response.text)\r\n",
        "    queue_token = response_dict['access_token']\r\n",
        "    print(\"queue_token::\",queue_token)\r\n",
        "    issued_at = response_dict['issued_at']\r\n",
        "    expires_in = response_dict['expires_in']\r\n",
        "    #print(\"---\",expires_in)\r\n",
        "\r\n",
        "    #print(\"sss::\",ccrt_subject, verb4,url4,headers4,body4,params4,p_folder)\r\n",
        "\r\n",
        "    headers_dict = json.loads(json.dumps(eval(headers4))) \r\n",
        "    #url4 = eval(url4)     \r\n",
        "#    queue_data = get_ccrt_data(url4,headers_dict,verb4,json.loads(params4),p_folder)\r\n",
        "all_agent_api = []\r\n",
        "#del_folder(p_folder)\r\n",
        "df = spark.read.load(raw_adls_path+\"pcs/group_id\",format = \"delta\")\r\n",
        "all_group_ids = df.select(\"id\").distinct().collect()\r\n",
        "for j in all_group_ids:\r\n",
        "  #print(j.id)\r\n",
        "  group_id = j.id\r\n",
        "  all_agent_api.append((json.loads(params4),verb4,eval(url4),headers_dict, group_id))\r\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=len(all_agent_api)) as executor:\r\n",
        "  executor.map(agent_api_call,all_agent_api)\r\n",
        "\r\n",
        "## convert list of list to a single list  \r\n",
        "print(len(all_agent_data))\r\n",
        "flat_list = []\r\n",
        "flat_list = [item for sublist in all_agent_data for item in sublist]\r\n",
        "print(len(flat_list))  \r\n",
        "df = spark.createDataFrame(flat_list,queue_schema)\r\n",
        "df = df.withColumn(\"lastUpdateDate\",F.lit(dt_datetime_type))\r\n",
        "write_to_file(df,raw_adls_path+agent_folder) \r\n",
        "  \r\n",
        "  \r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print(all_group_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "\r\n",
        "\r\n",
        "# from pyspark.sql import SparkSession\r\n",
        "# from pyspark.sql.functions import from_json, col, collect_list\r\n",
        "# from pyspark.sql.types import StructType, StructField, StringType, ArrayType\r\n",
        "\r\n",
        "# # Initialize a Spark session\r\n",
        "# #spark = SparkSession.builder.appName(\"ConvertToListOfNames\").getOrCreate()\r\n",
        "\r\n",
        "# # Sample string containing a list of dictionaries\r\n",
        "# # data = \"\"\"[{'participantAssignNumber': 1\r\n",
        "# # , 'participantType': 'Agent'\r\n",
        "# # , 'participantId': 'ag9K9PcuAlSdq8NBG6aQBAdg'\r\n",
        "# # , 'participantName': 'Jennifer Gray'\r\n",
        "# # , 'participantOfferTime': '2023-09-06T16:47:45.091-04:00'\r\n",
        "# # , 'participantOfferAction': 'OfferTimeout'\r\n",
        "# # , 'participantOfferActionTime': '2023-09-06T16:48:00.096-04:00'\r\n",
        "# # , 'participantOfferDuration': {'value': 15005, 'ongoing': False}\r\n",
        "# # , 'participantHandlingEndTime': None\r\n",
        "# # , 'participantHandlingDuration': None\r\n",
        "# # , 'participantWrapUpEndTime': None\r\n",
        "# # , 'participantWrapUpDuration': None\r\n",
        "# # , 'participantProcessingDuration': None\r\n",
        "# # , 'participantBusyDuration': {'value': 15005, 'ongoing': False}\r\n",
        "# # , 'warmTransfersCompleted': 0\r\n",
        "# # , 'blindTransferToAgent': 0\r\n",
        "# # , 'blindTransferToQueue': 0\r\n",
        "# # , 'consultationsEstablished': 0\r\n",
        "# # , 'conferencesEstablished': 0\r\n",
        "# # , 'participantHold': None\r\n",
        "# # , 'participantHoldDuration': None\r\n",
        "# # , 'participantLongestHoldDuration': None\r\n",
        "# # , 'participantMute': None\r\n",
        "# # , 'participantMuteDuration': None\r\n",
        "# # , 'participantLongestMuteDuration': None\r\n",
        "# # , 'transactionCodeListId': []\r\n",
        "# # , 'transactionCodeListName': []\r\n",
        "# # , 'transactionCodeItemId': []\r\n",
        "# # , 'transactionCodeItemText': []\r\n",
        "# # , 'transactionCodeItemReportText': []\r\n",
        "# # , 'transactionCodeItemShortCode': []}\r\n",
        "# # , {'participantAssignNumber': 2\r\n",
        "# # , 'participantType': 'Agent'\r\n",
        "# # , 'participantId': 'agwXpAfDwtQLiuiKbE3SinKQ'\r\n",
        "# # , 'participantName': 'Lanardia Paschal'\r\n",
        "# # , 'participantOfferTime': '2023-09-06T16:48:01.961-04:00'\r\n",
        "# # , 'participantOfferAction': 'Accepted'\r\n",
        "# # , 'participantOfferActionTime': '2023-09-06T16:48:07.552-04:00'\r\n",
        "# # , 'participantOfferDuration': {'value': 5591, 'ongoing': False}\r\n",
        "# # , 'participantHandlingEndTime': '2023-09-06T16:55:17.624-04:00'\r\n",
        "# # , 'participantHandlingDuration': {'value': 430072, 'ongoing': False}\r\n",
        "# # , 'participantWrapUpEndTime': '2023-09-06T16:55:19.331-04:00'\r\n",
        "# # , 'participantWrapUpDuration': {'value': 1707, 'ongoing': False}\r\n",
        "# # , 'participantProcessingDuration': {'value': 431779, 'ongoing': False}\r\n",
        "# # , 'participantBusyDuration': {'value': 437370, 'ongoing': False}\r\n",
        "# # , 'warmTransfersCompleted': 0\r\n",
        "# # , 'blindTransferToAgent': 0\r\n",
        "# # , 'blindTransferToQueue': 0\r\n",
        "# # , 'consultationsEstablished': 0\r\n",
        "# # , 'conferencesEstablished': 0\r\n",
        "# # , 'participantHold': None\r\n",
        "# # , 'participantHoldDuration': None\r\n",
        "# # , 'participantLongestHoldDuration': None\r\n",
        "# # , 'participantMute': None\r\n",
        "# # , 'participantMuteDuration': None\r\n",
        "# # , 'participantLongestMuteDuration': None\r\n",
        "# # , 'transactionCodeListId': []\r\n",
        "# # , 'transactionCodeListName': []\r\n",
        "# # , 'transactionCodeItemId': []\r\n",
        "# # , 'transactionCodeItemText': []\r\n",
        "# # , 'transactionCodeItemReportText': []\r\n",
        "# # , 'transactionCodeItemShortCode': []}]\"\"\"\r\n",
        "\r\n",
        "# from pyspark.sql import SparkSession\r\n",
        "# from pyspark.sql.functions import from_json, col, collect_list\r\n",
        "# from pyspark.sql.types import StructType, StructField, StringType, ArrayType\r\n",
        "\r\n",
        "# data = \"\"\"[{'participantAssignNumber': 1\r\n",
        "# , 'participantType': 'Agent'\r\n",
        "# , 'participantId': 'ag9K9PcuAlSdq8NBG6aQBAdg'\r\n",
        "# , 'participantName': 'Jennifer Gray'\r\n",
        "# , 'participantOfferTime': '2023-09-06T16:47:45.091-04:00'\r\n",
        "# , 'participantOfferAction': 'OfferTimeout'\r\n",
        "# , 'participantOfferActionTime': '2023-09-06T16:48:00.096-04:00'\r\n",
        "# , 'participantOfferDuration': {'value': 15005, 'ongoing': False}\r\n",
        "# , 'participantHandlingEndTime': 'None'\r\n",
        "# , 'participantHandlingDuration': 'None'\r\n",
        "# , 'participantWrapUpEndTime': 'None'\r\n",
        "# , 'participantWrapUpDuration': 'None'\r\n",
        "# , 'participantProcessingDuration': 'None'\r\n",
        "# , 'participantBusyDuration': {'value': 15005, 'ongoing': False}\r\n",
        "# , 'warmTransfersCompleted': 0\r\n",
        "# , 'blindTransferToAgent': 0\r\n",
        "# , 'blindTransferToQueue': 0\r\n",
        "# , 'consultationsEstablished': 0\r\n",
        "# , 'conferencesEstablished': 0\r\n",
        "# , 'participantHold': 'None'\r\n",
        "# , 'participantHoldDuration': 'None'\r\n",
        "# , 'participantLongestHoldDuration': 'None'\r\n",
        "# , 'participantMute': 'None'\r\n",
        "# , 'participantMuteDuration': 'None'\r\n",
        "# , 'participantLongestMuteDuration': 'None'\r\n",
        "# }\r\n",
        "# , {'participantAssignNumber': 2\r\n",
        "# , 'participantType': 'Agent'\r\n",
        "# , 'participantId': 'agwXpAfDwtQLiuiKbE3SinKQ'\r\n",
        "# , 'participantName': 'Lanardia Paschal'\r\n",
        "# , 'participantOfferTime': '2023-09-06T16:48:01.961-04:00'\r\n",
        "# , 'participantOfferAction': 'Accepted'\r\n",
        "# , 'participantOfferActionTime': '2023-09-06T16:48:07.552-04:00'\r\n",
        "# , 'participantOfferDuration': {'value': 5591, 'ongoing': False}\r\n",
        "# , 'participantHandlingEndTime': '2023-09-06T16:55:17.624-04:00'\r\n",
        "# , 'participantHandlingDuration': {'value': 430072, 'ongoing': False}\r\n",
        "# , 'participantWrapUpEndTime': '2023-09-06T16:55:19.331-04:00'\r\n",
        "# , 'participantWrapUpDuration': {'value': 1707, 'ongoing': False}\r\n",
        "# , 'participantProcessingDuration': {'value': 431779, 'ongoing': False}\r\n",
        "# , 'participantBusyDuration': {'value': 437370, 'ongoing': False}\r\n",
        "# , 'warmTransfersCompleted': 0\r\n",
        "# , 'blindTransferToAgent': 0\r\n",
        "# , 'blindTransferToQueue': 0\r\n",
        "# , 'consultationsEstablished': 0\r\n",
        "# , 'conferencesEstablished': 0\r\n",
        "# , 'participantHold': 'None'\r\n",
        "# , 'participantHoldDuration': 'None'\r\n",
        "# , 'participantLongestHoldDuration': 'None'\r\n",
        "# , 'participantMute': 'None'\r\n",
        "# , 'participantMuteDuration': 'None'\r\n",
        "# , 'participantLongestMuteDuration': 'None'\r\n",
        "# }]\"\"\"\r\n",
        "\r\n",
        "\r\n",
        "# participants_schema = StructType([\r\n",
        "#     StructField('participantAssignNumber', IntegerType(), True),\r\n",
        "#     StructField('participantType', StringType(), True),\r\n",
        "#     StructField('participantId', StringType(), True),\r\n",
        "#     StructField('participantName', StringType(), True),\r\n",
        "#     StructField('participantOfferTime', StringType(), True),\r\n",
        "#     StructField('participantOfferAction', StringType(), True),\r\n",
        "#     StructField('participantOfferActionTime', StringType(), True),\r\n",
        "#     StructField('participantOfferDuration', StructType([\r\n",
        "#         StructField('value', IntegerType(), True),\r\n",
        "#         StructField('ongoing', BooleanType(), True)\r\n",
        "#     ]), True),\r\n",
        "#     StructField('participantHandlingEndTime', StringType(), True),\r\n",
        "#     StructField('participantHandlingDuration', StructType([\r\n",
        "#         StructField('value', IntegerType(), True),\r\n",
        "#         StructField('ongoing', BooleanType(), True)\r\n",
        "#     ]), True),\r\n",
        "#     StructField('participantWrapUpEndTime', StringType(), True),\r\n",
        "#     StructField('participantWrapUpDuration', StructType([\r\n",
        "#         StructField('value', IntegerType(), True),\r\n",
        "#         StructField('ongoing', BooleanType(), True)\r\n",
        "#     ]), True),\r\n",
        "#     StructField('participantProcessingDuration', StructType([\r\n",
        "#         StructField('value', IntegerType(), True),\r\n",
        "#         StructField('ongoing', BooleanType(), True)\r\n",
        "#     ]), True),\r\n",
        "#     StructField('participantBusyDuration', StructType([\r\n",
        "#         StructField('value', IntegerType(), True),\r\n",
        "#         StructField('ongoing', BooleanType(), True)\r\n",
        "#     ]), True),\r\n",
        "#     StructField('warmTransfersCompleted', IntegerType(), True),\r\n",
        "#     StructField('blindTransferToAgent', IntegerType(), True),\r\n",
        "#     StructField('blindTransferToQueue', IntegerType(), True),\r\n",
        "#     StructField('consultationsEstablished', IntegerType(), True),\r\n",
        "#     StructField('conferencesEstablished', IntegerType(), True),\r\n",
        "#     StructField('participantHold', StringType(), True),\r\n",
        "#     StructField('participantHoldDuration', StringType(), True),\r\n",
        "#     StructField('participantLongestHoldDuration', StringType(), True),\r\n",
        "#     StructField('participantMute', StringType(), True),\r\n",
        "#     StructField('participantMuteDuration', StringType(), True),\r\n",
        "#     StructField('participantLongestMuteDuration', StringType(), True)\r\n",
        "# ])\r\n",
        "      \r\n",
        "\r\n",
        "# # Create a DataFrame with a single column named \"data\"\r\n",
        "# schema = StructType([StructField(\"data\", StringType(), True)])\r\n",
        "# df = spark.createDataFrame([(data,)], schema)\r\n",
        "# display(df)\r\n",
        "\r\n",
        "# # Use from_json to parse the \"data\" column into an array of structs\r\n",
        "# df = df.withColumn(\"parsed_data\", from_json(col(\"data\"), ArrayType(participants_schema)))\r\n",
        "# display(df.select(\"parsed_data.participantName\"))\r\n",
        "# # Use collect_list to collect 'name' values into a list\r\n",
        "# ##result_df = df.select(collect_list(\"parsed_data.name\").alias(\"name_list\"))\r\n",
        "\r\n",
        "# #display(result_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# from pyspark.sql import SparkSession\r\n",
        "# from pyspark.sql.functions import from_json, col, collect_list\r\n",
        "# from pyspark.sql.types import StructType, StructField, StringType, ArrayType,MapType,IntegerType,BooleanType\r\n",
        "# import json\r\n",
        "\r\n",
        "# # Create a SparkSession\r\n",
        "# #spark = SparkSession.builder.appName(\"example\").getOrCreate()\r\n",
        "\r\n",
        "# # Define the JSON data as a string\r\n",
        "# #data = \"\"\"[{'participantAssignNumber': 1,'participantType': 'Agent','participantId':'ag9K9PcuAlSdq8NBG6aQBAdg', 'participantName': 'Jennifer Gray', 'participantOfferTime': '2023-09-06T16:47:45.091-04:00', 'participantOfferAction': 'OfferTimeout', 'participantOfferActionTime': '2023-09-06T16:48:00.096-04:00', 'participantOfferDuration': {'value': 15005, 'ongoing': False}, 'participantHandlingEndTime': None, 'participantHandlingDuration': None, 'participantWrapUpEndTime': None, 'participantWrapUpDuration': None, 'participantProcessingDuration': None, 'participantBusyDuration': {'value': 15005, 'ongoing': False}, 'warmTransfersCompleted': 0, 'blindTransferToAgent': 0, 'blindTransferToQueue': 0, 'consultationsEstablished': 0, 'conferencesEstablished': 0, 'participantHold': None, 'participantHoldDuration': None, 'participantLongestHoldDuration': None, 'participantMute': None, 'participantMuteDuration': None, 'participantLongestMuteDuration': None}, {'participantAssignNumber': 2, 'participantType': 'Agent', 'participantId': 'agwXpAfDwtQLiuiKbE3SinKQ', 'participantName': 'Lanardia Paschal', 'participantOfferTime': '2023-09-06T16:48:01.961-04:00', 'participantOfferAction': 'Accepted', 'participantOfferActionTime': '2023-09-06T16:48:07.552-04:00', 'participantOfferDuration': {'value': 5591, 'ongoing': False}, 'participantHandlingEndTime': '2023-09-06T16:55:17.624-04:00', 'participantHandlingDuration': {'value': 430072, 'ongoing': False}, 'participantWrapUpEndTime': '2023-09-06T16:55:19.331-04:00', 'participantWrapUpDuration': {'value': 1707, 'ongoing': False}, 'participantProcessingDuration': {'value': 431779, 'ongoing': False}, 'participantBusyDuration': {'value': 437370, 'ongoing': False}, 'warmTransfersCompleted': 0, 'blindTransferToAgent': 0, 'blindTransferToQueue': 0, 'consultationsEstablished': 0, 'conferencesEstablished': 0, 'participantHold': None, 'participantHoldDuration': None, 'participantLongestHoldDuration': None, 'participantMute': None, 'participantMuteDuration': None, 'participantLongestMuteDuration': None}]\"\"\"\r\n",
        "# data = \"\"\"[\r\n",
        "#     {'participantAssignNumber': 1\r\n",
        "# ,'participantType': 'Agent'\r\n",
        "# , 'participantId': 'ag9K9PcuAlSdq8NBG6aQBAdg'\r\n",
        "# , 'participantName': 'Jennifer Gray'\r\n",
        "# , 'participantOfferTime': '2023-09-06T16:47:45.091-04:00'\r\n",
        "# , 'participantOfferAction': 'OfferTimeout'\r\n",
        "# , 'participantOfferActionTime': '2023-09-06T16:48:00.096-04:00'\r\n",
        "# , 'participantOfferDuration': {'value': 15005, 'ongoing': False}\r\n",
        "\r\n",
        "# , 'blindTransferToAgent': 0\r\n",
        "\r\n",
        "\r\n",
        "# }\r\n",
        "# ,{\r\n",
        "#     'participantAssignNumber': 1\r\n",
        "#     ,'participantType': 'Agent'\r\n",
        "#     , 'participantId': 'ag9K9PcuAlSdq8NBG6aQBAdg'\r\n",
        "#     , 'participantName': 'Jennifer Gray'\r\n",
        "#     , 'participantOfferTime': '2023-09-06T16:47:45.091-04:00'\r\n",
        "#     , 'participantOfferAction': 'OfferTimeout'\r\n",
        "#     , 'participantOfferActionTime': '2023-09-06T16:48:00.096-04:00'\r\n",
        "# , 'participantOfferDuration':{'value': 15005, 'ongoing': False}\r\n",
        "\r\n",
        "# , 'blindTransferToAgent': 0\r\n",
        "\r\n",
        "\r\n",
        "# }\r\n",
        "\r\n",
        "\r\n",
        "# ]\"\"\"\r\n",
        "\r\n",
        "# # Convert the JSON string to a list of dictionaries\r\n",
        "# #data_list = json.loads(data)\r\n",
        "\r\n",
        "# # Create a PySpark DataFrame\r\n",
        "# schema = StructType([StructField(\"data\", StringType(), True)])\r\n",
        "# df = spark.createDataFrame([(data,)], schema)\r\n",
        "\r\n",
        "# display(df)\r\n",
        "# participants_schema = StructType([StructField('participantAssignNumber', StringType(), True),\r\n",
        "# StructField('participantType', StringType(), True),\r\n",
        "# StructField('participantId', StringType(), True),\r\n",
        "# StructField('participantName', StringType(), True),\r\n",
        "# StructField('participantOfferAction', StringType(), True),\r\n",
        "# StructField(\"participantOfferDuration\", StructType([\r\n",
        "#         StructField(\"value\", IntegerType(), True),\r\n",
        "#         StructField(\"ongoing\", BooleanType(), True)\r\n",
        "#     ]), True),\r\n",
        "\r\n",
        "# ])\r\n",
        "# df = df.withColumn(\"parsed_data\", from_json(col(\"data\"), ArrayType(participants_schema)))\r\n",
        "# display(df)\r\n",
        "# #display(df.select(\"parsed_data.participantAssignNumber\",\"parsed_data.participantId\",\"parsed_data.participantName\",\"parsed_data.participantOfferAction\"))\r\n",
        "# # Show the DataFrame\r\n",
        "# #df.show()\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# from pyspark.sql import SparkSession\r\n",
        "# from pyspark.sql.functions import from_json, col\r\n",
        "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType\r\n",
        "\r\n",
        "# # Initialize SparkSession\r\n",
        "# #spark = SparkSession.builder.appName(\"JSONToDataFrame\").getOrCreate()\r\n",
        "\r\n",
        "# # Sample DataFrame with a column named 'data' containing JSON strings\r\n",
        "# data = [\r\n",
        "#     \"\"\"{\r\n",
        "#         \"participantAssignNumber\": 1,\r\n",
        "#         \"participantType\": \"Agent\",\r\n",
        "#         \"participantId\": \"ag9K9PcuAlSdq8NBG6aQBAdg\",\r\n",
        "#         \"participantName\": \"Jennifer Gray\",\r\n",
        "#         \"participantOfferTime\": \"2023-09-06T16:47:45.091-04:00\",\r\n",
        "#         \"participantOfferAction\": \"OfferTimeout\",\r\n",
        "#         \"participantOfferActionTime\": \"2023-09-06T16:48:00.096-04:00\",\r\n",
        "#         \"participantOfferDuration\": {\"value\": 15005, \"ongoing\": false},\r\n",
        "#         \"blindTransferToAgent\": 0\r\n",
        "#     }\"\"\",\r\n",
        "#     \"\"\"{\r\n",
        "#         \"participantAssignNumber\": 1,\r\n",
        "#         \"participantType\": \"Agent\",\r\n",
        "#         \"participantId\": \"ag9K9PcuAlSdq8NBG6aQBAdg\",\r\n",
        "#         \"participantName\": \"Jennifer Gray2\",\r\n",
        "#         \"participantOfferTime\": \"2023-09-06T16:47:45.091-04:00\",\r\n",
        "#         \"participantOfferAction\": \"OfferTimeout\",\r\n",
        "#         \"participantOfferActionTime\": \"2023-09-06T16:48:00.096-04:00\",\r\n",
        "#         \"participantOfferDuration\": {\"value\": 15005, \"ongoing\": false},\r\n",
        "#         \"blindTransferToAgent\": 0\r\n",
        "#     }\"\"\"\r\n",
        "# ]\r\n",
        "\r\n",
        "# # Define the schema for parsing the JSON data\r\n",
        "# participants_schema = StructType([\r\n",
        "#     StructField(\"participantAssignNumber\", IntegerType(), True),\r\n",
        "#     StructField(\"participantType\", StringType(), True),\r\n",
        "#     StructField(\"participantId\", StringType(), True),\r\n",
        "#     StructField(\"participantName\", StringType(), True),\r\n",
        "#     StructField(\"participantOfferAction\", StringType(), True),\r\n",
        "#     StructField(\"participantOfferDuration\", StructType([\r\n",
        "#         StructField(\"value\", IntegerType(), True),\r\n",
        "#         StructField(\"ongoing\", BooleanType(), True)\r\n",
        "#     ]), True),\r\n",
        "# ])\r\n",
        "\r\n",
        "# # Create a DataFrame with the 'data' column\r\n",
        "# df = spark.createDataFrame([data], StringType())\r\n",
        "\r\n",
        "# # Parse the 'data' column as JSON\r\n",
        "# df = df.withColumn(\"parsed_data\", from_json(col(\"value\"), ArrayType(participants_schema)))\r\n",
        "\r\n",
        "# display(df.select(\"parsed_data.participantName\"))\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# from pyspark.sql import SparkSession\r\n",
        "# from pyspark.sql.functions import from_json, col\r\n",
        "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType,ArrayType\r\n",
        "# participants_schema =  StructType([\r\n",
        "#     StructField('participantAssignNumber', IntegerType(), True),\r\n",
        "#     StructField('participantType', StringType(), True),\r\n",
        "#     StructField('participantId', StringType(), True),\r\n",
        "#     StructField('participantName', StringType(), True),\r\n",
        "#     StructField('participantOfferTime', StringType(), True),\r\n",
        "#     StructField('participantOfferAction', StringType(), True),\r\n",
        "#     StructField('participantOfferActionTime', StringType(), True),\r\n",
        "#     StructField('participantOfferDuration', StructType([\r\n",
        "#         StructField('value', IntegerType(), True),\r\n",
        "#         StructField('ongoing', BooleanType(), True)\r\n",
        "#     ]), True),\r\n",
        "#     StructField('participantHandlingEndTime', StringType(), True),\r\n",
        "#     StructField('participantHandlingDuration', StructType([\r\n",
        "#         StructField('value', IntegerType(), True),\r\n",
        "#         StructField('ongoing', BooleanType(), True)\r\n",
        "#     ]), True),\r\n",
        "#     StructField('participantWrapUpEndTime', StringType(), True),\r\n",
        "#     StructField('participantWrapUpDuration', StructType([\r\n",
        "#         StructField('value', IntegerType(), True),\r\n",
        "#         StructField('ongoing', BooleanType(), True)\r\n",
        "#     ]), True),\r\n",
        "#     StructField('participantProcessingDuration', StructType([\r\n",
        "#         StructField('value', IntegerType(), True),\r\n",
        "#         StructField('ongoing', BooleanType(), True)\r\n",
        "#     ]), True),\r\n",
        "#     StructField('participantBusyDuration', StructType([\r\n",
        "#         StructField('value', IntegerType(), True),\r\n",
        "#         StructField('ongoing', BooleanType(), True)\r\n",
        "#     ]), True),\r\n",
        "#     StructField('warmTransfersCompleted', IntegerType(), True),\r\n",
        "#     StructField('blindTransferToAgent', IntegerType(), True),\r\n",
        "#     StructField('blindTransferToQueue', IntegerType(), True),\r\n",
        "#     StructField('consultationsEstablished', IntegerType(), True),\r\n",
        "#     StructField('conferencesEstablished', IntegerType(), True),\r\n",
        "#     StructField('participantHold', StringType(), True),\r\n",
        "#     StructField('participantHoldDuration', StringType(), True),\r\n",
        "#     StructField('participantLongestHoldDuration', StringType(), True),\r\n",
        "#     StructField('participantMute', StringType(), True),\r\n",
        "#     StructField('participantMuteDuration', StringType(), True),\r\n",
        "#     StructField('participantLongestMuteDuration', StringType(), True)\r\n",
        "\r\n",
        "# ,StructField('transactionCodeListId', StringType(), True),\r\n",
        "# StructField('transactionCodeListName', StringType(), True),\r\n",
        "# StructField('transactionCodeItemId', StringType(), True),\r\n",
        "# StructField('transactionCodeItemText', StringType(), True),\r\n",
        "# StructField('transactionCodeItemReportText', StringType(), True),\r\n",
        "# StructField('transactionCodeItemShortCode', StringType(), True),\r\n",
        "# ])  \r\n",
        "\r\n",
        "# # participants_schema = StructType([StructField('participantAssignNumber', StringType(), True),\r\n",
        "# # StructField('participantType', StringType(), True),\r\n",
        "# # StructField('participantId', StringType(), True),\r\n",
        "# # StructField('participantName', StringType(), True),\r\n",
        "# # StructField('participantOfferAction', StringType(), True),\r\n",
        "# # StructField(\"participantOfferDuration\", StructType([\r\n",
        "# #         StructField(\"value\", IntegerType(), True),\r\n",
        "# #         StructField(\"ongoing\", BooleanType(), True)\r\n",
        "# #     ]), True),\r\n",
        "\r\n",
        "\r\n",
        "# # ])\r\n",
        "# participants_schema = StructType([\r\n",
        "#     StructField(\"participantAssignNumber\", StringType(), True),\r\n",
        "#     #StructField(\"participantType\", StringType(), True),\r\n",
        "#     StructField(\"participantId\", StringType(), True),\r\n",
        "#     StructField(\"participantName\", StringType(), True),\r\n",
        "#     StructField(\"participantOfferAction\", StringType(), True),\r\n",
        "    \r\n",
        "# ])\r\n",
        "# df = spark.sql(f\"\"\"select interactionId\r\n",
        "#                    ,participants\r\n",
        "#                    --,participantAssignNumber\r\n",
        "#                    --,participantId\r\n",
        "#                    ,participantName\r\n",
        "#                    --,participantOfferAction\r\n",
        "#   from delta.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/historical_analytics/incremental/20230921`\r\n",
        "# where participants is not null\"\"\")\r\n",
        "# df = df.withColumn(\"parsed_data\", from_json(col(\"participants\"), ArrayType(participants_schema)))\r\n",
        "# #display(df)\r\n",
        "# display(df.select(\"parsed_data.participantAssignNumber\",\"parsed_data.participantId\",\"parsed_data.participantName\",\"parsed_data.participantOfferAction\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#%%sql\r\n",
        "\r\n",
        "#OPTIMIZE delta.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/historical_analytics/incremental/**`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# -- %%sql\r\n",
        "# -- select distinct split(split(queueWaitDuration,',')[0],':')[1]/1000\r\n",
        "# -- --participantAssignNumber,participantId,participantName,participantOfferAction\r\n",
        "# --   from delta.`abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/historical_analytics/incremental/20230921`\r\n",
        "# --   where interactionId = 'int-18aafa907ce-TwSMHTchKftrlWWF9DQHK2zbZ-phone-03-wolverineworldwid01'\r\n",
        "# -- ;\r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "description": "process apis for contact center ops (historical, pcs)",
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}