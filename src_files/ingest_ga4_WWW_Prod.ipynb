{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# GA4 Flat Table Load\r\n",
        "There is a table with flattened google analytics Data. This Notebook ingests that data.\r\n",
        "\r\n",
        "**Revision History**\r\n",
        "Created 9/12/2024 Vish\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import concurrent.futures\n",
        "from delta import *\n",
        "from pyspark.sql.types import StructType, StructField, ArrayType, StringType, LongType, DoubleType, BooleanType, MapType,IntegerType\n",
        "from pyspark.sql.functions import *\n",
        "from functools import reduce\n",
        "from pyspark.sql.dataframe import DataFrame\n",
        "import pyspark.sql.functions as F\n",
        "import json\n",
        "import base64\n",
        "from datetime import datetime,timedelta\n",
        "from time import sleep\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"DYNAMIC\")\n",
        "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from pyspark.sql.functions import max as spark_max"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Run the common functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Retrieve Google Big Query Credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "project = \"ga360-connection-267115\"\n",
        "\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary  \n",
        "ga4_credentials = token_library.getSecret(kv_name, \"GA4-credentials\", \"ls_kv_adap\")  \n",
        "print(ga4_credentials)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Set Load Type: Full or Incremental\r\n",
        "If the load_type is set to full then we will query all the records from big query, and write in raw zone, in parquet format, and overwrite the existing data. A full load takes around 20 minutes.\r\n",
        "\r\n",
        "\r\n",
        "If the load type is Incremental then we query new records based on a watermark variable and append the newer records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#load_type = 'Full'\r\n",
        "load_type = 'Incremental'\r\n",
        "water_mark_timestamp = 1724817599998701"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Set folder location to save data to Raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "base_folder = 'GA4/www_prod'\r\n",
        "output_folder = f'{raw_adls_path}{base_folder}'\r\n",
        "print(output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Set the water_mark variable\r\n",
        "<u>**We query the raw table to find out the maximum event_timestamp. Then we can use that to query newer events from google big query**</u>\r\n",
        "### Get the storage account Key\r\n",
        "The storage account key is stored in the keyvault associated with Synpase workspace. The secret name in the key vault is 'storage-key'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "keyvult_key = 'storage-key'\r\n",
        "account_key = mssparkutils.credentials.getSecret(kv_name , keyvult_key,'ls_kv_adap' )\r\n",
        "storage_account_name = raw_adls_path.split('@')[1].split('.')[0]\r\n",
        "container_name = 'raw'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Create Blob Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "blob_service_client = BlobServiceClient(account_url=f\"https://{storage_account_name}.blob.core.windows.net\", credential=account_key)\r\n",
        "container_name = 'raw'\r\n",
        "container_client = blob_service_client.get_container_client(container_name)\r\n",
        "blob_name = f\"GA4/watermark.json\"\r\n",
        "blob_client = container_client.get_blob_client(blob_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Read Blob Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "blob_data = blob_client.download_blob().readall()\r\n",
        "\r\n",
        "# If the blob content is JSON, parse it\r\n",
        "blob_content = json.loads(blob_data)\r\n",
        "water_mark_timestamp = blob_content.get('max_event_timestamp', 0)\r\n",
        "\r\n",
        "# Print the value to verify\r\n",
        "print(water_mark_timestamp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Load data in Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "    if load_type == 'Full':\n",
        "        df_WWW_Prod = spark.read.format(\"bigquery\")\\\n",
        "            .option(\"credentials\",ga4_credentials)\\\n",
        "            .option(\"parentProject\",'ga360-connection-267115')\\\n",
        "            .option(\"dataset\",\"WWW_PROD\")\\\n",
        "            .option(\"table\",\"ga4_raw_events\")\\\n",
        "            .load()\n",
        "    elif load_type == 'Incremental':\n",
        "        # Load data from BigQuery and filter based on event_timestamp\n",
        "        df_WWW_Prod = spark.read.format(\"bigquery\")\\\n",
        "        .option(\"credentials\", ga4_credentials)\\\n",
        "        .option(\"parentProject\", 'ga360-connection-267115')\\\n",
        "        .option(\"dataset\", \"WWW_PROD\")\\\n",
        "        .option(\"table\", \"ga4_raw_events\")\\\n",
        "        .load()\\\n",
        "        .filter(f\"event_timestamp > {water_mark_timestamp}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Find the maximum event_timestamp, this will be used to update watermark\r\n",
        "new_max_event_timestamp = df_WWW_Prod.agg(spark_max(\"event_timestamp\")).collect()[0][0]\r\n",
        "\r\n",
        "# Print the maximum event_timestamp\r\n",
        "print(f\"new_max_event_timestamp: {new_max_event_timestamp}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_WWW_Prod.count()\r\n",
        "#Incremental count - 108679151\r\n",
        "# Full Count -      2010847393\r\n",
        "# Incrementa count2  118607626\r\n",
        "# Incremental Count  258527943 Oct-11-2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Set folder location to save data to Raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "base_folder = 'GA4/www_prod'\n",
        "output_folder = f'{raw_adls_path}{base_folder}'\n",
        "print(output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Save Data in Parquet format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if load_type == 'Full':\r\n",
        "    df_WWW_Prod.write.format(\"parquet\").mode(\"overwrite\").save(output_folder)\r\n",
        "elif load_type == 'Incremental':\r\n",
        "    df_WWW_Prod.write.format(\"parquet\").mode(\"append\").save(output_folder)\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Update the Watermark file\r\n",
        "We write the new timestamp to the file, so that the next time we pull data incrementally from this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "blob_data = blob_client.download_blob().readall()\r\n",
        "\r\n",
        "# Parse the blob content as JSON\r\n",
        "blob_content = json.loads(blob_data)\r\n",
        "\r\n",
        "# Update the max_event_timestamp with the new value\r\n",
        "\r\n",
        "blob_content['max_event_timestamp'] = new_max_event_timestamp\r\n",
        "\r\n",
        "# Convert the updated JSON content back to a string\r\n",
        "updated_blob_data = json.dumps(blob_content)\r\n",
        "\r\n",
        "# Upload the updated content back to the blob\r\n",
        "blob_client.upload_blob(updated_blob_data, overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}