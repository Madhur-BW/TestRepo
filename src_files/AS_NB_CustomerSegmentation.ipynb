{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Customer Segmentation\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azure.storage.blob import BlobServiceClient\n",
        "from notebookutils import mssparkutils\n",
        "import urllib.parse\n",
        "import re\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import col, to_date, coalesce, regexp_replace, date_add, current_date, last_day, expr, lit, unbase64,col, hex, split, lead, when, min as min_, max, translate, desc\n",
        "\n",
        "from datetime import timedelta\n",
        "from pyspark.sql import functions as F, Window\n",
        "from delta.tables import DeltaTable\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### includes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Set Configuration and Get Secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# SharePoint API Details\n",
        "tenant_id = \"deace5d6-717b-4f79-ab12-6357206c0c36\"\n",
        "\n",
        "match = re.search(r'@([^.]+)\\.dfs\\.core\\.windows\\.net', raw_adls_path)\n",
        "storage_account = match.group(1) if match else None\n",
        "print(f\"storage_account: {storage_account}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Set pipeline parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Add 1 day to current date\n",
        "tomorrow = date_add(current_date(), 1)\n",
        "week_start_date = '2025-11-01'\n",
        "month_start_date = '2025-09-01'\n",
        "##ull_refresh = 1       ### set full_refresh = 1 to reload from Week_start_date and Month_start_date  ### not used anymore ###\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Get Source data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "target_path = f\"abfss://gold@{storage_account}.dfs.core.windows.net/CustomerSegmentation/CustomerSegmentationHistory/\"\n",
        "cust_path = f\"abfss://gold@{storage_account}.dfs.core.windows.net/mParticle/CustomerMasterHistory/\"\n",
        "SalesUnion_path = f\"abfss://gold@{storage_account}.dfs.core.windows.net/SalesUnion/\"\n",
        "hub_path = f\"abfss://raw@{storage_account}.dfs.core.windows.net/Snowflake/ANALYTICS_PROD/DV_RDV/HUB_CUSTOMER/*.parquet\"\n",
        "exchange_path = f\"abfss://gold@{storage_account}.dfs.core.windows.net/SAP/BW/FxRatesExtendedCalendarDay/\"\n",
        "custkey_path = f\"abfss://silver@{storage_account}.dfs.core.windows.net/mParticle/CustomerKeyLookup/\"\n",
        "date_path = f\"abfss://raw@{storage_account}.dfs.core.windows.net/Snowflake/ANALYTICS_PROD/ANALYTICS_DATA/DIM_CALENDAR/*.parquet\"\n",
        "material_path = f\"abfss://gold@{storage_account}.dfs.core.windows.net/SAP/BW/Material/\"\n",
        "hierarchy_path = f\"abfss://gold@{storage_account}.dfs.core.windows.net/SAP/BW/ProductHierarchyLevel2/\"\n",
        "site_path = f\"abfss://gold@{storage_account}.dfs.core.windows.net/SAP/BW/Site/\"\n",
        "upc_path = f\"abfss://raw@{storage_account}.dfs.core.windows.net/SAP/BW/ZUPC/\"\n",
        "\n",
        "ex_df = spark.read.format(\"delta\").load(exchange_path)\n",
        "hub_df = spark.read.format(\"parquet\").load(hub_path)\n",
        "cust_df = spark.read.format(\"parquet\").load(cust_path)\n",
        "custkey_df = spark.read.format(\"delta\").load(custkey_path)\n",
        "sales_df = spark.read.format(\"parquet\").load(SalesUnion_path)\n",
        "date_df = spark.read.format(\"parquet\").load(date_path)\n",
        "material_df = spark.read.format(\"delta\").load(material_path)\n",
        "prod_hierarchy_df = spark.read.format(\"delta\").load(hierarchy_path)\n",
        "site_df = spark.read.format(\"delta\").load(site_path)\n",
        "upc_df = spark.read.format(\"parquet\").load(upc_path)\n",
        "\n",
        "print(f\"exchange_path: {exchange_path}\")\n",
        "print(f\"hub_path: {hub_path}\")\n",
        "print(f\"cust_path: {cust_path}\")\n",
        "print(f\"custkey_path: {custkey_path}\")\n",
        "print(f\"SalesUnion_path: {SalesUnion_path}\")\n",
        "print(f\"target_path: {target_path}\")\n",
        "\n",
        "sales_df.createOrReplaceTempView(\"salesunion\")\n",
        "site_df.createOrReplaceTempView(\"site\")\n",
        "##sales_df.show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Get incremental Dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "editable": true,
        "run_control": {
          "frozen": false
        }
      },
      "source": [
        "max_week = None\n",
        "max_month = None\n",
        "\n",
        "if DeltaTable.isDeltaTable(spark, target_path):\n",
        "    target_df = spark.read.format(\"delta\").load(target_path)\n",
        "    max_month = target_df.filter(col(\"period\") == \"monthly\").agg(max(\"Segmentation_date\")).collect()[0][0]\n",
        "    max_week = target_df.filter(col(\"period\") == \"weekly\").agg(max(\"Segmentation_date\")).collect()[0][0]\n",
        "\n",
        "print(f\"max_month: {max_month}\")\n",
        "print(f\"max_week: {max_week}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "date_df = date_df.select(col(\"calendar_date\").alias(\"calendar_date\"))\n",
        "\n",
        "# month conditions \n",
        "month_conditions = (\n",
        "    (col(\"calendar_date\") == last_day(col(\"calendar_date\"))) &\n",
        "    (col(\"calendar_date\") <= tomorrow) &\n",
        "    (col(\"calendar_date\") >= month_start_date)\n",
        ")\n",
        "\n",
        "if max_month is not None:\n",
        "    month_conditions = month_conditions & (col(\"calendar_date\") > max_month)\n",
        "\n",
        "# Monthly period\n",
        "monthly_df2 = date_df.filter(month_conditions).withColumn(\"segmentation_date\", col(\"calendar_date\")) \\\n",
        " .withColumn(\"period\", lit(\"monthly\"))\n",
        "\n",
        "monthly_df = monthly_df2.orderBy(col(\"segmentation_date\")).limit(12)\n",
        "\n",
        "# Week conditions \n",
        "Week_conditions = (\n",
        "    (col(\"weekday_name\") == \"Mon\") &\n",
        "    (col(\"calendar_date\") <= tomorrow) &\n",
        "    (col(\"calendar_date\") >= week_start_date)\n",
        ")\n",
        "\n",
        "if max_week is not None:\n",
        "    Week_conditions = Week_conditions & (col(\"calendar_date\") > max_week)\n",
        "\n",
        "\n",
        "# Weekly period\n",
        "weekly_df2 = date_df.filter(Week_conditions).withColumn(\"segmentation_date\", col(\"calendar_date\")) \\\n",
        " .withColumn(\"period\", lit(\"weekly\"))\n",
        "\n",
        "weekly_df = weekly_df2.orderBy(col(\"segmentation_date\")).limit(52)\n",
        "\n",
        "\n",
        "# Combine both\n",
        "segmentation_periods_df = monthly_df.unionByName(weekly_df)\n",
        "segmentation_periods_df.createOrReplaceTempView(\"segmentation_periods\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Materials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "ph1_df = prod_hierarchy_df.alias(\"ph1\")\n",
        "ph2_df = prod_hierarchy_df.alias(\"ph2\")\n",
        "ph3_df = prod_hierarchy_df.alias(\"ph3\")\n",
        "\n",
        "material2_df = (\n",
        "    material_df.alias(\"main\")\n",
        "    .join(ph1_df, F.col(\"main.ProductHierarchylevel1\") == F.col(\"ph1.PROD_HIER\"), \"left\")\n",
        "    .join(ph2_df, F.col(\"main.ProductHierarchylevel2\") == F.col(\"ph2.PROD_HIER\"), \"left\")\n",
        "    .join(ph3_df, F.col(\"main.ProductHierarchylevel3\") == F.col(\"ph3.PROD_HIER\"), \"left\")\n",
        "    .select(\n",
        "        \"main.*\",\n",
        "        F.col(\"ph1.TXTMD\").alias(\"ProductHierarchy1Text\"),\n",
        "        F.col(\"ph2.TXTMD\").alias(\"ProductHierarchy2Text\"),\n",
        "        F.col(\"ph3.TXTMD\").alias(\"ProductHierarchy3Text\")\n",
        "    )\n",
        ")\n",
        "\n",
        "material2_df.createOrReplaceTempView(\"material\")\n",
        "\n",
        "### upc \n",
        "\n",
        "material3_df = (\n",
        "    material2_df.alias(\"m\")\n",
        "    .join(upc_df,F.col(\"m.Material\") == F.col(\"ZMATNUM\"),\"left\")\n",
        "    .select(\"m.*\",\n",
        "        F.col(\"AF_GRDVAL\").alias(\"gridvalue\"),\n",
        "        F.col(\"ZUPC\").alias(\"upc\")  \n",
        "    )\n",
        "    .distinct()\n",
        "\n",
        ")\n",
        "material3_df.createOrReplaceTempView(\"upc\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Customer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "cust_df.createOrReplaceTempView(\"cust\")\n",
        "\n",
        "customer_df = spark.sql(\"\"\"\n",
        "with c as \n",
        "(\n",
        "    select mparticleuserkey,\n",
        "    GlobalCustomerKey,\n",
        "    Email,\n",
        "    case when CustomerType = 0 then 'Registered'\n",
        "        when CustomerType = 1 then 'New'\n",
        "        when CustomerType = 2 then 'Returning'\n",
        "        end as CustomerType,\n",
        "    EffectiveFrom,\n",
        "    (validfrom) as validfrom,\n",
        "    row_number() over(partition by mparticleuserkey, (validfrom) order by validto) R\n",
        "    from cust\n",
        "\n",
        ")\n",
        "SELECT  distinct\n",
        "  mparticleuserkey,\n",
        "  GlobalCustomerKey,\n",
        "  Email,\n",
        "  CustomerType,\n",
        "  EffectiveFrom as cust_EffectiveFrom,\n",
        "  CASE \n",
        "    WHEN validfrom = MIN(validfrom) OVER (PARTITION BY mparticleuserkey) THEN ('1900-01-01')\n",
        "    ELSE validfrom\n",
        "  END AS validfrom,\n",
        "  COALESCE(\n",
        "    LEAD(validfrom) OVER (\n",
        "      PARTITION BY mparticleuserkey\n",
        "      ORDER BY validfrom, EffectiveFrom\n",
        "    ), \n",
        "    TO_TIMESTAMP('9999-12-31')\n",
        "  ) AS validto\n",
        "FROM c\n",
        "where r = 1\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "customer_df.createOrReplaceTempView(\"customers\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Order Rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "order_rank_df = spark.sql(\"\"\"\n",
        "WITH sales AS (\n",
        "    SELECT \n",
        "        s.* ,\n",
        "        CASE WHEN coalesce(IsReturn,0) = 0 THEN 'Sales' ELSE 'Returns' END AS transaction_type,\n",
        "        m.LongDescription as ProductDescription,\n",
        "        m.colorDescription as Color,\n",
        "        m.ProductHierarchy2Text as Category,\n",
        "        m.ProductHierarchy3Text as SubCategory,\n",
        "        i.MediumDescription as StoreName\n",
        "    FROM salesunion s \n",
        "    left join material m on s.product = m.OldMaterialNumber\n",
        "    left join site i on i.site = s.store\n",
        "),\n",
        "orders AS (\n",
        "    SELECT \n",
        "        Tenant, \n",
        "        MparticleUserID,\n",
        "        max(TransactionChannel) as TransactionChannel,\n",
        "        OrderId,\n",
        "        OrderDate,\n",
        "        (OrderDateTime) as OrderDateTime,\n",
        "        transaction_type,\n",
        "        coalesce(IsReturn,0) as IsReturn,\n",
        "        Currency,\n",
        "        max(store) as store,\n",
        "        StoreName,\n",
        "        concat_ws('|', sort_array(array_distinct(collect_list(PromoId)))) AS PromoId,\n",
        "        max(ShippingCountryCode) as ShippingCountryCode,\n",
        "        coalesce(customertype,'Registered') as DerivedCustomerType,\n",
        "        concat_ws(';', sort_array(array_distinct(collect_list(Category)))) AS OrderCategories,\n",
        "        concat_ws(';', sort_array(array_distinct(collect_list(SubCategory)))) AS OrderSubCategories,\n",
        "        count(*) as lines,\n",
        "        sum( case when discountflag = 'Y' then 1 else 0 end ) as DiscountedLines,\n",
        "        concat_ws(';', sort_array(array_distinct(collect_list(ProductDescription)))) AS OrderItems,\n",
        "        SUM(orderquantity) AS orderquantity,\n",
        "        sum(salesquantity) AS salesquantity,\n",
        "        SUM(CASE WHEN PriceType = 'MD' THEN 1 ELSE 0 END) AS no_of_md,\n",
        "        SUM(CASE WHEN PriceType = 'FP' THEN 1 ELSE 0 END) AS no_of_fp,\n",
        "        SUM(CASE WHEN PriceType LIKE '%POS' THEN 1 ELSE 0 END) AS no_of_pos,\n",
        "        SUM(CASE WHEN PriceType = 'MD' THEN COALESCE(order_value_incl_tax_gbp, sales_value_incl_tax_gbp, 0) END) AS sum_of_md,\n",
        "        SUM(CASE WHEN PriceType = 'FP' THEN COALESCE(order_value_incl_tax_gbp, sales_value_incl_tax_gbp, 0) END) AS sum_of_fp,\n",
        "        SUM(CASE WHEN PriceType LIKE '%POS' THEN COALESCE(order_value_incl_tax_gbp, sales_value_incl_tax_gbp, 0) END) AS sum_of_pos,\n",
        "        SUM(COALESCE(order_value_incl_tax, sales_value_incl_tax, 0)) AS order_value_incl_tax,\n",
        "        SUM(COALESCE(order_value_incl_tax_usd, sales_value_incl_tax_usd, 0)) AS order_value_incl_tax_usd,\n",
        "        SUM(COALESCE(order_value_incl_tax_gbp, sales_value_incl_tax_gbp, 0)) AS order_value_incl_tax_gbp,\n",
        "        SUM(COALESCE(sales_value_incl_tax, 0)) AS sales_value_incl_tax,\n",
        "        SUM(COALESCE(sales_value_incl_tax_usd, 0)) AS sales_value_incl_tax_usd,\n",
        "        SUM(COALESCE(sales_value_incl_tax_gbp, 0)) AS sales_value_incl_tax_gbp,\n",
        "        max(EffectiveFrom) as order_EffectiveFrom\n",
        "    FROM sales\n",
        "    GROUP BY \n",
        "        Tenant, MparticleUserID,  OrderId, OrderDate,StoreName,\n",
        "        (OrderDateTime), transaction_type, Currency,  coalesce(IsReturn,0), customertype\n",
        ")\n",
        "    SELECT *,\n",
        "\n",
        "        CASE \n",
        "            WHEN no_of_md = GREATEST(no_of_md, no_of_fp, no_of_pos) AND sum_of_md = \n",
        "                GREATEST(\n",
        "                CASE WHEN GREATEST(no_of_md, no_of_fp, no_of_pos) = no_of_md THEN sum_of_md ELSE 0 END,\n",
        "                CASE WHEN GREATEST(no_of_md, no_of_fp, no_of_pos) = no_of_fp THEN sum_of_fp ELSE 0 END,\n",
        "                CASE WHEN GREATEST(no_of_md, no_of_fp, no_of_pos) = no_of_pos THEN sum_of_pos ELSE 0 END\n",
        "                )\n",
        "            THEN 'MD'\n",
        "\n",
        "            WHEN no_of_fp = GREATEST(no_of_md, no_of_fp, no_of_pos) AND sum_of_fp = \n",
        "                GREATEST(\n",
        "                CASE WHEN GREATEST(no_of_md, no_of_fp, no_of_pos) = no_of_md THEN sum_of_md ELSE 0 END,\n",
        "                CASE WHEN GREATEST(no_of_md, no_of_fp, no_of_pos) = no_of_fp THEN sum_of_fp ELSE 0 END,\n",
        "                CASE WHEN GREATEST(no_of_md, no_of_fp, no_of_pos) = no_of_pos THEN sum_of_pos ELSE 0 END\n",
        "                )\n",
        "            THEN 'FP'\n",
        "\n",
        "            WHEN no_of_pos = GREATEST(no_of_md, no_of_fp, no_of_pos) AND sum_of_pos = \n",
        "                GREATEST(\n",
        "                CASE WHEN GREATEST(no_of_md, no_of_fp, no_of_pos) = no_of_md THEN sum_of_md ELSE 0 END,\n",
        "                CASE WHEN GREATEST(no_of_md, no_of_fp, no_of_pos) = no_of_fp THEN sum_of_fp ELSE 0 END,\n",
        "                CASE WHEN GREATEST(no_of_md, no_of_fp, no_of_pos) = no_of_pos THEN sum_of_pos ELSE 0 END\n",
        "                )\n",
        "            THEN 'POS'\n",
        "        END AS price_type,\n",
        "\n",
        "        ROW_NUMBER() OVER(PARTITION BY MparticleUserID, IsReturn ORDER BY OrderDateTime, regexp_replace(OrderId, '[^0-9]', '')) AS ORDER_SEQ,\n",
        "        CASE WHEN ROW_NUMBER() OVER(PARTITION BY MparticleUserID, IsReturn ORDER BY OrderDateTime DESC, OrderId Desc) = 1 THEN 1 ELSE 0 END AS LAST_ORDER_FLAG\n",
        "\n",
        "    FROM orders F\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "order_rank_df.createOrReplaceTempView(\"order_rank_df\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Orders with Segmentation Dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "orders_df = spark.sql(\"\"\"\n",
        "\t\tselect\n",
        "           s.segmentation_date,\n",
        "           s.period,\n",
        "           o.*,\n",
        "           c.CustomerType,\n",
        "           coalesce(o.order_EffectiveFrom,c.cust_EffectiveFrom) as EffectiveFrom\n",
        "     from customers c\n",
        "     left join order_rank_df o\n",
        "        on c.MparticleUserKey = o.MparticleUserId\n",
        "        and (o.OrderDateTime) >= (c.validfrom)\n",
        "        and (o.OrderDateTime) <  (c.validto)\n",
        "     join segmentation_periods s\n",
        "       on (o.OrderDateTime) < (s.segmentation_date)\n",
        "     \n",
        "\"\"\")\n",
        "\n",
        "##orders_df.show(2)\n",
        "orders_df.createOrReplaceTempView(\"Orders_with_segmentation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Segmentation Period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "period_df = spark.sql(\"\"\"\n",
        "\t\tselect distinct period, segmentation_date from Orders_with_segmentation\n",
        "\"\"\")\n",
        "\n",
        "##orders_df.show(2)\n",
        "period_df.createOrReplaceTempView(\"seg_periods\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Order Lines with Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "order_lines_df = spark.sql(\"\"\"\n",
        "\t\tselect\n",
        "           s.segmentation_date,\n",
        "           s.period,\n",
        "           o.*,\n",
        "           m.colorDescription as Color,\n",
        "        m.ProductHierarchy2Text as Category,\n",
        "        m.ProductHierarchy3Text as SubCategory,\n",
        "        m.ConsumerTerritory as hero,\n",
        "        CASE WHEN m.ProductHierarchy2Text= 'Leggings' THEN m.GridValue end as LeggingSize,\n",
        "        i.MediumDescription as StoreName\n",
        "     from salesunion o\n",
        "     join segmentation_periods s\n",
        "       on o.OrderDateTime < s.segmentation_date\n",
        "    left join upc m on o.barcode = m.upc\n",
        "    left join site i on i.site = o.store\n",
        "    where o.isreturn = 0 \n",
        "       \"\"\")\n",
        "order_lines_df.createOrReplaceTempView(\"order_line_segmentation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Item Level Derivations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "order_items_df = spark.sql(\"\"\"\n",
        "\t\tselect\n",
        "           o.segmentation_date,\n",
        "           o.period,\n",
        "           o.MparticleUserId,\n",
        "           count(distinct barcode ) as total_distinct_items,\n",
        "           count(distinct Category) as NumberOfCategories\n",
        "     from order_line_segmentation o\n",
        "        group by \n",
        "            o.segmentation_date,\n",
        "            o.period,\n",
        "            o.MparticleUserId\n",
        "       \"\"\")\n",
        "order_items_df.createOrReplaceTempView(\"order_items\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### First Order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "first_order_df = spark.sql(\"\"\"\n",
        "WITH cte_RETURNs AS \n",
        "(\n",
        "    select MparticleUserID,\n",
        "            OrderId,\n",
        "            sum(coalesce(r.orderquantity,r.salesquantity,0)) as returnorderquantity\n",
        "        from order_rank_df r\n",
        "    where IsReturn = 1 and ORDER_SEQ = 1 \n",
        "    group by OrderId, MparticleUserID\n",
        ")\n",
        ", cte_sales as \n",
        "(\n",
        "SELECT \n",
        "    F.MparticleUserID,\n",
        "    F.OrderId,\n",
        "    F.TransactionChannel AS FirstOrderSource,\n",
        "    F.OrderDateTime AS FirstOrderDate,\n",
        "    F.Currency as FirstOrderCurrency,\n",
        "    f.store,\n",
        "    f.StoreName,\n",
        "    f.PromoId,\n",
        "    coalesce(f.orderquantity,f.salesquantity,0) as orderquantity,\n",
        "    f.salesquantity,\n",
        "    f.ShippingCountryCode,\n",
        "    COALESCE(F.order_value_incl_tax, F.sales_value_incl_tax) AS Firstordervalue,\n",
        "    COALESCE(F.order_value_incl_tax_usd, F.sales_value_incl_tax_usd) AS Firstordervalueusd,\n",
        "    COALESCE(F.order_value_incl_tax_gbp, F.sales_value_incl_tax_gbp) AS Firstordervaluegbp,\n",
        "    f.PRICE_TYPE AS FIRST_ORDER_PRICE_TYPE,\n",
        "    F.OrderItems,\n",
        "    CASE WHEN DiscountedLines/coalesce(lines,1) > 0.5 then 'Sale' else 'Full Price' end as FPorSale,\n",
        "    F.OrderCategories\n",
        "FROM order_rank_df F\n",
        "WHERE F.ORDER_SEQ = 1 AND F.ISReturn = 0\n",
        ")\n",
        "SELECT \n",
        "s.*,coalesce(r.returnorderquantity,0) as returnorderquantity\n",
        "from cte_sales s \n",
        "left join cte_returns r on s.OrderId = r.OrderId and s.MparticleUserID = r.MparticleUserID\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "##first_order_df.show(2)\n",
        "first_order_df.createOrReplaceTempView(\"first_order\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### First Order Price Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "first_order_pt_df = spark.sql(\"\"\"\n",
        "with sales as \n",
        "(\n",
        "    SELECT \n",
        "        F.MparticleUserID,\n",
        "        F.OrderId,\n",
        "        F.OrderDateTime ,\n",
        "        F.Currency,\n",
        "        SUM(COALESCE(F.order_value_incl_tax, F.sales_value_incl_tax)) AS TOTAL_AMOUNT,\n",
        "        SUM(CASE WHEN F.pricetype LIKE 'FP%' THEN COALESCE(F.order_value_incl_tax_gbp, F.sales_value_incl_tax_gbp) ELSE 0 END) AS FP_AMOUNT,\n",
        "\t\tSUM(CASE WHEN F.pricetype LIKE 'MD%' THEN COALESCE(F.order_value_incl_tax_gbp, F.sales_value_incl_tax_gbp) ELSE 0 END) AS MD_AMOUNT,\n",
        "\t\tSUM(CASE WHEN F.PROMOID <> '' and F.PROMOID is not null  THEN COALESCE(F.order_value_incl_tax_gbp, F.sales_value_incl_tax_gbp) ELSE 0 END) AS PROMO_AMOUNT\n",
        "    FROM order_rank_df o \n",
        "    join order_line_segmentation F on o.orderid = f.orderid and o.mparticleuserid = f.mparticleuserid and f.OrderDateTime = o.OrderDateTime\n",
        "    WHERE o.ORDER_SEQ = 1 AND o.ISReturn = 0\n",
        "    and o.MPARTICLEUSERID IS NOT NULL \n",
        "    GROUP BY F.MparticleUserID,\n",
        "        F.OrderId,\n",
        "        F.OrderDateTime ,\n",
        "        F.Currency\n",
        ")\n",
        "SELECT \n",
        "    s.*,\n",
        "    CASE WHEN  TOTAL_AMOUNT = 0 THEN 'MIXED'\n",
        "\t\t\tWHEN 100 * FP_AMOUNT/TOTAL_AMOUNT >= 70 THEN 'FP'\n",
        "\t\t\tWHEN 100 * MD_AMOUNT/TOTAL_AMOUNT >= 70 THEN 'MD'\n",
        "\t\t\tWHEN 100 * PROMO_AMOUNT/TOTAL_AMOUNT >= 70 THEN 'PROMO'\n",
        "\t\t\tELSE 'MIXED'\n",
        "\t\tEND CUSTOMER_FIRST_ORDER_PRICE_TYPE_V2\n",
        "from sales s \n",
        "\n",
        "\"\"\")\n",
        "\n",
        "##first_order_df.show(2)\n",
        "first_order_pt_df.createOrReplaceTempView(\"first_order_pt_df\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Customer 12M Rolling Price Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "cust_12m_rolling_pricetype_df = spark.sql(\"\"\"\n",
        "with sales as \n",
        "(\n",
        "    SELECT \n",
        "        F.MparticleUserID,\n",
        "        F.segmentation_date,\n",
        "        F.period,\n",
        "        min(F.OrderDateTime) as min_OrderDateTime ,\n",
        "        SUM(COALESCE(F.order_value_incl_tax, F.sales_value_incl_tax)) AS TOTAL_AMOUNT,\n",
        "        SUM(CASE WHEN F.pricetype LIKE 'FP%' THEN COALESCE(F.order_value_incl_tax_gbp, F.sales_value_incl_tax_gbp) ELSE 0 END) AS FP_AMOUNT,\n",
        "\t\tSUM(CASE WHEN F.pricetype LIKE 'MD%' THEN COALESCE(F.order_value_incl_tax_gbp, F.sales_value_incl_tax_gbp) ELSE 0 END) AS MD_AMOUNT,\n",
        "\t\tSUM(CASE WHEN F.PROMOID <> '' and F.PROMOID is not null  THEN COALESCE(F.order_value_incl_tax_gbp, F.sales_value_incl_tax_gbp) ELSE 0 END) AS PROMO_AMOUNT\n",
        "    FROM order_line_segmentation F\n",
        "    WHERE F.ISReturn = 0\n",
        "    and F.MPARTICLEUSERID IS NOT NULL \n",
        "    and F.orderdatetime > add_months(segmentation_date, -12)\n",
        "    GROUP BY F.MparticleUserID,\n",
        "            F.segmentation_date,\n",
        "            F.period\n",
        ")\n",
        "SELECT \n",
        "    s.*,\n",
        "    CASE WHEN  coalesce(TOTAL_AMOUNT,0) = 0 THEN 'INACTIVE'\n",
        "\t\t\tWHEN 100 * FP_AMOUNT/TOTAL_AMOUNT >= 70 THEN 'FP'\n",
        "\t\t\tWHEN 100 * MD_AMOUNT/TOTAL_AMOUNT >= 70 THEN 'MD'\n",
        "\t\t\tWHEN 100 * PROMO_AMOUNT/TOTAL_AMOUNT >= 70 THEN 'PROMO'\n",
        "\t\t\tELSE 'MIXED'\n",
        "\t\tEND CUSTOMER_12M_ROLLING_PRICETYPE\n",
        "from sales s \n",
        "\n",
        "\"\"\")\n",
        "\n",
        "##first_order_df.show(2)\n",
        "cust_12m_rolling_pricetype_df.createOrReplaceTempView(\"cust_12m_rolling_pricetype_df\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Second Order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "second_order_df = spark.sql(\"\"\"\n",
        "WITH RETURNs AS \n",
        "(\n",
        "    select MparticleUserID,\n",
        "            OrderId,\n",
        "            sum(coalesce(r.orderquantity,r.salesquantity,0)) as returnorderquantity\n",
        "        from order_rank_df r\n",
        "    where IsReturn = 1 and ORDER_SEQ = 2 \n",
        "    group by OrderId, MparticleUserID\n",
        ")\n",
        ", sales as \n",
        "(\n",
        "SELECT \n",
        "    S.MparticleUserID,\n",
        "    s.OrderId,\n",
        "    S.Currency as SecondOrderCurrency,\n",
        "    S.TransactionChannel AS SecondOrderSource,\n",
        "    S.OrderDateTime AS SecondOrderDate,\n",
        "    s.store,\n",
        "    s.StoreName,\n",
        "    s.PromoId,\n",
        "    coalesce(s.orderquantity,s.salesquantity,0) as orderquantity,\n",
        "    s.salesquantity,\n",
        "    COALESCE(S.order_value_incl_tax, S.sales_value_incl_tax) AS Secondordervalue,\n",
        "    COALESCE(S.order_value_incl_tax_usd, S.sales_value_incl_tax_usd) AS Secondordervalueusd,\n",
        "    COALESCE(S.order_value_incl_tax_gbp, S.sales_value_incl_tax_gbp) AS Secondordervaluegbp,\n",
        "    s.PRICE_TYPE AS Second_ORDER_PRICE_TYPE,\n",
        "    s.OrderItems,\n",
        "    S.OrderCategories\n",
        "FROM order_rank_df S \n",
        "WHERE S.ORDER_SEQ = 2 AND S.ISReturn = 0\n",
        ")\n",
        "SELECT \n",
        "s.*,coalesce(r.returnorderquantity,0) as returnorderquantity\n",
        "from sales s \n",
        "left join returns r on s.OrderId = r.OrderId and s.MparticleUserID = r.MparticleUserID\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "##second_order_df.show(2)\n",
        "second_order_df.createOrReplaceTempView(\"second_order\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Last Order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "last_order_df = spark.sql(\"\"\"\n",
        "WITH base_data AS (\n",
        "    SELECT \n",
        "        o.MparticleUserId,\n",
        "        o.segmentation_date,\n",
        "        o.period,\n",
        "        o.orderid,\n",
        "        o.orderdatetime as LastOrderDateTime,\n",
        "        o.TransactionChannel as LastOrderSource,\n",
        "        o.store,\n",
        "        o.StoreName,\n",
        "        o.PromoId,\n",
        "        o.IsReturn,\n",
        "        o.customertype,\n",
        "        o.DerivedCustomerType,\n",
        "        o.currency as LastOrderCurrency,\n",
        "        o.price_type as Last_order_price_type,\n",
        "        SUM(COALESCE(o.order_value_incl_tax_gbp, o.sales_value_incl_tax_gbp)) AS LastordervalueGBP,\n",
        "        SUM(COALESCE(o.order_value_incl_tax, o.sales_value_incl_tax)) AS Lastordervalue,\n",
        "        SUM(COALESCE(o.order_value_incl_tax_usd, o.sales_value_incl_tax_usd)) AS Lastordervalueusd,\n",
        "        SUM(COALESCE(o.orderquantity, o.salesquantity,0)) AS orderquantity,\n",
        "        concat_ws(';', sort_array(array_distinct(collect_list(o.OrderItems)))) as OrderItems\n",
        "    FROM Orders_with_segmentation o\n",
        "    GROUP BY \n",
        "        o.MparticleUserId,\n",
        "        o.segmentation_date,\n",
        "        o.period,\n",
        "        o.orderid,\n",
        "        o.orderdatetime,\n",
        "        o.TransactionChannel,\n",
        "        o.store,\n",
        "        o.StoreName,\n",
        "        o.PromoId,\n",
        "        o.currency,\n",
        "        o.price_type,\n",
        "        o.IsReturn,\n",
        "        o.customertype,\n",
        "        o.DerivedCustomerType\n",
        "),\n",
        "last_order AS (\n",
        "    SELECT *,\n",
        "        ROW_NUMBER() OVER (\n",
        "            PARTITION BY MparticleUserId, period, segmentation_date, IsReturn\n",
        "            ORDER BY LastOrderDateTime DESC, orderid DESC\n",
        "        ) AS rn\n",
        "    FROM base_data\n",
        ")\n",
        ", final_data as (\n",
        "SELECT * \n",
        "FROM last_order\n",
        "WHERE rn = 1\n",
        ")\n",
        "select s.* , coalesce(r.orderquantity,0) as returnorderquantity\n",
        "from final_data s \n",
        "left join final_data r on s.orderid = r.orderid \n",
        "                and s.segmentation_Date = r.segmentation_date \n",
        "                and s.period = r.period                 \n",
        "                and r.isReturn = 1\n",
        "                and s.MparticleUserId = r.MparticleUserId\n",
        "where s.isreturn = 0 \n",
        "\"\"\")\n",
        "\n",
        "last_order_df.createOrReplaceTempView(\"last_order\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Total Orders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "total_order_df = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        o.MparticleUserId,\n",
        "        o.segmentation_date,\n",
        "        o.period,        \n",
        "        SUM(case when ISReturn = 0 then COALESCE(o.order_value_incl_tax_gbp, o.sales_value_incl_tax_gbp,0) else 0 end) AS TotalordervalueGBP,\n",
        "        SUM(case when ISReturn = 0 then COALESCE(o.order_value_incl_tax_usd, o.sales_value_incl_tax_usd,0)  else 0 end) AS Totalordervalueusd,\n",
        "        SUM(case when ISReturn = 0 then COALESCE(o.sales_value_incl_tax_gbp,0) else 0 end) AS TotalSalesvalueGBP,\n",
        "        SUM(case when ISReturn = 0 then COALESCE(o.sales_value_incl_tax_usd,0) else 0 end) AS TotalSalesvalueusd,\n",
        "        SUM(COALESCE(o.orderquantity, o.salesquantity)) AS TotalSalesQuantity,\n",
        "        SUM(o.salesquantity) AS TotalOrderQuantity,\n",
        "        SUM(case when ISReturn = 0 then COALESCE(o.order_value_incl_tax_gbp, o.sales_value_incl_tax_gbp,0) else 0 end) AS CustomerLifeTimeValueGBP,\n",
        "        SUM(case when ISReturn = 0 then COALESCE(o.order_value_incl_tax_usd, o.sales_value_incl_tax_usd,0) else 0 end) AS CustomerLifeTimeValueUSD,\n",
        "        sum(case when ISReturn = 1 then coalesce(order_value_incl_tax_gbp,sales_value_incl_tax_gbp,0) end) as totalreturnamountGBP,\n",
        "        sum(case when ISReturn = 1 then coalesce(order_value_incl_tax_usd,sales_value_incl_tax_usd,0) end) as totalreturnamountUSD,\n",
        "        COUNT(DISTINCT CASE WHEN orderdatetime >= DATEADD(YEAR, -1, segmentation_date) THEN orderid END) AS number_of_orders_last_12_month,\n",
        "        COUNT(DISTINCT case when ISReturn = 0 then orderid end) AS TotalOrderCount,\n",
        "        COUNT(DISTINCT case when isreturn = 0 then orderid end) AS TotalSalesCount,\n",
        "        COUNT(DISTINCT case when lower(TransactionChannel) = 'retail' then orderid end) AS TotalOrderCountRetail,\n",
        "        COUNT(DISTINCT case when lower(TransactionChannel) = 'digital' then orderid end) AS TotalOrderCountWeb,\n",
        "        COUNT(DISTINCT case when ISReturn = 0 and lower(TransactionChannel) = 'retail' then orderid end) AS TotalOrderCountRetailSales,\n",
        "        COUNT(DISTINCT case when ISReturn = 0 and lower(TransactionChannel) = 'digital' then orderid end) AS TotalOrderCountWebSales,\n",
        "        min(case when lower(TransactionChannel) = 'retail' then cast(orderdatetime as date) end) as firstretailorderdate,\n",
        "        min(case when lower(TransactionChannel) = 'digital' then cast(orderdatetime as date) end) as firstweborderdate,\n",
        "        max(EffectiveFrom) as EffectiveFrom\n",
        "\n",
        "\n",
        "    FROM Orders_with_segmentation o\n",
        "    GROUP BY \n",
        "        o.MparticleUserId,\n",
        "        o.segmentation_date,\n",
        "        o.period\n",
        "\"\"\")\n",
        "\n",
        "total_order_df.createOrReplaceTempView(\"total_order\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Percentage discount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "editable": true,
        "run_control": {
          "frozen": false
        }
      },
      "source": [
        "pct_disc_df = spark.sql(\"\"\"\n",
        "WITH base_data AS (\n",
        "  SELECT \n",
        "    MparticleUserId,\n",
        "    segmentation_date,\n",
        "    period,\n",
        "    orderdatetime,\n",
        "    sales_original_rrp_value_incl_tax_gbp,\n",
        "    sales_pos_rrp_value_incl_tax_gbp,\n",
        "    CASE \n",
        "      WHEN sales_original_rrp_value_incl_tax_gbp - sales_pos_rrp_value_incl_tax_gbp > 0 THEN 1 \n",
        "      ELSE 0 \n",
        "    END AS is_discounted\n",
        "  FROM Order_line_segmentation\n",
        "),\n",
        "windowed_data AS (\n",
        "  SELECT *,\n",
        "         SUM(is_discounted) OVER (PARTITION BY MparticleUserId) AS numberOfDiscountedItems,\n",
        "         COUNT(is_discounted) OVER (PARTITION BY MparticleUserId) AS numberOfItems,\n",
        "         ROW_NUMBER() OVER (\n",
        "           PARTITION BY MparticleUserId, period, segmentation_date \n",
        "           ORDER BY orderdatetime ASC\n",
        "         ) AS rn\n",
        "  FROM base_data\n",
        ")\n",
        "SELECT \n",
        "  MparticleUserId,\n",
        "  segmentation_date,\n",
        "  period,\n",
        "  orderdatetime,\n",
        "  sales_original_rrp_value_incl_tax_gbp,\n",
        "  sales_pos_rrp_value_incl_tax_gbp,\n",
        "  is_discounted,\n",
        "  numberOfDiscountedItems,\n",
        "  numberOfItems,\n",
        "  numberOfDiscountedItems / COALESCE(numberOfItems, 1) AS ratio\n",
        "FROM windowed_data\n",
        "WHERE rn = 1\n",
        "\"\"\")\n",
        "\n",
        "pct_disc_df.createOrReplaceTempView(\"pct_disc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Fav Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "fav_cat_df = spark.sql(\"\"\"\n",
        "WITH ranked_data AS (\n",
        "  SELECT \n",
        "    MparticleUserID,\n",
        "    segmentation_date,\n",
        "    period,\n",
        "    category,\n",
        "    orderdatetime,\n",
        "    SUM(salesquantity) as sales_quntity,\n",
        "    SUM(order_value_incl_tax_gbp) AS sum_of_order_value,\n",
        "    COUNT(orderid) AS cnt_order,\n",
        "    ROW_NUMBER() OVER (\n",
        "      PARTITION BY MparticleUserID, period, segmentation_date\n",
        "      ORDER BY \n",
        "        SUM(salesquantity) DESC,\n",
        "        SUM(order_value_incl_tax_gbp) DESC,\n",
        "        orderdatetime DESC,\n",
        "        category DESC\n",
        "    ) AS rn\n",
        "  FROM order_line_segmentation\n",
        "  WHERE category IS NOT NULL\n",
        "  and orderdatetime > add_months(segmentation_date, -12)\n",
        "  GROUP BY \n",
        "    MparticleUserID,\n",
        "    segmentation_date,\n",
        "    period,\n",
        "    category,\n",
        "    orderdatetime\n",
        ")\n",
        "SELECT *\n",
        "FROM ranked_data\n",
        "WHERE rn = 1\n",
        "\"\"\")\n",
        "\n",
        "fav_cat_df.createOrReplaceTempView(\"fav_cat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Fav SubCategory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "fav_subcat_df = spark.sql(\"\"\"\n",
        "WITH ranked_data AS (\n",
        "  SELECT \n",
        "    MparticleUserID,\n",
        "    segmentation_date,\n",
        "    period,\n",
        "    Subcategory,\n",
        "    orderdatetime,\n",
        "    SUM(salesquantity) as sales_quntity,\n",
        "    SUM(order_value_incl_tax_gbp) AS sum_of_order_value,\n",
        "    COUNT(orderid) AS cnt_order,\n",
        "    ROW_NUMBER() OVER (\n",
        "      PARTITION BY MparticleUserID, period, segmentation_date\n",
        "      ORDER BY \n",
        "        SUM(salesquantity) DESC,\n",
        "        SUM(order_value_incl_tax_gbp) DESC,\n",
        "        orderdatetime DESC,\n",
        "        Subcategory DESC\n",
        "    ) AS rn\n",
        "  FROM order_line_segmentation\n",
        "  WHERE Subcategory IS NOT NULL\n",
        "  and orderdatetime > add_months(segmentation_date, -12)\n",
        "  GROUP BY \n",
        "    MparticleUserID,\n",
        "    segmentation_date,\n",
        "    period,\n",
        "    Subcategory,\n",
        "    orderdatetime\n",
        ")\n",
        "SELECT *\n",
        "FROM ranked_data\n",
        "WHERE rn = 1\n",
        "\"\"\")\n",
        "\n",
        "fav_subcat_df.createOrReplaceTempView(\"fav_subcat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Fav Colour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "fav_color_df = spark.sql(\"\"\"\n",
        "WITH ranked_data AS (\n",
        "  SELECT \n",
        "    MparticleUserID,\n",
        "    segmentation_date,\n",
        "    period,\n",
        "    Color,\n",
        "    orderdatetime,\n",
        "    SUM(salesquantity) as sales_quntity,\n",
        "    SUM(order_value_incl_tax_gbp) AS sum_of_order_value,\n",
        "    COUNT(orderid) AS cnt_order,\n",
        "    ROW_NUMBER() OVER (\n",
        "      PARTITION BY MparticleUserID, period, segmentation_date\n",
        "      ORDER BY \n",
        "        SUM(salesquantity) DESC,\n",
        "        SUM(order_value_incl_tax_gbp) DESC,\n",
        "        orderdatetime DESC,\n",
        "        Color DESC\n",
        "    ) AS rn\n",
        "  FROM order_line_segmentation\n",
        "  WHERE Color IS NOT NULL\n",
        "  and orderdatetime > add_months(segmentation_date, -12)\n",
        "  GROUP BY \n",
        "    MparticleUserID,\n",
        "    segmentation_date,\n",
        "    period,\n",
        "    Color,\n",
        "    orderdatetime\n",
        ")\n",
        "SELECT *\n",
        "FROM ranked_data\n",
        "WHERE rn = 1\n",
        "\"\"\")\n",
        "\n",
        "fav_color_df.createOrReplaceTempView(\"fav_color\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Fav Hero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "fav_hero_df = spark.sql(\"\"\"\n",
        "WITH ranked_data AS (\n",
        "  SELECT \n",
        "    MparticleUserID,\n",
        "    segmentation_date,\n",
        "    period,\n",
        "    hero,\n",
        "    orderdatetime,\n",
        "    SUM(salesquantity) as sales_quntity,\n",
        "    SUM(order_value_incl_tax_gbp) AS sum_of_order_value,\n",
        "    COUNT(orderid) AS cnt_order,\n",
        "    ROW_NUMBER() OVER (\n",
        "      PARTITION BY MparticleUserID, period, segmentation_date\n",
        "      ORDER BY \n",
        "        SUM(salesquantity) DESC,\n",
        "        SUM(order_value_incl_tax_gbp) DESC,\n",
        "        orderdatetime DESC,\n",
        "        hero DESC\n",
        "    ) AS rn\n",
        "  FROM order_line_segmentation\n",
        "  WHERE hero IS NOT NULL\n",
        "  and orderdatetime > add_months(segmentation_date, -12)\n",
        "  GROUP BY \n",
        "    MparticleUserID,\n",
        "    segmentation_date,\n",
        "    period,\n",
        "    hero,\n",
        "    orderdatetime\n",
        ")\n",
        "SELECT *\n",
        "FROM ranked_data\n",
        "WHERE rn = 1\n",
        "\"\"\")\n",
        "\n",
        "fav_hero_df.createOrReplaceTempView(\"fav_hero\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Fav Legging Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "fav_leg_df = spark.sql(\"\"\"\n",
        "WITH ranked_data AS (\n",
        "  SELECT \n",
        "    MparticleUserID,\n",
        "    segmentation_date,\n",
        "    period,\n",
        "    LeggingSize,\n",
        "    orderdatetime,\n",
        "    SUM(salesquantity) as sales_quntity,\n",
        "    SUM(order_value_incl_tax_gbp) AS sum_of_order_value,\n",
        "    COUNT(orderid) AS cnt_order,\n",
        "    ROW_NUMBER() OVER (\n",
        "      PARTITION BY MparticleUserID, period, segmentation_date\n",
        "      ORDER BY \n",
        "        SUM(salesquantity) DESC,\n",
        "        SUM(order_value_incl_tax_gbp) DESC,\n",
        "        orderdatetime DESC,\n",
        "        LeggingSize DESC\n",
        "    ) AS rn\n",
        "  FROM order_line_segmentation\n",
        "  WHERE LeggingSize IS NOT NULL\n",
        "  and orderdatetime > add_months(segmentation_date, -12)\n",
        "  GROUP BY \n",
        "    MparticleUserID,\n",
        "    segmentation_date,\n",
        "    period,\n",
        "    LeggingSize,\n",
        "    orderdatetime\n",
        ")\n",
        "SELECT *\n",
        "FROM ranked_data\n",
        "WHERE rn = 1\n",
        "\"\"\")\n",
        "\n",
        "fav_leg_df.createOrReplaceTempView(\"fav_leg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Fav source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "fav_source_df = spark.sql(\"\"\"\n",
        "WITH ranked_data AS (\n",
        "  SELECT \n",
        "    MparticleUserID,\n",
        "    segmentation_date,\n",
        "    period,\n",
        "    TransactionChannel,\n",
        "    orderdatetime,\n",
        "    SUM(salesquantity) as sales_quntity,\n",
        "    SUM(order_value_incl_tax_gbp) AS sum_of_order_value,\n",
        "    COUNT(orderid) AS cnt_order,\n",
        "    ROW_NUMBER() OVER (\n",
        "      PARTITION BY MparticleUserID, period, segmentation_date\n",
        "      ORDER BY \n",
        "        SUM(salesquantity) DESC,\n",
        "        SUM(order_value_incl_tax_gbp) DESC,\n",
        "        orderdatetime DESC,\n",
        "        TransactionChannel DESC\n",
        "    ) AS rn\n",
        "  FROM order_line_segmentation\n",
        "  WHERE TransactionChannel IS NOT NULL\n",
        "  and orderdatetime > add_months(segmentation_date, -12)\n",
        "  GROUP BY \n",
        "    MparticleUserID,\n",
        "    segmentation_date,\n",
        "    period,\n",
        "    TransactionChannel,\n",
        "    orderdatetime\n",
        ")\n",
        "SELECT *\n",
        "FROM ranked_data\n",
        "WHERE rn = 1\n",
        "\"\"\")\n",
        "\n",
        "fav_source_df.createOrReplaceTempView(\"fav_source\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Fav Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "fav_store_df = spark.sql(\"\"\"\n",
        "WITH ranked_data AS (\n",
        "  SELECT \n",
        "    MparticleUserID,\n",
        "    segmentation_date,\n",
        "    period,\n",
        "    Store,\n",
        "    StoreName,\n",
        "    orderdatetime,\n",
        "    SUM(salesquantity) as sales_quntity,\n",
        "    SUM(order_value_incl_tax_gbp) AS sum_of_order_value,\n",
        "    COUNT(orderid) AS cnt_order,\n",
        "    ROW_NUMBER() OVER (\n",
        "      PARTITION BY MparticleUserID, period, segmentation_date\n",
        "      ORDER BY \n",
        "        SUM(salesquantity) DESC,\n",
        "        SUM(order_value_incl_tax_gbp) DESC,\n",
        "        orderdatetime DESC,\n",
        "        Store DESC\n",
        "    ) AS rn\n",
        "  FROM order_line_segmentation\n",
        "  WHERE Store IS NOT NULL\n",
        "  and orderdatetime > add_months(segmentation_date, -12)\n",
        "  GROUP BY \n",
        "    MparticleUserID,\n",
        "    segmentation_date,\n",
        "    period,\n",
        "    Store,\n",
        "    StoreName,\n",
        "    orderdatetime\n",
        ")\n",
        "SELECT *\n",
        "FROM ranked_data\n",
        "WHERE rn = 1\n",
        "\"\"\")\n",
        "\n",
        "fav_store_df.createOrReplaceTempView(\"fav_store\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### FP or Sale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "fp_sale_df = spark.sql(\"\"\"\n",
        "SELECT *\n",
        "FROM (\n",
        "    SELECT \n",
        "        MparticleUserId,\n",
        "        segmentation_date,\n",
        "        period,\n",
        "        orderdatetime,\n",
        "        COUNT(*) AS lines,\n",
        "        CASE \n",
        "            WHEN SUM(CASE WHEN discountflag = 'Y' THEN 1 ELSE 0 END) / COALESCE(COUNT(*), 1) > 0.5 THEN 'Sale'\n",
        "            ELSE 'Full price'\n",
        "        END AS fp_or_sale,\n",
        "        ROW_NUMBER() OVER (\n",
        "            PARTITION BY MparticleUserId, period, segmentation_date\n",
        "            ORDER BY orderdatetime ASC\n",
        "        ) AS rn\n",
        "    FROM order_line_segmentation\n",
        "    WHERE orderid IS NOT NULL\n",
        "    and isreturn = 0\n",
        "    GROUP BY MparticleUserId, segmentation_date, period, orderdatetime\n",
        ") tmp\n",
        "WHERE rn = 1\n",
        "\"\"\")\n",
        "\n",
        "fp_sale_df.createOrReplaceTempView(\"fp_sale\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Segmentaion Final Joins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "segmentation_prep_df = spark.sql(\"\"\"\n",
        "SELECT   \n",
        "        O.Segmentation_date,\n",
        "        O.period,\n",
        "        c.MparticleUserKey,\n",
        "        c.GlobalCustomerKey,\n",
        "        c.Email,\n",
        "        l.customertype,\n",
        "        case when coalesce(t.TotalOrderCount,0) = 0 then 'Registered'\n",
        "                when t.TotalOrderCount = 1 then 'New'\n",
        "                when t.TotalOrderCount > 1 then 'Returning'\n",
        "          end as       DerivedCustomerType,\n",
        "        FirstOrderSource,\n",
        "        cast(FirstOrderDate as timestamp) as FirstOrderDate,\n",
        "        FirstOrderCurrency,\n",
        "        Firstordervalue,\n",
        "        Cast(Firstordervalueusd as decimal(38,3)) as Firstordervalueusd,\n",
        "        Firstordervaluegbp,\n",
        "        FIRST_ORDER_PRICE_TYPE as FirstOrderPriceType,\n",
        "        f.OrderQuantity as FirstOrderNumberOfItemsGross,\n",
        "        f.OrderQuantity - abs(f.returnorderquantity) as FirstOrderNumberOfItemsNet,\n",
        "        f.OrderItems as FirstOrderItems,\n",
        "        F.OrderCategories AS FirstOrderCategories,\n",
        "        f.Store as FirstOrderStore,\n",
        "        cast(F.StoreName as varchar(200)) as FirstOrderStoreName,\n",
        "        f.ShippingCountryCode,\n",
        "\n",
        "        SecondOrderSource,\n",
        "        cast(SecondOrderDate as timestamp) as SecondOrderDate,\n",
        "        SecondOrderCurrency,\n",
        "        Secondordervalue,\n",
        "        Cast(Secondordervalueusd as decimal(38,3)) as Secondordervalueusd,\n",
        "        Secondordervaluegbp,\n",
        "        Second_ORDER_PRICE_TYPE as SecondOrderPriceType,\n",
        "        s.store as SecondOrderStore,\n",
        "        cast(F.StoreName as varchar(200)) as SecondOrderStoreName,\n",
        "        s.OrderQuantity as SecondOrderNumberOfItemsGross,\n",
        "        s.OrderQuantity - abs(s.returnorderquantity) as SecondOrderNumberOfItemsNet,\n",
        "        s.OrderItems as SecondOrderItems,\n",
        "        s.OrderCategories AS SecondOrderCategories,\n",
        "\n",
        "        LastOrderSource,\n",
        "        cast(LastOrderDateTime as timestamp) as LastOrderDate,\n",
        "        LastOrderCurrency,\n",
        "        Lastordervalue,\n",
        "        Cast(Lastordervalueusd as decimal(38,3)) as Lastordervalueusd,\n",
        "        Lastordervaluegbp,\n",
        "        Last_ORDER_PRICE_TYPE as LastOrderPriceType,\n",
        "        l.store as LastOrderStore,\n",
        "        cast(L.StoreName as varchar(200)) as LastOrderStoreName,\n",
        "        l.OrderQuantity as LastOrderNumberOfItemsGross,\n",
        "        l.OrderQuantity - coalesce(abs(l.returnorderquantity),0) as LastOrderNumberOfItemsNet,\n",
        "        l.OrderItems as LastOrderItems,\n",
        "\n",
        "        t.TotalordervalueGBP,\n",
        "        Cast(t.Totalordervalueusd as decimal(38,3)) as Totalordervalueusd,\n",
        "        t.TotalSalesvalueGBP,\n",
        "        Cast(t.TotalSalesvalueusd as decimal(38,3)) as TotalSalesvalueusd,\n",
        "        t.TotalSalescount,\n",
        "        t.TotalOrdercount,\n",
        "        t.totalreturnamountGBP,\n",
        "        t.totalreturnamountUSD,\n",
        "        t.number_of_orders_last_12_month as NumberOfOrdersLast12Months,\n",
        "        coalesce(t.CustomerLifeTimeValueGBP,0) as CustomerLifeTimeValueGBP,\n",
        "        Cast(coalesce(t.CustomerLifeTimeValueUSD,0) as decimal(38,3)) as CustomerLifeTimeValueUSD,\n",
        "        cast((coalesce(t.TotalSalesvalueGBP,0) + coalesce(t.totalreturnamountGBP,0)) as decimal(38,3)) as NetLifeTimeValueGBP,\n",
        "        cast((coalesce(t.TotalSalesvalueUSD,0) + coalesce(t.totalreturnamountUSD,0)) as decimal(38,3)) as NetLifeTimeValueUSD,\n",
        "        cast(round(t.TotalordervalueGBP/t.TotalOrdercount,3) as decimal(38,3)) as AverageOrderValueGBP,\n",
        "        cast(round(t.TotalordervalueUSD/t.TotalOrdercount,3) as decimal(38,3)) as AverageOrderValueUSD,\n",
        "\n",
        "        t.TotalOrderCountRetail,\n",
        "        cast(t.firstretailorderdate as date) as firstretailorderdate,\n",
        "        cast(t.firstweborderdate as date) as firstweborderdate,\n",
        "\n",
        "        case when  coalesce(t.TotalSalesCount,0) > 0 then  cast(TotalOrderCountRetail/t.TotalSalesCount as numeric(38,2)) end as PercetageStoreOrders,\n",
        "        CASE \n",
        "                WHEN LastOrderDateTime BETWEEN add_months(o.segmentation_date, -12) AND o.segmentation_date THEN 'Active'\n",
        "                WHEN LastOrderDateTime BETWEEN add_months(o.segmentation_date, -24) AND add_months(o.segmentation_date, -12) THEN 'Lapsed'\n",
        "                WHEN LastOrderDateTime < add_months(o.segmentation_date, -24) THEN 'Dormant'\n",
        "        END AS recency,\n",
        "        case when TotalOrderCount = 1 then 'Single'\n",
        "                when TotalOrderCount > 1 then 'Multi'\n",
        "        end as frequency,\n",
        "        case when TotalOrderCountRetailSales > 0 and TotalOrderCountWebSales = 0 then 'Retail'\n",
        "                 when TotalOrderCountRetailSales = 0 and TotalOrderCountWebSales > 0 then 'Digital'\n",
        "                 when TotalOrderCountRetailSales > 0 and TotalOrderCountWebSales > 0 then 'Omnichannel'\n",
        "                 else 'No orders/unknown'\n",
        "            end as ChannelsShopped,\n",
        "        \n",
        "        fps.fp_or_sale as NewCustomerAcquiredFPorSale,\n",
        "\n",
        "        cat.Category as FavouriteCategory ,\n",
        "        scat.SubCategory as FavouriteSubCategory,\n",
        "        color.color as FavouriteColor,\n",
        "        fs.TransactionChannel as FavouriteSource,\n",
        "        i.total_distinct_items as TotalDistinctItems,\n",
        "        i.NumberOfCategories,\n",
        "        cast(pd.ratio as decimal(38,2)) as PercentageDiscountItems,\n",
        "        st.Store as FavouriteStore,\n",
        "        st.StoreName as FavouriteStoreName,\n",
        "        \n",
        "        f.PromoId as FirstOrderPromoId,\n",
        "        cast(null as varchar(200))  as FirstOrderStyles,\n",
        "        s.PromoId as SecondOrderPromoID,\n",
        "        cast(null as varchar(200))  as SecondOrderStyles,\n",
        "        l.PromoId  as LastOrderPromoID,\n",
        "        cast(null as varchar(200))  as LastOrderStyles,\n",
        "        cast(null as varchar(200))  as FavouriteColourType,\n",
        "        cast(h.hero as varchar(200))  as FavouriteHero,\n",
        "        cast(leg.LeggingSize as varchar(200))  as FavouriteLeggingSize,\n",
        "        fp.CUSTOMER_FIRST_ORDER_PRICE_TYPE_V2 ,\n",
        "        coalesce(pt.CUSTOMER_12M_ROLLING_PRICETYPE,'INACTIVE') as  CUSTOMER_12M_ROLLING_PRICETYPE,\n",
        "        t.EffectiveFrom\n",
        "\n",
        "FROM customers c\n",
        "join seg_periods o  on o.segmentation_date between to_timestamp(c.validfrom) and to_timestamp(c.validto)\n",
        "LEFT JOIN First_Order f on c.MparticleUserKey = f.MParticleUserId and f.FirstOrderDate <= o.segmentation_date\n",
        "LEFT JOIN second_order s on c.MparticleUserKey = s.MParticleUserId and s.SecondOrderDate <= o.segmentation_date\n",
        "LEFT JOIN last_order l on c.MparticleUserKey = l.MparticleUserId and l.segmentation_date = o.segmentation_date and o.period = l.period\n",
        "LEFT JOIN total_order t on c.MparticleUserKey = t.MparticleUserId and t.segmentation_date = o.segmentation_date and o.period = t.period\n",
        "LEFT JOIN fav_cat cat on c.MparticleUserKey = cat.MparticleUserId and cat.segmentation_date = o.segmentation_date and o.period = cat.period\n",
        "LEFT JOIN fav_subcat scat on c.MparticleUserKey = scat.MparticleUserId and scat.segmentation_date = o.segmentation_date and o.period = scat.period\n",
        "LEFT JOIN fav_color color on c.MparticleUserKey = color.MparticleUserId and color.segmentation_date = o.segmentation_date and o.period = color.period\n",
        "LEFT JOIN fav_hero h on c.MparticleUserKey = h.MparticleUserId and h.segmentation_date = o.segmentation_date and o.period = h.period\n",
        "LEFT JOIN fav_leg leg on c.MparticleUserKey = leg.MparticleUserId and leg.segmentation_date = o.segmentation_date and o.period = leg.period\n",
        "LEFT JOIN fav_source fs on c.MparticleUserKey = fs.MparticleUserId and fs.segmentation_date = o.segmentation_date and o.period = fs.period\n",
        "LEFT JOIN order_items i on c.MparticleUserKey = i.MparticleUserId and i.segmentation_date = o.segmentation_date and o.period = i.period\n",
        "LEFT JOIN pct_disc pd on c.MparticleUserKey = pd.MparticleUserId and pd.segmentation_date = o.segmentation_date and o.period = pd.period\n",
        "LEFT JOIN fav_store st on c.MparticleUserKey = st.MparticleUserId and st.segmentation_date = o.segmentation_date and o.period = st.period\n",
        "LEFT JOIN fp_sale fps on c.MparticleUserKey = fps.MparticleUserId and fps.segmentation_date = o.segmentation_date and o.period = fps.period\n",
        "LEFT JOIN first_order_pt_df fp on c.MparticleUserKey = fp.MparticleUserId and fp.OrderDateTime <= o.segmentation_date\n",
        "LEFT JOIN cust_12m_rolling_pricetype_df pt on c.MparticleUserKey = pt.MparticleUserId and pt.segmentation_date = o.segmentation_date and o.period = pt.period\n",
        "\"\"\")\n",
        "\n",
        "segmentation_prep_df.createOrReplaceTempView(\"segmentation_prep\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Aggregate Derivations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "editable": true,
        "run_control": {
          "frozen": false
        }
      },
      "source": [
        "segmentation_prep2_df = spark.sql(\"\"\"\n",
        "SELECT       \n",
        "    *,\n",
        "    CONCAT(recency, ' ', frequency) AS lifecycle_stage,\n",
        "    cast(case when coalesce(o.TotalSalescount,0) > 0 then o.TotalDistinctItems / o.TotalSalescount  end as decimal(38,2))  as average_order_items,\n",
        "    CASE WHEN DATEDIFF(SecondOrderDate, FirstOrderDate) <= 30 THEN 1 ELSE 0 END as RR_30Days_Flag,\n",
        "    CASE WHEN DATEDIFF(SecondOrderDate, FirstOrderDate) <= 90 THEN 1 ELSE 0 END as RR_90Days_Flag,\n",
        "    CASE WHEN DATEDIFF(SecondOrderDate, FirstOrderDate) <= 180 THEN 1 ELSE 0 END as RR_180Days_Flag,\n",
        "    CASE WHEN DATEDIFF(SecondOrderDate, FirstOrderDate) <= 360 THEN 1 ELSE 0 END as RR_360Days_Flag\n",
        "FROM segmentation_prep o\n",
        "\"\"\")\n",
        "segmentation_prep2_df.createOrReplaceTempView(\"segmentation_prep2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### hashdiff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "segmentation_df = spark.sql(\"\"\"\n",
        "SELECT *,\n",
        "       CAST(\n",
        "         md5(\n",
        "           concat_ws('||',\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(Email AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(customertype AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(DerivedCustomerType AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FirstOrderSource AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FirstOrderDate AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FirstOrderCurrency AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(Firstordervalue AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(Firstordervalueusd AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(Firstordervaluegbp AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FirstOrderPriceType AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FirstOrderNumberOfItemsGross AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FirstOrderNumberOfItemsNet AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FirstOrderItems AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(NewCustomerAcquiredFPorSale AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FirstOrderCategories AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FirstOrderStore AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FirstOrderStoreName AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(ShippingCountryCode AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(SecondOrderSource AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(SecondOrderDate AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(SecondOrderCurrency AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(Secondordervalue AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(Secondordervalueusd AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(Secondordervaluegbp AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(SecondOrderPriceType AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(SecondOrderStore AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(SecondOrderStoreName AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(SecondOrderNumberOfItemsGross AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(SecondOrderNumberOfItemsNet AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(SecondOrderItems AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(SecondOrderCategories AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(LastOrderSource AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(LastOrderDate AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(LastOrderCurrency AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(Lastordervalue AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(Lastordervalueusd AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(Lastordervaluegbp AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(LastOrderPriceType AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(LastOrderStore AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(LastOrderStoreName AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(LastOrderNumberOfItemsGross AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(LastOrderNumberOfItemsNet AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(LastOrderItems AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(TotalordervalueGBP AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(Totalordervalueusd AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(TotalSalesvalueGBP AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(TotalSalesvalueusd AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(TotalSalescount AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(TotalOrdercount AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(totalreturnamountGBP AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(totalreturnamountUSD AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(NumberOfOrdersLast12Months AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(CustomerLifeTimeValueGBP AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(CustomerLifeTimeValueUSD AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(NetLifeTimeValueGBP AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(NetLifeTimeValueUSD AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(AverageOrderValueGBP AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(AverageOrderValueUSD AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(TotalOrderCountRetail AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(firstretailorderdate AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(firstweborderdate AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(PercetageStoreOrders AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(recency AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(frequency AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(ChannelsShopped AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FavouriteCategory AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FavouriteSubCategory AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FavouriteColor AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FavouriteSource AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(TotalDistinctItems AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(NumberOfCategories AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(PercentageDiscountItems AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FavouriteStore AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FavouriteStoreName AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FirstOrderPromoId AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FirstOrderStyles AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(SecondOrderPromoID AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(SecondOrderStyles AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(LastOrderPromoID AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(LastOrderStyles AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FavouriteColourType AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FavouriteHero AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(FavouriteLeggingSize AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(lifecycle_stage AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(average_order_items AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(CUSTOMER_FIRST_ORDER_PRICE_TYPE_V2 AS STRING))), ''), '^^'),\n",
        "\t\t\t\tCOALESCE(NULLIF(UPPER(TRIM(CAST(CUSTOMER_12M_ROLLING_PRICETYPE AS STRING))), ''), '^^')\n",
        "           )\n",
        "         ) AS STRING\n",
        "       ) AS customermetricshashdiff\n",
        "FROM segmentation_prep2\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Write result into Gold Delta Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "editable": true,
        "run_control": {
          "frozen": false
        }
      },
      "source": [
        "segmentation_df.write.format(\"delta\").mode(\"append\").partitionBy(\"period\", \"Segmentation_date\").save(target_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Exit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "mssparkutils.notebook.exit(\"0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Miscellaneous ad hoc code cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import min, max, count\n",
        "\n",
        "#segmentation_periods_df.select(min(\"segmentation_date\").alias(\"min_date\"), max(\"segmentation_date\").alias(\"max_date\")).show()\n",
        "last_order_df.groupBy(\"period\",\"Segmentation_date\").agg(count(\"*\").alias(\"total_count\")).show()\n",
        "\n",
        "## cust_df\n",
        "## result_df\n",
        "## segmentation_periods_df\n",
        "## order_rank_df\n",
        "## orders_df\n",
        "## total_order_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = first_order_df.filter(last_order_df[\"MparticleUserId\"] == \"-110968071520021894\")\n",
        "df.show(15,truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.sql(\"\"\"\n",
        "    SELECT *,\n",
        "            ROW_NUMBER() OVER(PARTITION BY MparticleUserid ORDER BY cast(OrderDateTime as timestamp), regexp_replace(OrderId, '[^0-9]', '')) AS ORDER_SEQ2\n",
        "    FROM order_rank_df\n",
        "    WHERE MparticleUserid = '752429565253543102'\n",
        "\"\"\")\n",
        "df.show(30,truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "segmentation_df.createOrReplaceTempView(\"segmentation_df\")\n",
        "\n",
        "df = spark.sql(\"\"\"\n",
        "    SELECT MparticleUserKey,\n",
        "            GlobalCustomerKey,\n",
        "            Segmentation_date,\n",
        "            Period,\n",
        "            customermetricshashdiff\n",
        "    FROM segmentation_df c\n",
        "    WHERE c.MparticleUserKey = '2827407266731948222'\n",
        "    order by segmentation_date\n",
        "\"\"\")\n",
        "df.show(30,truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "##segmentation_df.createOrReplaceTempView(\"segmentation_df\")\n",
        "\n",
        "df = spark.sql(\"\"\"\n",
        "    SELECT MparticleUserKey,\n",
        "            GlobalCustomerKey,\n",
        "            Segmentation_date,\n",
        "            Period,\n",
        "            ValidFrom,\n",
        "            ValidTo\n",
        "    FROM customers c\n",
        "    join seg_periods o  on o.segmentation_date between to_timestamp(c.validfrom) and to_timestamp(c.validto)\n",
        "    WHERE c.MparticleUserKey = '2827407266731948222'\n",
        "    order by segmentation_date, ValidFrom\n",
        "\"\"\")\n",
        "df.show(30,truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "segmentation_df.createOrReplaceTempView(\"segmentation_df\")\n",
        "\n",
        "df = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "            Segmentation_date,\n",
        "            Period,\n",
        "            count(*)\n",
        "    FROM segmentation_prep2 c\n",
        "    group by Segmentation_date,\n",
        "            Period       \n",
        "    order by segmentation_date desc\n",
        "\"\"\")\n",
        "df.show(30,truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.sql(\"\"\"\n",
        "SELECT \n",
        "    max(OrderDateTime)\n",
        "FROM    salesunion\n",
        "\"\"\")\n",
        "df.show(10, truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "cust_12m_rolling_pricetype_df.filter(col(\"MparticleUSerId\") == '-1000354369157909164').selectExpr('MparticleUserId','min_OrderDateTime','CUSTOMER_12M_ROLLING_PRICETYPE').show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### delete delta partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "spark.sql(\"\"\"DELETE FROM delta.`abfss://gold@azwwwnonproddevadapadls.dfs.core.windows.net/CustomerSegmentation/CustomerSegmentationHistory/`\n",
        "where period = 'weekly' and segmentation_date > '2025-07-01'\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}