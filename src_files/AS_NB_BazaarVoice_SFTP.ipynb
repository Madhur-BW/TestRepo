{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Pull and Process files from BazaarVoice SFTP Site"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Initialize Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# We will need an SFTP package along with many of the usuals\r\n",
        "from delta import *\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType, LongType\r\n",
        "from datetime import datetime\r\n",
        "import json\r\n",
        "from azure.storage.blob import BlobServiceClient\r\n",
        "from notebookutils import mssparkutils\r\n",
        "# SFTP\r\n",
        "import paramiko\r\n",
        "import gzip\r\n",
        "import io\r\n",
        "from pyspark.sql.functions import col, when, array, explode, expr\r\n",
        "import re\r\n",
        "from delta.tables import DeltaTable\r\n",
        "from pyspark.sql.utils import AnalysisException"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Include Common Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Ingest from SFTP to raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Retrieve the SFTP secret from AKV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "connection_info_string = mssparkutils.credentials.getSecret(kv_name, 'bazaar-voice-sftp-connection-info', 'ls_kv_adap')\r\n",
        "connection_info = json.loads(connection_info_string)\r\n",
        "host = connection_info['host']\r\n",
        "user = connection_info['user']\r\n",
        "password = connection_info['password']\r\n",
        "sftp_port = 22\r\n",
        "\r\n",
        "print(f\"SFTP host: {host}\")\r\n",
        "print(f\"SFTP user: {user}\")\r\n",
        "print(f\"SFTP password len: {len(password)}\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Get the Files from the SFTP Server and Save to raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get the last watermark datetime\r\n",
        "process_name = 'BazaarVoiceSFTP'\r\n",
        "# For now, put the watermark delta table under the raw/BazaarVoice folder\r\n",
        "# raw_adls_path\") abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/ \r\n",
        "delta_table_path = f\"{raw_adls_path}BazaarVoice/sftp_watermark\"\r\n",
        "try:\r\n",
        "    watermark_value = get_datetime_watermark_value_for_process(process_name=process_name, delta_table_path=delta_table_path)\r\n",
        "    print(f\"watermark_value: {watermark_value}\")\r\n",
        "except Exception as e:\r\n",
        "    print(\"error=\",str(e))\r\n",
        "    raise Exception(f\"Could not retrieve watermark value for {process_name} at {delta_table_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Just test the watermark functions\r\n",
        "# watermark_value = datetime.now()\r\n",
        "# set_datetime_watermark_value_for_process(process_name, watermark_value, delta_table_path)\r\n",
        "# watermark_value = get_datetime_watermark_value_for_process(process_name, delta_table_path)\r\n",
        "# print(f\"read watermark_value: {watermark_value}\")\r\n",
        "\r\n",
        "# Reset the watermark to 1/1/1900 to get all files in feeds folder that we need.\r\n",
        "# process_name = 'BazaarVoiceSFTP'\r\n",
        "# watermark_value = datetime(1900, 1, 1)\r\n",
        "# set_datetime_watermark_value_for_process(process_name, watermark_value, delta_table_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Connec to the SFTP server and get the files since the last watermark datetime\r\n",
        "transport = None\r\n",
        "sftp = None\r\n",
        "remote_folder = 'feeds'\r\n",
        "\r\n",
        "# Build the relative path for the gzipped XML files, using a simple date partition folder (date=yyyyMMdd)\r\n",
        "today_str = datetime.today().strftime('%Y%m%d')\r\n",
        "todays_blob_folder = f\"/BazaarVoice/date={today_str}\"\r\n",
        "print(f\"todays blob_folder: {todays_blob_folder}\")\r\n",
        "\r\n",
        "# Capture the max file mod time to use for our new watermark\r\n",
        "# Start with a datetime long ago.\r\n",
        "max_file_mod_time = datetime(1900, 1, 1)\r\n",
        "\r\n",
        "try:\r\n",
        "    # Connect to the SFTP server\r\n",
        "    transport = paramiko.Transport((host, sftp_port))\r\n",
        "    transport.connect(username=user, password=password)\r\n",
        "    if(not transport.is_active()):\r\n",
        "        print(\"Transport connection is inactive. SFTP client will not be created.\")\r\n",
        "        transport.close()\r\n",
        "        raise Exception(\"Inactive paramiko transport\")\r\n",
        "    sftp = paramiko.SFTPClient.from_transport(transport)\r\n",
        "    print(f\"Connected to {host}...\")\r\n",
        "\r\n",
        "    # Get the list of files\r\n",
        "    for file_attr in sftp.listdir_attr(remote_folder):\r\n",
        "        #print(dir(file_attr))\r\n",
        "        file_name = file_attr.filename\r\n",
        "        remote_file_path = f\"{remote_folder}/{file_name}\"\r\n",
        "\r\n",
        "        # Filter off unwanted files.  We only want the two *_standard_client_feed_* files.\r\n",
        "        if 'tryzen' in file_name:\r\n",
        "            # Skip the bv_sweatybetty_tryzendevelopment* files.\r\n",
        "            print(f\"Skippping file {file_name}\")\r\n",
        "            continue\r\n",
        "\r\n",
        "        # Get the file's last modified time and see if we need to set the max file mod time for our new watermark\r\n",
        "        file_mod_time = datetime.fromtimestamp(file_attr.st_mtime)\r\n",
        "        if file_mod_time > max_file_mod_time:\r\n",
        "            max_file_mod_time = file_mod_time\r\n",
        "\r\n",
        "        print('-----------------------------------------------------')\r\n",
        "        print(F\"Comparing remote file {file_name} last modified {file_mod_time} to watermark_value {watermark_value}\")\r\n",
        "        if file_mod_time > watermark_value:\r\n",
        "            print(f\"Downloading {remote_file_path}...\")\r\n",
        "            # Download the gz file to raw. We can decompress the gz file on read when processing raw into bronze.\r\n",
        "            with sftp.open(remote_file_path, \"rb\") as remote_file:\r\n",
        "                file_data = remote_file.read()\r\n",
        "\r\n",
        "            local_file_path = f\"{todays_blob_folder}/{file_name}\"\r\n",
        "            print(f\"Saving file {file_name} to {local_file_path}...\")\r\n",
        "\r\n",
        "            # Use the same pattern for BLOB uploading to raw as the StoreTech notebook\r\n",
        "            account_key = mssparkutils.credentials.getSecret(kv_name, 'storage-key', 'ls_kv_adap')\r\n",
        "            blob_service_client = BlobServiceClient(account_url=blob_adls_path, credential=account_key)\r\n",
        "            container_client = blob_service_client.get_container_client(\"raw\")\r\n",
        "            blob_client = container_client.get_blob_client(local_file_path) # (blob_name)\r\n",
        "            blob_client.upload_blob(file_data, overwrite=True)\r\n",
        "\r\n",
        "        else:\r\n",
        "            print(f\"Skipping {file_name} - file mod time is after our watermark.\")\r\n",
        "\r\n",
        "    # Set the new watermark\r\n",
        "    print(f\"SFTP download complete. Setting new watermark: {max_file_mod_time}\")\r\n",
        "    set_datetime_watermark_value_for_process(process_name, max_file_mod_time, delta_table_path)\r\n",
        "except Exception as e:\r\n",
        "    print(\"error=\",str(e))\r\n",
        "    raise Exception(\"Could not connect to SFTP server and get the files.\")\r\n",
        "finally:\r\n",
        "    # Ensure SFTP session is closed\r\n",
        "    if sftp:\r\n",
        "        sftp.close()\r\n",
        "    # Ensure transport is closed\r\n",
        "    if transport:\r\n",
        "        transport.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Bronze - conform to Delta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Helper functions to work with the BazaarVoice folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Function to tell us if a folder exists\r\n",
        "def folder_exists(abfss_path):\r\n",
        "    from notebookutils import mssparkutils\r\n",
        "    try:\r\n",
        "        mssparkutils.fs.ls(abfss_path)\r\n",
        "        return True\r\n",
        "    except:\r\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Function returns a DeltaTable object or None if it doesn't exist\r\n",
        "# This does NOT load the data - it's a lightweight op\r\n",
        "def get_delta_table(delta_table_path):\r\n",
        "    from delta.tables import DeltaTable\r\n",
        "    from pyspark.sql.utils import AnalysisException\r\n",
        "\r\n",
        "    delta_table = None\r\n",
        "    try:\r\n",
        "        delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
        "    except AnalysisException:\r\n",
        "        delta_table = None\r\n",
        "\r\n",
        "    return delta_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Function to get the raw BazaarVoice folders to process - after or equal to the input date string (yyyyMMdd)\r\n",
        "def get_filtered_date_folders(abfss_path, min_date):\r\n",
        "    \"\"\"\r\n",
        "    Lists all folders in the given ABFSS path, filters those that match\r\n",
        "    the pattern \"date=yyyyMMdd\" and are greater than or equal to min_date.\r\n",
        "    \r\n",
        "    :param abfss_path: The ABFSS root path containing date-based folders.\r\n",
        "    :param min_date: The minimum date in \"yyyyMMdd\" format as a string.\r\n",
        "    :return: A sorted list of matching folders.\r\n",
        "    \"\"\"\r\n",
        "    from notebookutils import mssparkutils\r\n",
        "    import re\r\n",
        "    from pyspark.sql.utils import AnalysisException\r\n",
        "    \r\n",
        "    try:\r\n",
        "        # List all items in the given path\r\n",
        "        items = mssparkutils.fs.ls(abfss_path)\r\n",
        "\r\n",
        "        # Define regex pattern for \"date=yyyyMMdd\"\r\n",
        "        date_pattern = re.compile(r\"date=(\\d{8})\")\r\n",
        "\r\n",
        "        # Extract valid date folders that meet the condition\r\n",
        "        valid_folders = [\r\n",
        "            item.name.strip('/') for item in items \r\n",
        "            if (match := date_pattern.match(item.name.strip('/'))) and match.group(1) >= min_date\r\n",
        "        ]\r\n",
        "\r\n",
        "        # Return sorted list of valid folders\r\n",
        "        return sorted(valid_folders)\r\n",
        "\r\n",
        "    except AnalysisException as e:\r\n",
        "        print(f\"Error accessing {abfss_path}: {e}\")\r\n",
        "        return []\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### See what we have already loaded into bronze from raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Do we have data in bronze already for BazaarVoice?\r\n",
        "bv_subfolder = 'BazaarVoice'\r\n",
        "bv_bronze_folder_path = f\"{bronze_adls_path}{bv_subfolder}\"   # abfss://bronze@azwwwnonproddevadapadls.dfs.core.windows.net/\r\n",
        "print(bv_bronze_folder_path)\r\n",
        "\r\n",
        "# Does this main BazzarVoice folder path exist? e.g. abfss://bronze@azwwwnonproddevadapadls.dfs.core.windows.net/BazaarVoice\r\n",
        "if folder_exists(bv_bronze_folder_path):\r\n",
        "    print(f\"The {bv_subfolder} subfolder exists in {bronze_adls_path}\")\r\n",
        "    first_time_loading_to_bronze = False\r\n",
        "else:\r\n",
        "    print(f\"Nope - The {bv_subfolder} subfolder does NOT exist in {bronze_adls_path}\")\r\n",
        "    first_time_loading_to_bronze = True\r\n",
        "\r\n",
        "# If we already have BV bronze folder, see if we have our tables\r\n",
        "# We want to use the max date loaded into the bronze delta tables (product_reviews and product_ratings)\r\n",
        "bronze_reviews_delta_path = f\"{bv_bronze_folder_path}/product_reviews\"\r\n",
        "bronze_ratings_delta_path = f\"{bv_bronze_folder_path}/product_ratings\"\r\n",
        "\r\n",
        "bronze_reviews_delta_table = get_delta_table(bronze_reviews_delta_path)\r\n",
        "bronze_ratings_delta_table = get_delta_table(bronze_ratings_delta_path)\r\n",
        "\r\n",
        "if first_time_loading_to_bronze:\r\n",
        "    # Create the BazaarVoice subfolder in bronze\r\n",
        "    mssparkutils.fs.mkdirs(bv_bronze_folder_path)\r\n",
        "    # Set our watermark date strings to well before to get ALL files from all folders into bronze.\r\n",
        "    \r\n",
        "    max_reviews_ingested_at_str = '19000101'\r\n",
        "    max_reviews_ingested_at = datetime.strptime(max_reviews_ingested_at_str, \"%Y%m%d\")\r\n",
        "    max_ratings_ingested_at_str = '19000101'\r\n",
        "    max_ratings_ingested_at = datetime.strptime(max_ratings_ingested_at_str, \"%Y%m%d\")\r\n",
        "\r\n",
        "if bronze_reviews_delta_table:\r\n",
        "    # Get the latest load date (ingested_at is a datetime, but we can get the date from it)\r\n",
        "    max_reviews_ingested_at = (\r\n",
        "        bronze_reviews_delta_table\r\n",
        "        .toDF()  # Convert DeltaTable to a Spark DataFrame\r\n",
        "        .select(max(col(\"ingested_at\")).alias(\"max_ingested_at\"))\r\n",
        "        .collect()[0][\"max_ingested_at\"]  # Extract the value from Row object\r\n",
        "    )\r\n",
        "    max_reviews_ingested_at_str = max_reviews_ingested_at.strftime(\"%Y%m%d\")\r\n",
        "else:\r\n",
        "    max_reviews_ingested_at_str = '19000101'\r\n",
        "    max_reviews_ingested_at = datetime.strptime(max_reviews_ingested_at_str, \"%Y%m%d\")\r\n",
        "\r\n",
        "if bronze_ratings_delta_table:\r\n",
        "    max_ratings_ingested_at = (\r\n",
        "        bronze_ratings_delta_table\r\n",
        "        .toDF()  # Convert DeltaTable to a Spark DataFrame\r\n",
        "        .select(max(col(\"ingested_at\")).alias(\"max_ingested_at\"))\r\n",
        "        .collect()[0][\"max_ingested_at\"]  # Extract the value from Row object\r\n",
        "    )\r\n",
        "    max_ratings_ingested_at_str = max_ratings_ingested_at.strftime(\"%Y%m%d\")\r\n",
        "else:\r\n",
        "    max_ratings_ingested_at_str = '19000101'\r\n",
        "    max_ratings_ingested_at = datetime.strptime(max_ratings_ingested_at_str, \"%Y%m%d\")\r\n",
        "    \r\n",
        "#print(f\"latest_date_loaded_in_bronze: {latest_date_loaded_in_bronze}\")\r\n",
        "print(f\"max_reviews_ingested_at: {max_reviews_ingested_at}\")\r\n",
        "print(f\"max_reviews_ingested_at_str: {max_reviews_ingested_at_str}\")\r\n",
        "print(f\"max_ratings_ingested_at: {max_ratings_ingested_at}\")\r\n",
        "print(f\"max_ratings_ingested_at_str: {max_ratings_ingested_at_str}\")\r\n",
        "\r\n",
        "print(f\"type(max_reviews_ingested_at): {type(max_reviews_ingested_at)}\")\r\n",
        "print(f\"type(max_ratings_ingested_at): {type(max_ratings_ingested_at)}\")\r\n",
        "\r\n",
        "# Get the minimum of these\r\n",
        "# TODO: I don't love this strategy.\r\n",
        "# This next line is broken\r\n",
        "min_ingested_at = __builtins__.min(max_reviews_ingested_at, max_ratings_ingested_at)\r\n",
        "min_ingested_at_str = min_ingested_at.strftime(\"%Y%m%d\")\r\n",
        "\r\n",
        "print(f\"min_ingested_at: {min_ingested_at}\")\r\n",
        "print(f\"min_ingested_at_str: {min_ingested_at_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Now get the list of raw folders we need to process based on what we found in bronze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Get the folders in raw that we need to load, based on what we found in bronze\r\n",
        "# TODO: Do we need to do this for both ratings and reviews? Or just use the combined minimum? using the minimum for now.\r\n",
        "bv_raw_folder_path = f\"{raw_adls_path}{bv_subfolder}\"\r\n",
        "raw_folders_to_process = get_filtered_date_folders(abfss_path=bv_raw_folder_path, min_date=min_ingested_at_str)\r\n",
        "print(F\"raw_folders_to_process: {raw_folders_to_process}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### And finally, get the list of raw files we need to process into bronze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "metadata": {},
      "source": [
        "raw_files_to_process = []\r\n",
        "for raw_folder in raw_folders_to_process:\r\n",
        "    # Now add to the abfss path\r\n",
        "    raw_folder_path = f\"{bv_raw_folder_path}/{raw_folder}\"\r\n",
        "    print('-' * 100)\r\n",
        "    print(f\"Looking for files in: {raw_folder_path}\")\r\n",
        "\r\n",
        "    # Get the files in the curent folder\r\n",
        "    raw_files_in_folder = mssparkutils.fs.ls(raw_folder_path)\r\n",
        "\r\n",
        "    # Save the whole FileInfo\r\n",
        "    for raw_file in raw_files_in_folder:\r\n",
        "        raw_file_path = raw_file.path\r\n",
        "        raw_files_to_process.append(raw_file)\r\n",
        "        print(raw_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Functions for xforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### product_reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "metadata": {},
      "source": [
        "# df_final_reviews = transform_reviews_dataframe(\r\n",
        "#     df_reviews,\r\n",
        "#     feed_type=\"full\",\r\n",
        "#     generated_on=\"2024-04-13\",\r\n",
        "#     raw_file_modify_time=raw_file_mod_time,\r\n",
        "#     raw_file_path=raw_file_path\r\n",
        "# )\r\n",
        "\r\n",
        "from pyspark.sql import DataFrame\r\n",
        "from pyspark.sql.functions import col, to_json, lit\r\n",
        "from pyspark.sql.types import TimestampType\r\n",
        "\r\n",
        "def transform_reviews_dataframe(\r\n",
        "    df_reviews: DataFrame,\r\n",
        "    feed_type: str,\r\n",
        "    generated_on: str,\r\n",
        "    raw_file_modify_time,\r\n",
        "    raw_file_path: str\r\n",
        ") -> DataFrame:\r\n",
        "    \"\"\"\r\n",
        "    Transforms the df_reviews DataFrame into the final schema and adds metadata fields.\r\n",
        "\r\n",
        "    Parameters:\r\n",
        "    - df_reviews: Spark DataFrame containing review data\r\n",
        "    - feed_type: Indicates full or incremental feed\r\n",
        "    - generated_on: Date string parsed from file name\r\n",
        "    - raw_file_modify_time: Timestamp from raw file's last modified time\r\n",
        "    - raw_file_path: ABFSS path of the raw input file\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - A new Spark DataFrame with selected and transformed columns\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    return df_reviews.select(\r\n",
        "        col(\"_id\").alias(\"product_id\"), \r\n",
        "        col(\"product_reviews.UserProfileReference._id\").alias(\"user_id\"), \r\n",
        "        col(\"_disabled\").alias(\"disabled\"), \r\n",
        "        col(\"_removed\").alias(\"removed\"), \r\n",
        "        \r\n",
        "        col(\"product_reviews._id\").alias(\"review_id\"), \r\n",
        "        col(\"product_reviews.AuthenticationType\").alias(\"authentication_type\"),\r\n",
        "        col(\"product_reviews.CampaignId\").alias(\"campaign_id\"),\r\n",
        "        col(\"product_reviews.ContentCodes\").alias(\"content_codes\"),\r\n",
        "        to_json(col(\"product_reviews.ContextDataValues\")).alias(\"context_data_values\"),\r\n",
        "        col(\"product_reviews.DisplayLocale\").alias(\"display_locale\"),\r\n",
        "        col(\"product_reviews.Featured\").alias(\"featured\"),\r\n",
        "        col(\"product_reviews.FirstPublishTime\").alias(\"first_publish_time\"),\r\n",
        "        col(\"product_reviews.Guid\").alias(\"guid\"),\r\n",
        "        col(\"product_reviews.LastModificationTime\").alias(\"last_modification_time\"),\r\n",
        "        col(\"product_reviews.LastPublishTime\").alias(\"last_publish_time\"),\r\n",
        "        col(\"product_reviews.ModerationStatus\").alias(\"moderation_status\"),\r\n",
        "        col(\"product_reviews.NetPromoterScore\").alias(\"net_promoter_score\"),\r\n",
        "        col(\"product_reviews.NetPromoterComment\").alias(\"net_promoter_comment\"),\r\n",
        "        col(\"product_reviews.NumComments\").alias(\"num_comments\"),\r\n",
        "        col(\"product_reviews.NumFeedbacks\").alias(\"num_feedbacks\"),\r\n",
        "        col(\"product_reviews.NumNegativeFeedbacks\").alias(\"num_negative_feedbacks\"),\r\n",
        "        col(\"product_reviews.NumPositiveFeedbacks\").alias(\"num_positive_feedbacks\"),\r\n",
        "        col(\"product_reviews.OriginatingDisplayCode\").alias(\"originating_display_code\"),\r\n",
        "        col(\"product_reviews.ProductReviewsDeepLinkedUrl\").alias(\"product_reviews_deep_linked_url\"),\r\n",
        "        col(\"product_reviews.Rating\").alias(\"rating\"),\r\n",
        "        col(\"product_reviews.RatingRange\").alias(\"rating_range\"),\r\n",
        "        to_json(col(\"product_reviews.RatingValues.RatingValue\")).alias(\"rating_values\"),\r\n",
        "        col(\"product_reviews.RatingsOnly\").alias(\"ratings_only\"),\r\n",
        "        col(\"product_reviews.Recommended\").alias(\"recommended\"),\r\n",
        "        col(\"product_reviews.ReviewText\").alias(\"review_text\"),\r\n",
        "        col(\"product_reviews.ReviewerLocation\").alias(\"reviewer_location\"),\r\n",
        "        col(\"product_reviews.ReviewerNickname\").alias(\"reviewer_nickname\"),\r\n",
        "        col(\"product_reviews.SendEmailAlertWhenCommented\").alias(\"send_email_alert_when_commented\"),\r\n",
        "        col(\"product_reviews.SendEmailAlertWhenPublished\").alias(\"send_email_alert_when_published\"),\r\n",
        "        col(\"product_reviews.SubmissionTime\").alias(\"submission_time\"),\r\n",
        "        col(\"product_reviews.Title\").alias(\"title\"),\r\n",
        "        to_json(col(\"product_reviews.UserProfileReference\")).alias(\"user_profile_reference\"),\r\n",
        "        col(\"product_reviews.Videos\").alias(\"videos\"),\r\n",
        "\r\n",
        "        lit(feed_type).alias(\"feed_type\"),\r\n",
        "        lit(generated_on).cast(\"date\").alias(\"generated_on\"),\r\n",
        "        lit(raw_file_modify_time).cast(TimestampType()).alias(\"ingested_at\"),\r\n",
        "        lit(raw_file_path).alias(\"ingested_from\")\r\n",
        "    )\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### product_ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import DataFrame\r\n",
        "from pyspark.sql.functions import col, to_json, lit\r\n",
        "from pyspark.sql.types import TimestampType\r\n",
        "\r\n",
        "def transform_ratings_dataframe(\r\n",
        "    df_raw_product: DataFrame,\r\n",
        "    feed_type: str,\r\n",
        "    generated_on: str,\r\n",
        "    raw_file_modify_time,\r\n",
        "    raw_file_path: str\r\n",
        ") -> DataFrame:\r\n",
        "    \"\"\"\r\n",
        "    Transforms the df_raw_product DataFrame into the final schema and adds metadata fields.\r\n",
        "\r\n",
        "    Parameters:\r\n",
        "    - df_raw_product: Spark DataFrame containing product data\r\n",
        "    - feed_type: Indicates full or incremental feed\r\n",
        "    - generated_on: Date string parsed from file name\r\n",
        "    - raw_file_modify_time: Timestamp from raw file's last modified time\r\n",
        "    - raw_file_path: ABFSS path of the raw input file\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - A new Spark DataFrame with selected and transformed columns\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    return df_raw_product.select(\r\n",
        "        col(\"_disabled\").alias(\"disabled\"),\r\n",
        "        col(\"_id\").alias(\"product_id\"),\r\n",
        "        col(\"_removed\").alias(\"removed\"),\r\n",
        "        to_json(col(\"Attributes\")).alias(\"attributes\"),\r\n",
        "        to_json(col(\"Brand\")).alias(\"brand\"),\r\n",
        "        to_json(col(\"CategoryItems\")).alias(\"category_items\"),\r\n",
        "        col(\"Description\").alias(\"description\"),\r\n",
        "        to_json(col(\"Descriptions\")).alias(\"descriptions\"),\r\n",
        "        to_json(col(\"EANs\")).alias(\"eans\"),\r\n",
        "        col(\"ExternalId\").alias(\"external_id\"),\r\n",
        "        col(\"ImageUrl\").alias(\"image_url\"),\r\n",
        "        to_json(col(\"ImageUrls\")).alias(\"image_urls\"),\r\n",
        "        col(\"Name\").alias(\"name\"),\r\n",
        "        to_json(col(\"Names\")).alias(\"names\"),\r\n",
        "        to_json(col(\"NativeReviewStatistics\")).alias(\"native_review_statistics\"),\r\n",
        "        col(\"NumAnswers\").alias(\"num_answers\"),\r\n",
        "        col(\"NumNativeAnswers\").alias(\"num_native_answers\"),\r\n",
        "        col(\"NumNativeQuestions\").alias(\"num_native_questions\"),\r\n",
        "        col(\"NumQuestions\").alias(\"num_questions\"),\r\n",
        "        col(\"NumReviews\").alias(\"num_reviews\"),\r\n",
        "        col(\"NumStories\").alias(\"num_stories\"),\r\n",
        "        col(\"ProductPageUrl\").alias(\"product_page_url\"),\r\n",
        "        to_json(col(\"ProductPageUrls\")).alias(\"product_page_urls\"),\r\n",
        "        to_json(col(\"ReviewStatistics\")).alias(\"review_statistics\"),\r\n",
        "        col(\"Source\").alias(\"source\"),\r\n",
        "\r\n",
        "        lit(feed_type).alias(\"feed_type\"),\r\n",
        "        lit(generated_on).cast(\"date\").alias(\"generated_on\"),\r\n",
        "        lit(raw_file_modify_time).cast(TimestampType()).alias(\"ingested_at\"),\r\n",
        "        lit(raw_file_path).alias(\"ingested_from\")\r\n",
        "    )\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Process the raw files into bronze delta tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Process each of the files\r\n",
        "for raw_file_info in raw_files_to_process:\r\n",
        "    # Feed is the root XML tag. Then, we have Product, Category, and UserProfile elements\r\n",
        "    # Get file path and other elements from the FileInfo\r\n",
        "    #print(dir(raw_file_info)) # 'modifyTime', 'name', 'path', 'size'\r\n",
        "    raw_file_path = raw_file_info.path\r\n",
        "    print(f\"Processing {raw_file_path}\")\r\n",
        "    raw_file_modify_time = datetime.utcfromtimestamp(raw_file_info.modifyTime / 1000)\r\n",
        "    raw_file_name = raw_file_info.name\r\n",
        "\r\n",
        "    # Categorize the file - standard client feed? full or incremental?\r\n",
        "    # We really only need to process a single FULL (manually below), then the incremental files only for ongoing processing.\r\n",
        "    if 'standard_client_feed' in raw_file_name:\r\n",
        "        if 'incremental' in raw_file_name:\r\n",
        "            feed_type = 'incremental'\r\n",
        "            # Now we can look for the date in the incremental file\r\n",
        "            # Parse the \"generated on\" from the file name\r\n",
        "            match = re.search(r'(\\d{8})', raw_file_name)\r\n",
        "            if match:\r\n",
        "                date_str = match.group(1)\r\n",
        "                generated_on = datetime.strptime(date_str, \"%Y%m%d\").date()\r\n",
        "            else:\r\n",
        "                # We should skip this file since it doesn't have a date string in it.\r\n",
        "                # This should never happenm but we're leaving it for safety\r\n",
        "                print(f\"No date string found in file name. Skipping: {raw_file_name}\")\r\n",
        "                continue\r\n",
        "        else:\r\n",
        "            # We don't process full files in the reular daily flow\r\n",
        "            print(f\"Skipping FULL file: {raw_file_name}\")\r\n",
        "            continue\r\n",
        "    else:\r\n",
        "        # We don't process the ratings file currently\r\n",
        "        print(f\"NOT PROCESSING: {raw_file_name}\")\r\n",
        "        continue\r\n",
        "\r\n",
        "    print(f\"raw_file_name: {raw_file_name}\")\r\n",
        "    print(f\"feed_type: {feed_type}\")\r\n",
        "    print(f\"raw_file_modify_time: {raw_file_modify_time}\")\r\n",
        "    print(f\"generated_on: {generated_on}\")\r\n",
        "\r\n",
        "    # Get the Product elements from the XML file\r\n",
        "    print(f\"Reading XML from file {raw_file_path}...\")\r\n",
        "    df_raw_product = spark.read.format(\"xml\") \\\r\n",
        "        .option(\"rowTag\", \"Product\") \\\r\n",
        "        .load(raw_file_path)\r\n",
        "\r\n",
        "    # See if our delta tables exist\r\n",
        "    bronze_reviews_delta_table = get_delta_table(bronze_reviews_delta_path)\r\n",
        "    bronze_ratings_delta_table = get_delta_table(bronze_ratings_delta_path)\r\n",
        "\r\n",
        "    # Reviews **********************************************************************\r\n",
        "    # Explode the reviews and filter to only get the Product attributes we need\r\n",
        "    df_reviews = df_raw_product.select(\"_id\", \"_disabled\", \"_removed\", explode(col(\"Reviews.Review\")).alias(\"product_reviews\"))\r\n",
        "\r\n",
        "    # See what we have so we can dial in the SELECT to match the dbt\r\n",
        "    #df_reviews.printSchema()\r\n",
        "\r\n",
        "    # Get the columns we need from the Review.  Do we want to use the dbt aliases or keep the orig where we can? We want to be pascal case in gold anyway.  hmm.\r\n",
        "    df_final_reviews = transform_reviews_dataframe(\r\n",
        "        df_reviews,\r\n",
        "        feed_type=feed_type,\r\n",
        "        generated_on=generated_on,\r\n",
        "        raw_file_modify_time=raw_file_modify_time,\r\n",
        "        raw_file_path=raw_file_path\r\n",
        "    )\r\n",
        "\r\n",
        "    # Save/merge to bronze table. I think our keys are just the product_id and review_id\r\n",
        "    if bronze_reviews_delta_table:\r\n",
        "        print(f\"Reviews bronze table exists. Merging data from {raw_file_path}\")\r\n",
        "        # Merge it\r\n",
        "        merge_condition = \"target.product_id = source.product_id AND target.review_id = source.review_id and target.ingested_from = source.ingested_from\"\r\n",
        "        (\r\n",
        "            bronze_reviews_delta_table.alias(\"target\")\r\n",
        "            .merge(df_final_reviews.alias(\"source\"), merge_condition)\r\n",
        "            .whenMatchedUpdateAll()\r\n",
        "            .whenNotMatchedInsertAll()\r\n",
        "            .execute()\r\n",
        "        )\r\n",
        "    else:\r\n",
        "        print(f\"Overwriting bronze reviews delta table with: {raw_file_path} dataframe\")\r\n",
        "        # Table doesn't exist, just write/save the reviews dataframe to the delta location\r\n",
        "        (\r\n",
        "            df_final_reviews\r\n",
        "            .write\r\n",
        "            .format(\"delta\")\r\n",
        "            .mode(\"overwrite\")\r\n",
        "            .save(bronze_reviews_delta_path)\r\n",
        "        )\r\n",
        "\r\n",
        "    # Product Ratings **********************************************************************************************\r\n",
        "    df_final_ratings = transform_ratings_dataframe(\r\n",
        "        df_raw_product,\r\n",
        "        feed_type=feed_type,\r\n",
        "        generated_on=generated_on,\r\n",
        "        raw_file_modify_time=raw_file_modify_time,\r\n",
        "        raw_file_path=raw_file_path\r\n",
        "    )\r\n",
        "\r\n",
        "    # Save/merge ratings to bronze table - \r\n",
        "    # Trying the product_id and the source file for now for PKs.\r\n",
        "    if bronze_ratings_delta_table:\r\n",
        "        print(f\"Ratings bronze table exists. Merging data from {raw_file_path}\")\r\n",
        "        # Merge it\r\n",
        "        merge_condition = \"target.product_id = source.product_id and target.ingested_from = source.ingested_from\"\r\n",
        "        (\r\n",
        "            bronze_ratings_delta_table.alias(\"target\")\r\n",
        "            .merge(df_final_ratings.alias(\"source\"), merge_condition)\r\n",
        "            .whenMatchedUpdateAll()\r\n",
        "            .whenNotMatchedInsertAll()\r\n",
        "            .execute()\r\n",
        "        )\r\n",
        "    else:\r\n",
        "        print(f\"Overwriting bronze ratings delta table with: {raw_file_path} dataframe\")\r\n",
        "        # Just write the reviews dataframe to the delta location\r\n",
        "        (\r\n",
        "            df_final_ratings\r\n",
        "            .write\r\n",
        "            .format(\"delta\")\r\n",
        "            .mode(\"overwrite\")\r\n",
        "            .save(bronze_ratings_delta_path)\r\n",
        "        )\r\n",
        "\r\n",
        "print(\"DONE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Silver Processing TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Gold Processing and final Load TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Exit the notebook to keep our miscellaneous code from running automatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "mssparkutils.notebook.exit(\"0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Helper Cells that don't run normally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Load the initial full file\r\n",
        "### This section only runs once, to create the delta tables using the first FULL file. \r\n",
        "### Only incremental files after this load.\r\n",
        "### !! Don't forget to also run the two xform function cells above to create the functions: def transform_ratings_dataframe AND def transform_reviews_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def find_first_full_file_path(bv_raw_root_path):\r\n",
        "    for item in mssparkutils.fs.ls(bv_raw_root_path):\r\n",
        "        if item.isDir:\r\n",
        "            item_path = item.path\r\n",
        "            item_name = item.name\r\n",
        "            print(f\"Looking in subfolder {item_name}\")\r\n",
        "            for file in mssparkutils.fs.ls(item_path):\r\n",
        "                if file.isFile and file.name == 'bv_sweatybetty_standard_client_feed.xml.gz':\r\n",
        "                    # Found it!\r\n",
        "                    return file.path\r\n",
        "    return None  # Not found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Imports\r\n",
        "from notebookutils import mssparkutils\r\n",
        "\r\n",
        "# Find our first full file in th raw zone\r\n",
        "bv_raw_root_path = f\"{raw_adls_path}/BazaarVoice\"\r\n",
        "full_file_name = 'bv_sweatybetty_standard_client_feed.xml.gz'\r\n",
        "print(f\"Looking in {bv_raw_root_path} for {full_file_name}\")\r\n",
        "\r\n",
        "first_full_file_path = find_first_full_file_path(bv_raw_root_path)\r\n",
        "print(f\"Processing FULL file: {first_full_file_path}\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Now let's read the FULL file's Products into our dataframe \r\n",
        "raw_file_path = first_full_file_path\r\n",
        "print(f\"Reading FULL XML from file {raw_file_path}...\")\r\n",
        "df_raw_product = spark.read.format(\"xml\") \\\r\n",
        "    .option(\"rowTag\", \"Product\") \\\r\n",
        "    .load(raw_file_path)\r\n",
        "\r\n",
        "print(f\"finished reading FULL XML file {raw_file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Don't forget to run the xform function creation cells before running this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "\r\n",
        "# TODO: Get the ingested_at, generated_on, ingested_from, and feed_type vars set\r\n",
        "from datetime import datetime\r\n",
        "from pyspark.sql.functions import explode\r\n",
        "\r\n",
        "feed_type = \"full\"\r\n",
        "\r\n",
        "raw_file_info = mssparkutils.fs.ls(raw_file_path)[0]\r\n",
        "raw_file_modify_time = datetime.utcfromtimestamp(raw_file_info.modifyTime / 1000) # using this for ingested_at\r\n",
        "generated_on = raw_file_modify_time.date()\r\n",
        "ingested_from = raw_file_path\r\n",
        "\r\n",
        "print(f\"feed_type: {feed_type}\")\r\n",
        "print(f\"generated_on: {generated_on}\")\r\n",
        "print(f\"ingested_at: {raw_file_modify_time}\")\r\n",
        "print(f\"ingested_from: {ingested_from}\")\r\n",
        "\r\n",
        "# Explode the reviews and filter to only get the Product attributes we need\r\n",
        "print(f\"Exploding the Reviews...\")\r\n",
        "df_reviews = df_raw_product.select(\"_id\", \"_disabled\", \"_removed\", explode(col(\"Reviews.Review\")).alias(\"product_reviews\"))\r\n",
        "\r\n",
        "# Get the columns we need from the Review.  Do we want to use the dbt aliases or keep the orig where we can? We want to be pascal case in gold anyway.  hmm.\r\n",
        "print(f\"Finished exploding Reviews. Creating df_final_reviews...\")\r\n",
        "df_final_reviews = transform_reviews_dataframe(\r\n",
        "    df_reviews,\r\n",
        "    feed_type=feed_type,\r\n",
        "    generated_on=generated_on,\r\n",
        "    raw_file_modify_time=raw_file_modify_time,\r\n",
        "    raw_file_path=raw_file_path\r\n",
        ")\r\n",
        "\r\n",
        "bv_bronze_folder_path = f\"{bronze_adls_path}/BazaarVoice\"\r\n",
        "\r\n",
        "bronze_reviews_delta_path = f\"{bv_bronze_folder_path}/product_reviews\"\r\n",
        "print(f\"Finished creating the df_final_review. Writing reviews to delta table in bronze {bronze_reviews_delta_path}...\")\r\n",
        "# Save to bronze table.\r\n",
        "(\r\n",
        "    df_final_reviews\r\n",
        "    .write\r\n",
        "    .format(\"delta\")\r\n",
        "    .mode(\"overwrite\")\r\n",
        "    .option(\"overwriteSchema\", True)\r\n",
        "    .save(bronze_reviews_delta_path)\r\n",
        ")\r\n",
        "\r\n",
        "# Product Ratings **********************************************************************************************\r\n",
        "print(\"Finished writing product_reviews delta table in bronze. Building the final ratings dataframe...\")\r\n",
        "df_final_ratings = transform_ratings_dataframe(\r\n",
        "    df_raw_product,\r\n",
        "    feed_type=feed_type,\r\n",
        "    generated_on=generated_on,\r\n",
        "    raw_file_modify_time=raw_file_modify_time,\r\n",
        "    raw_file_path=raw_file_path\r\n",
        ")\r\n",
        "\r\n",
        "# Saveratings to bronze table - \r\n",
        "# Trying the product_id and the source file for now for PKs.\r\n",
        "bronze_ratings_delta_path = f\"{bv_bronze_folder_path}/product_ratings\"\r\n",
        "print(f\"Finished bulding the final ratings dataframe. Writing bronze ratings delta tablein bronze {bronze_ratings_delta_path}...\")\r\n",
        "# Just write the reviews dataframe to the delta location\r\n",
        "(\r\n",
        "    df_final_ratings\r\n",
        "    .write\r\n",
        "    .format(\"delta\")\r\n",
        "    .mode(\"overwrite\")\r\n",
        "    .option(\"overwriteSchema\", True)\r\n",
        "    .save(bronze_ratings_delta_path)\r\n",
        ")\r\n",
        "\r\n",
        "print(\"DONE!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Check our work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# TODO: Print the schema for the two final dataframes - reviews and ratings\r\n",
        "print(\"df_final_reviews schema:\")\r\n",
        "df_final_reviews.printSchema()\r\n",
        "print(\"-\" * 50)\r\n",
        "print(\"df_final_ratings schema:\")\r\n",
        "df_final_ratings.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(df_reviews.count())\r\n",
        "print(df_final_reviews.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Schema Checks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Quick check of all existing bronze delta tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# dev_ratings_delta_path = \"abfss://bronze@azwwwnonproddevadapadls.dfs.core.windows.net/BazaarVoice/product_ratings\"\r\n",
        "# dev_reviews_delta_path = \"abfss://bronze@azwwwnonproddevadapadls.dfs.core.windows.net/BazaarVoice/product_reviews\"\r\n",
        "# tst_ratings_delta_path = \"abfss://bronze@azwwwnonprodtestadapadls.dfs.core.windows.net/BazaarVoice/product_ratings\"\r\n",
        "# tst_reviews_delta_path = \"abfss://bronze@azwwwnonprodtestadapadls.dfs.core.windows.net/BazaarVoice/product_reviews\"\r\n",
        "# prd_ratings_delta_path = \"abfss://bronze@azwwwprodprdadapadls.dfs.core.windows.net/BazaarVoice/product_ratings\"\r\n",
        "# prd_reviews_delta_path = \"abfss://bronze@azwwwprodprdadapadls.dfs.core.windows.net/BazaarVoice/product_reviews\"\r\n",
        "\r\n",
        "from delta import DeltaTable\r\n",
        "\r\n",
        "delta_paths = [\r\n",
        "    {\"descriptor\": \"dev_ratings\", \"abfss_path\": \"abfss://bronze@azwwwnonproddevadapadls.dfs.core.windows.net/BazaarVoice/product_ratings\"},\r\n",
        "    {\"descriptor\": \"dev_reviews\", \"abfss_path\": \"abfss://bronze@azwwwnonproddevadapadls.dfs.core.windows.net/BazaarVoice/product_reviews\"},\r\n",
        "    {\"descriptor\": \"tst_ratings\", \"abfss_path\": \"abfss://bronze@azwwwnonprodtestadapadls.dfs.core.windows.net/BazaarVoice/product_ratings\"},\r\n",
        "    {\"descriptor\": \"tst_reviews\", \"abfss_path\": \"abfss://bronze@azwwwnonprodtestadapadls.dfs.core.windows.net/BazaarVoice/product_reviews\"},\r\n",
        "    {\"descriptor\": \"prd_ratings\", \"abfss_path\": \"abfss://bronze@azwwwprodprdadapadls.dfs.core.windows.net/BazaarVoice/product_ratings\"},\r\n",
        "    {\"descriptor\": \"prd_reviews\", \"abfss_path\": \"abfss://bronze@azwwwprodprdadapadls.dfs.core.windows.net/BazaarVoice/product_reviews\"},\r\n",
        "]\r\n",
        "\r\n",
        "# Loop through each and print the schema\r\n",
        "for entry in delta_paths:\r\n",
        "    print(f\"Schema for {entry['descriptor']}:\")\r\n",
        "    delta_table = DeltaTable.forPath(spark, entry[\"abfss_path\"])\r\n",
        "    delta_table.toDF().printSchema()\r\n",
        "    print(\"-\" * 60)\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Look at inferred schema again and potential struct/array refinement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/\r\n",
        "# https://azwwwnonproddevadapadls.blob.core.windows.net/raw/BazaarVoice/date=20250219/bv_sweatybetty_incremental_standard_client_feed_20250120.xml.gz\r\n",
        "\r\n",
        "from pyspark.sql.functions import explode, col, to_json\r\n",
        "\r\n",
        "raw_abfss_path = 'abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/BazaarVoice/date=20250219/bv_sweatybetty_incremental_standard_client_feed_20250120.xml.gz'\r\n",
        "df_raw_product = (\r\n",
        "    spark\r\n",
        "    .read\r\n",
        "    .format(\"xml\")\r\n",
        "    .option(\"rowTag\", \"Product\")\r\n",
        "    .load(raw_abfss_path)\r\n",
        ")\r\n",
        "\r\n",
        "# Equivalent:\r\n",
        "# df_raw_product = spark.read.format(\"xml\") \\\r\n",
        "#     .option(\"rowTag\", \"Product\") \\\r\n",
        "#     .load(raw_file_path)\r\n",
        "\r\n",
        "print(f\"df_raw_product loaded from: {raw_abfss_path}\")\r\n",
        "\r\n",
        "df_reviews = df_raw_product.select(\"_id\", \"_disabled\", \"_removed\", explode(col(\"Reviews.Review\")).alias(\"product_reviews\"))\r\n",
        "print(\"exploded reviews\")\r\n",
        "\r\n",
        "df_reviews.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Write to a test delta table\r\n",
        "test_bronze_delta_path = \"abfss://bronze@azwwwnonproddevadapadls.dfs.core.windows.net/BazaarVoice/test_product_reviews\"\r\n",
        "(\r\n",
        "    df_final_reviews\r\n",
        "    .write\r\n",
        "    .format(\"delta\")\r\n",
        "    .save(test_bronze_delta_path)\r\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## How important are the full files vs incremental.\r\n",
        "### Seems like they have the same data. Do a check. NO, the full has hundreds of thousands of reviews.  the incrementals total only 10s of thousands.\r\n",
        "\r\n",
        "### Might only need one full, tho. ?!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Load a full file and save it as a table.  Then check dates within"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Read a full file.\r\n",
        "# https://azwwwnonproddevadapadls.blob.core.windows.net/raw/BazaarVoice/date=20250219/bv_sweatybetty_standard_client_feed.xml.gz\r\n",
        "raw_full_file_path = 'abfss://raw@azwwwnonproddevadapadls.dfs.core.windows.net/BazaarVoice/date=20250219/bv_sweatybetty_standard_client_feed.xml.gz'\r\n",
        "\r\n",
        "df_full_file_raw_product = (\r\n",
        "    spark\r\n",
        "    .read\r\n",
        "    .format(\"xml\")\r\n",
        "    .option(\"rowTag\", \"Product\")\r\n",
        "    .load(raw_full_file_path)\r\n",
        ")\r\n",
        "print(f\"Done reading {raw_full_file_path}\")\r\n",
        "# 00:01:07 to read - not bad.  But that was without the product rowTag.  Going to be longer with that...Eeek - 10 minutes already. 00:14:10 - yikes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_full_file_reviews = df_full_file_raw_product.select(\"_id\", \"_disabled\", \"_removed\", explode(col(\"Reviews.Review\")).alias(\"product_reviews\"))\r\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import lit\r\n",
        "\r\n",
        "feed_type = 'full'\r\n",
        "\r\n",
        "raw_file_name = raw_full_file_path\r\n",
        "raw_file_path = raw_full_file_path\r\n",
        "# Fake these\r\n",
        "raw_file_modify_time = datetime.utcnow()\r\n",
        "generated_on = datetime.utcnow()\r\n",
        "\r\n",
        "df_full_file_final_reviews = df_full_file_reviews.select(\r\n",
        "    col(\"_id\").alias(\"product_id\"), \r\n",
        "    col(\"product_reviews.UserProfileReference._id\").alias(\"user_id\"), \r\n",
        "    col(\"_disabled\").alias(\"disabled\"), \r\n",
        "    col(\"_removed\").alias(\"removed\"), \r\n",
        "    \r\n",
        "    col(\"product_reviews._id\").alias(\"review_id\"), \r\n",
        "    col(\"product_reviews.AuthenticationType\").alias(\"authentication_type\"),\r\n",
        "    col(\"product_reviews.CampaignId\").alias(\"campaign_id\"),\r\n",
        "    col(\"product_reviews.ContentCodes\").alias(\"content_codes\"),\r\n",
        "    to_json(col(\"product_reviews.ContextDataValues\")).alias(\"context_data_values\"), # newly JOSN\r\n",
        "    col(\"product_reviews.DisplayLocale\").alias(\"display_locale\"),\r\n",
        "    col(\"product_reviews.Featured\").alias(\"featured\"),\r\n",
        "    col(\"product_reviews.FirstPublishTime\").alias(\"first_publish_time\"),\r\n",
        "    col(\"product_reviews.Guid\").alias(\"guid\"),\r\n",
        "    col(\"product_reviews.LastModificationTime\").alias(\"last_modification_time\"),\r\n",
        "    col(\"product_reviews.LastPublishTime\").alias(\"last_publish_time\"),\r\n",
        "    col(\"product_reviews.ModerationStatus\").alias(\"moderation_status\"),\r\n",
        "    col(\"product_reviews.NetPromoterScore\").alias(\"net_promoter_score\"),\r\n",
        "    col(\"product_reviews.NetPromoterComment\").alias(\"net_promoter_comment\"),\r\n",
        "    col(\"product_reviews.NumComments\").alias(\"num_comments\"),\r\n",
        "    col(\"product_reviews.NumFeedbacks\").alias(\"num_feedbacks\"),\r\n",
        "    col(\"product_reviews.NumNegativeFeedbacks\").alias(\"num_negative_feedbacks\"),\r\n",
        "    col(\"product_reviews.NumPositiveFeedbacks\").alias(\"num_positive_feedbacks\"),\r\n",
        "    col(\"product_reviews.OriginatingDisplayCode\").alias(\"originating_display_code\"),\r\n",
        "    col(\"product_reviews.ProductReviewsDeepLinkedUrl\").alias(\"product_reviews_deep_linked_url\"),\r\n",
        "    col(\"product_reviews.Rating\").alias(\"rating\"),\r\n",
        "    col(\"product_reviews.RatingRange\").alias(\"rating_range\"),\r\n",
        "    # We need to handle when this is an array rather than just a single struct. *********************************************\r\n",
        "    to_json(col(\"product_reviews.RatingValues.RatingValue\")).alias(\"rating_values\"),\r\n",
        "    # ***********************************************************************************************************************\r\n",
        "    col(\"product_reviews.RatingsOnly\").alias(\"ratings_only\"),\r\n",
        "    col(\"product_reviews.Recommended\").alias(\"recommended\"),\r\n",
        "    col(\"product_reviews.ReviewText\").alias(\"review_text\"),\r\n",
        "    col(\"product_reviews.ReviewerLocation\").alias(\"reviewer_location\"),\r\n",
        "    col(\"product_reviews.ReviewerNickname\").alias(\"reviewer_nickname\"),\r\n",
        "    col(\"product_reviews.SendEmailAlertWhenCommented\").alias(\"send_email_alert_when_commented\"),\r\n",
        "    col(\"product_reviews.SendEmailAlertWhenPublished\").alias(\"send_email_alert_when_published\"),\r\n",
        "    col(\"product_reviews.SubmissionTime\").alias(\"submission_time\"),\r\n",
        "    col(\"product_reviews.Title\").alias(\"title\"),\r\n",
        "    to_json(col(\"product_reviews.UserProfileReference\")).alias(\"user_profile_reference\"),\r\n",
        "    col(\"product_reviews.Videos\").alias(\"videos\"),  \r\n",
        "\r\n",
        "    # Selecting some metadata columns, too: feed_type, generated_on, ingested_at, ingested_from\r\n",
        "    # feed_type - matches the incremental vs non-incremental file names. full is for the non-incremental files.\r\n",
        "    lit(feed_type).alias(\"feed_type\"),\r\n",
        "    # generated_on, This comes from the file name (parsed above)\r\n",
        "    lit(generated_on).cast(\"date\").alias(\"generated_on\"),\r\n",
        "    # ingested_at, # the create time (in our raw zone) of the raw file - yes\r\n",
        "    lit(raw_file_modify_time).cast(TimestampType()).alias(\"ingested_at\"),\r\n",
        "    # ingested_from # the file name\r\n",
        "    lit(raw_file_path).alias(\"ingested_from\") # Might want to just get the file name extracted from the abfss path.\r\n",
        ")\r\n",
        "\r\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Save as a test table\r\n",
        "(\r\n",
        "    df_full_file_final_reviews\r\n",
        "    .write\r\n",
        "    .format(\"delta\")\r\n",
        "    .save(\"abfss://bronze@azwwwnonproddevadapadls.dfs.core.windows.net/BazaarVoice/full_file_test\")\r\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_full_file_final_reviews.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Min and MAx timestamps\r\n",
        "df_full_file_final_reviews.agg(\r\n",
        "    min(\"last_modification_time\").alias(\"min_last_modification_time\"),\r\n",
        "    max(\"last_modification_time\").alias(\"max_last_modification_time\"),\r\n",
        "    min(\"last_publish_time\").alias(\"min_last_publish_time\"),\r\n",
        "    max(\"last_publish_time\").alias(\"max_last_publish_time\")\r\n",
        ").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Some full vs incremental counts checks..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# get our keys\r\n",
        "# merge_condition = \"target.product_id = source.product_id AND target.review_id = source.review_id and target.ingested_from = source.ingested_from\"\r\n",
        "#review_key_columns = [\"product_id\", \"review_id\"] # I think this is a sane key\r\n",
        "# The count before getting unique\r\n",
        "row_count = df_full_file_reviews.count()\r\n",
        "print(f\"unfiltered row count: {row_count}\") # 235,018\r\n",
        "\r\n",
        "df_unique_reviews = df_full_file_reviews.select(\r\n",
        "    col(\"_id\").alias(\"product_id\"), \r\n",
        "    col(\"product_reviews._id\").alias(\"review_id\")\r\n",
        ").groupBy(\r\n",
        "    \"product_id\",\r\n",
        "    \"review_id\"\r\n",
        ").count()\r\n",
        "\r\n",
        "row_count = df_unique_reviews.count()\r\n",
        "print(f\"distinct key row count: {row_count}\") # Also 235,018 - cool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Now do we already have these in our bronze table?\r\n",
        "dev_reviews_delta_path = \"abfss://bronze@azwwwnonproddevadapadls.dfs.core.windows.net/BazaarVoice/product_reviews\"\r\n",
        "df_bronze = spark.read.format(\"delta\").load(dev_reviews_delta_path)\r\n",
        "\r\n",
        "# We do have these in our raw data:\r\n",
        "# col(\"product_reviews.LastModificationTime\").alias(\"last_modification_time\"),\r\n",
        "# col(\"product_reviews.LastPublishTime\").alias(\"last_publish_time\"),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_not_matched = df_unique_reviews.join(\r\n",
        "    df_bronze.select(\"product_id\", \"review_id\").distinct(),\r\n",
        "    on=[\"product_id\", \"review_id\"],\r\n",
        "    how=\"left_anti\"\r\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(df_not_matched.count()) # This takes a while...\r\n",
        "# 223,414 - yikes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(df_bronze.count())\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "total_rows = 50854 + 223414\r\n",
        "print(total_rows)\r\n",
        "diff = total_rows - row_count \r\n",
        "print(f\"diff: {diff}\")"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}