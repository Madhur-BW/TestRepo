{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "pip install google-analytics-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from google.oauth2.credentials import Credentials\r\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\r\n",
        "from google.analytics.data_v1beta.types import *\r\n",
        "from google.oauth2 import service_account\r\n",
        "import json\r\n",
        "from pyspark.sql.functions import * \r\n",
        "import concurrent.futures\r\n",
        "from time import sleep \r\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType,IntegerType, DateType,DoubleType, MapType, ArrayType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {},
      "source": [
        "account_name = raw_adls_path.split('@')[1].split('.')[0]\r\n",
        "gold_container = 'gold'\r\n",
        "gold_wolverine_sessions_folder_API = 'GA4/Sessions_wolverine_summary_API_Final'\r\n",
        "gold_delta_table_path_wolverine_API = f\"abfss://{gold_container}@{account_name}.dfs.core.windows.net/{gold_wolverine_sessions_folder_API}\"\r\n",
        "\r\n",
        "#Valid Values for loadt_type are DateRange and Incremental\r\n",
        "#If You provide DateRange then also provide value for start_date and end_date\r\n",
        "load_type = \"Incremental\"\r\n",
        "#load_type = \"DateRange\"\r\n",
        "\r\n",
        "#example of start_date and end_date: \r\n",
        "start_date = \"2024-01-01\"\r\n",
        "end_date = \"2025-05-10\"\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(gold_delta_table_path_wolverine_API)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## get start date range and end date range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Logic based on Incremental and date range Switch\r\n",
        "from datetime import datetime, timedelta\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "def get_fiscal_weeks(start_date: str, end_date: str, jdbcDriver: str, jdbcUrl: str, jdbcUsername: str, jdbcPassword: str):\r\n",
        "    query = f\"\"\"\r\n",
        "        SELECT DISTINCT dd.fiscalweek, dd.weekbegindate, dd.weekenddate\r\n",
        "        FROM report.DateDim dd\r\n",
        "        WHERE daydate BETWEEN '{start_date}' AND '{end_date}'\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    df_date = spark.read.format(\"jdbc\")\\\r\n",
        "        .option(\"driver\", jdbcDriver)\\\r\n",
        "        .option(\"url\", jdbcUrl)\\\r\n",
        "        .option(\"query\", query)\\\r\n",
        "        .option(\"user\", jdbcUsername)\\\r\n",
        "        .option(\"password\", jdbcPassword)\\\r\n",
        "        .load()\r\n",
        "\r\n",
        "    gua_dates_range = df_date.collect()\r\n",
        "    if gua_dates_range:\r\n",
        "        fiscal_week = gua_dates_range[0][0]\r\n",
        "        week_begin_date = gua_dates_range[0][1]\r\n",
        "        week_end_date = gua_dates_range[0][2]\r\n",
        "        print(\"fiscal_week::\", fiscal_week, \"week_begin_date::\", week_begin_date, \"week_end_date::\", week_end_date)\r\n",
        "    else:\r\n",
        "        print(\"No records found for given date range.\")\r\n",
        "\r\n",
        "    display(df_date)\r\n",
        "    return df_date\r\n",
        "\r\n",
        "\r\n",
        "def get_date_range_based_on_loadtype(LoadType: str, jdbcDriver: str, jdbcUrl: str, jdbcUsername: str, jdbcPassword: str,start_date: str, end_date: str):\r\n",
        "    if LoadType == \"DateRange\":\r\n",
        "        # Hardcoded date range\r\n",
        "        return get_fiscal_weeks(start_date, end_date, jdbcDriver, jdbcUrl, jdbcUsername, jdbcPassword)\r\n",
        "\r\n",
        "    elif LoadType == \"Incremental\":\r\n",
        "        today = datetime.today()\r\n",
        "        # Get last Sunday\r\n",
        "        this_week_end = today - timedelta(days=today.weekday() + 1)\r\n",
        "        this_week_start = this_week_end - timedelta(days=6)\r\n",
        "\r\n",
        "        # Previous week\r\n",
        "        prev_week_end = this_week_start - timedelta(days=1)\r\n",
        "        prev_week_start = prev_week_end - timedelta(days=6)\r\n",
        "\r\n",
        "        start_date = prev_week_start.strftime('%Y-%m-%d')\r\n",
        "        end_date = this_week_end.strftime('%Y-%m-%d')\r\n",
        "\r\n",
        "        print(f\"Computed Incremental Date Range: {start_date} to {end_date}\")\r\n",
        "        return get_fiscal_weeks(start_date, end_date, jdbcDriver, jdbcUrl, jdbcUsername, jdbcPassword)\r\n",
        "\r\n",
        "    else:\r\n",
        "        raise ValueError(\"Invalid LoadType. Choose either 'DateRange' or 'Incremental'.\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Incremental Load or Date Range\r\n",
        "Normally we will run the incremental load. In case we need to backfill historical data we can run this with a specific date range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "\r\n",
        "#df_date = get_date_range_based_on_loadtype(\"DateRange\", jdbcDriver, jdbcUrl, jdbcUsername, jdbcPassword, start_date, end_date)\r\n",
        "# or\r\n",
        "df_date = get_date_range_based_on_loadtype(\"Incremental\", jdbcDriver, jdbcUrl, jdbcUsername, jdbcPassword, start_date, end_date)\r\n",
        "\r\n",
        "gua_dates_range = df_date.collect()\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "History Run Log<br>\r\n",
        "2024-12-29 - 2025-05-10<br>\r\n",
        "2023-12-31 - 2024-12-28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Defining Schema for GA4 Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Schema without financial fields\r\n",
        "ga4_schema = StructType([\r\n",
        "StructField('brand',StringType(), True)\r\n",
        ",StructField('device_category',StringType(), True)\r\n",
        ",StructField('sessions',IntegerType(), True)\r\n",
        ",StructField('week_end',StringType(), True)\r\n",
        "])\r\n",
        "\r\n",
        "ga4_qty_schema = StructType([\r\n",
        "StructField('brand',StringType(), True)\r\n",
        ",StructField('device_category',StringType(), True)\r\n",
        "#,StructField('quantity',IntegerType(), True)\r\n",
        ",StructField('week_end',StringType(), True)\r\n",
        "])\r\n",
        "qty_data = []\r\n",
        "non_qty_data = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# List All Properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {},
      "source": [
        "#V3 only sessions\r\n",
        "\r\n",
        "from google.analytics.data_v1beta import BetaAnalyticsDataClient\r\n",
        "from google.analytics.data_v1beta.types import RunReportRequest, DateRange, Metric, Dimension, Filter, FilterExpression\r\n",
        "from google.oauth2 import service_account\r\n",
        "import json\r\n",
        "from time import sleep\r\n",
        "\r\n",
        "# Full property_brand mapping\r\n",
        "property_brand = {\r\n",
        "    297479542: 'Saucony US', 309617502: 'Merrell US', 309589213: 'Chacos US', 309592771: 'Cat Footwear US',\r\n",
        "    309607030: 'Harley-Davidson Footwear US', 309561793: 'Grasshoppers US', 309609458: 'Keds US',\r\n",
        "    309650146: 'Hush Puppies US', 309592118: 'OnlineShoes US', 309626252: 'Prokeds US',\r\n",
        "    309606225: 'Merrell BE', 309619975: 'Hytest US', 309616895: 'Hush Puppies Canada',\r\n",
        "    309620743: 'Merrell ES', 309599233: 'Cat Footwear EMEA Emerging', 309591384: 'Wolverine US',\r\n",
        "    309561205: 'Cat Footwear Canada', 309607124: 'Keds Canada', 309621431: 'Cat Footwear DE',\r\n",
        "    309591477: 'Merrell SE', 309615440: 'Merrell DE', 309613069: 'Cat Footwear UK',\r\n",
        "    309607810: 'Merrell Canada', 309628914: 'Merrell EMEA Emerging', 309607948: 'Saucony FR',\r\n",
        "    309643444: 'Saucony EMEA Emerging', 309621834: 'Merrell NL', 309596198: 'Saucony Canada',\r\n",
        "    309599329: 'Merrell UK', 309579645: 'Merrell FR', 309599656: 'Wolverine Canada',\r\n",
        "    309624452: 'Saucony DE', 309600562: 'Saucony BE', 309629089: 'Saucony ES',\r\n",
        "    309537329: 'Saucony IT', 309587387: 'Saucony UK', 309603632: 'Saucony NL',\r\n",
        "    309713727: 'Bates US', 310709711: 'Saucony AT', 312016432: 'Dev',\r\n",
        "    312419831: 'EMEA Roll Up', 312454140: 'Canada Roll Up', 312455238: 'Global Roll up',\r\n",
        "    315639705: 'Merrell EMEA Roll Up', 315628392: 'Saucony EMEA Roll Up',\r\n",
        "    315625200: 'Cat Footwear EMEA Roll Up', 428511278: 'Server-side GTM',\r\n",
        "    433745652: 'Wolverine4Work', 440572276: 'WWW Meta Shopping',\r\n",
        "    309632059: 'Sperry US', 309650574: 'Sperry Canada'\r\n",
        "}\r\n",
        "\r\n",
        "# Exclusion list\r\n",
        "exclusion_list = {\r\n",
        "    309713727, 309561793, 309607030, 309650146, 309609458,\r\n",
        "    309607124, 309626252, 428511278, 309650574, 309632059,\r\n",
        "    309599656, 309616895\r\n",
        "}\r\n",
        "\r\n",
        "all_properties = list(property_brand.keys())\r\n",
        "property_list = [p for p in all_properties if p not in exclusion_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define all Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {},
      "source": [
        "non_qty_data = []\r\n",
        "\r\n",
        "metrics = [Metric(name=\"sessions\")]\r\n",
        "dimensions = [Dimension(name=\"deviceCategory\")]\r\n",
        "\r\n",
        "def initialize_analyticsreporting():\r\n",
        "    file_text = spark.read.text(f\"{raw_adls_path}GA4/credentials.json\", wholetext=True)\r\n",
        "    credential_data = file_text.collect()[0][0]\r\n",
        "    credentials = service_account.Credentials.from_service_account_info(json.loads(credential_data))\r\n",
        "    return BetaAnalyticsDataClient(credentials=credentials)\r\n",
        "\r\n",
        "client = initialize_analyticsreporting()\r\n",
        "\r\n",
        "def get_report(property_id, p_date_range, metric_list, max_retries=3):\r\n",
        "    for attempt in range(max_retries):\r\n",
        "        try:\r\n",
        "            request = RunReportRequest(\r\n",
        "                property=f\"properties/{property_id}\",\r\n",
        "                date_ranges=[p_date_range],\r\n",
        "                dimensions=dimensions,\r\n",
        "                metrics=metric_list,\r\n",
        "                dimension_filter=FilterExpression(\r\n",
        "                    filter=Filter(\r\n",
        "                        field_name=\"deviceCategory\",\r\n",
        "                        in_list_filter=Filter.InListFilter(values=[\"tablet\", \"desktop\", \"mobile\"])\r\n",
        "                    )\r\n",
        "                ),\r\n",
        "            )\r\n",
        "            response = client.run_report(request)\r\n",
        "            sleep(5)\r\n",
        "            return response\r\n",
        "        except Exception as e:\r\n",
        "            print(f\"Attempt {attempt + 1} failed for property {property_id}: {e}\")\r\n",
        "            sleep(2 * (attempt + 1))\r\n",
        "    raise RuntimeError(f\"All {max_retries} attempts failed for property {property_id}\")\r\n",
        "\r\n",
        "\r\n",
        "def write_to_delta(df, delta_table_path: str):\r\n",
        "    try:\r\n",
        "        df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\r\n",
        "        print(f\"Successfully wrote data to Delta table at: {delta_table_path}\")\r\n",
        "    except Exception as e:\r\n",
        "        print(f\"Failed to write to Delta table at {delta_table_path}: {e}\")\r\n",
        "\r\n",
        "def get_ga4_data(p_date_range, property_id):\r\n",
        "    print(f\"Date Range: {p_date_range.start_date} to {p_date_range.end_date}\")\r\n",
        "    print(f\"Property ID: {property_id}\")\r\n",
        "    response = get_report(property_id, p_date_range, metrics)\r\n",
        "    return extract_data(response, property_id, p_date_range.end_date)\r\n",
        "\r\n",
        "def extract_data(response, property_id, p_end_date):\r\n",
        "    result = []\r\n",
        "    for row in response.rows:\r\n",
        "        dimension_value = row.dimension_values[0].value\r\n",
        "        sessions = int(row.metric_values[0].value)\r\n",
        "        result.append((\r\n",
        "            property_brand.get(property_id, f\"Unknown-{property_id}\"),\r\n",
        "            dimension_value, sessions, p_end_date\r\n",
        "        ))\r\n",
        "    return result\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Call and process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {},
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\r\n",
        "from threading import Lock\r\n",
        "from pyspark.sql.functions import col, lit, regexp_replace, to_timestamp, date_format, to_utc_timestamp\r\n",
        "from builtins import min  # Ensure Python's min is used\r\n",
        "\r\n",
        "# Create a thread lock for writing to Delta\r\n",
        "delta_write_lock = Lock()\r\n",
        "\r\n",
        "def safe_write(df):\r\n",
        "    with delta_write_lock:\r\n",
        "        write_to_delta(df, gold_delta_table_path_wolverine_API)\r\n",
        "\r\n",
        "#New Logic replace Canada as CA\r\n",
        "from pyspark.sql.functions import col, regexp_replace, date_format, to_timestamp, to_utc_timestamp, lit\r\n",
        "\r\n",
        "def process_date_range(k):\r\n",
        "    date_range = DateRange(\r\n",
        "        start_date=k.weekbegindate.isoformat(),\r\n",
        "        end_date=k.weekenddate.isoformat()\r\n",
        "    )\r\n",
        "    print(f\"\\nProcessing Date Range: {date_range.start_date} to {date_range.end_date}\")\r\n",
        "\r\n",
        "    for property_index, property_id in enumerate(property_list, start=1):\r\n",
        "        print(f\"  Property {property_index} of {len(property_list)}: {property_id} - {property_brand.get(property_id, 'Unknown')}\")\r\n",
        "\r\n",
        "        # Thread-safe: get data as return value\r\n",
        "        data = get_ga4_data(date_range, property_id)\r\n",
        "\r\n",
        "        if data:\r\n",
        "            df = spark.createDataFrame(data=data, schema=ga4_schema)\r\n",
        "\r\n",
        "            final_df = (\r\n",
        "                df\r\n",
        "                .withColumnRenamed(\"device_category\", \"device_type\")\r\n",
        "                .withColumn(\"BrandCountryKey\", regexp_replace(col(\"brand\"), \" \", \"-\"))\r\n",
        "                .withColumn(\"BrandCountryKey\", regexp_replace(col(\"BrandCountryKey\"), \"-Canada\", \"-CA\"))\r\n",
        "                .withColumn(\"calday\", date_format(to_timestamp(\"week_end\"), \"yyyyMMdd\"))\r\n",
        "                .withColumn(\"est_date\", to_timestamp(\"week_end\"))\r\n",
        "                .withColumn(\"gmt_date\", to_utc_timestamp(col(\"est_date\"), \"America/New_York\"))\r\n",
        "                .withColumn(\"source\", lit(None).cast(\"string\"))\r\n",
        "                .withColumn(\"medium\", lit(None).cast(\"string\"))\r\n",
        "                .select(\"BrandCountryKey\", \"calday\", \"est_date\", \"gmt_date\", \"sessions\", \"device_type\", \"source\", \"medium\")\r\n",
        "            )\r\n",
        "\r\n",
        "            safe_write(final_df)\r\n",
        "        else:\r\n",
        "            print(f\"    No data returned for property {property_id} on {date_range.start_date}\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Multi thread Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Thread pool config â€” safe for medium Spark driver\r\n",
        "num_threads = min(8, len(gua_dates_range))\r\n",
        "\r\n",
        "# Execute threads\r\n",
        "with ThreadPoolExecutor(max_workers=num_threads) as executor:\r\n",
        "    futures = [executor.submit(process_date_range, k) for k in gua_dates_range]\r\n",
        "\r\n",
        "    for future in as_completed(futures):\r\n",
        "        try:\r\n",
        "            future.result()\r\n",
        "        except Exception as e:\r\n",
        "            print(f\"Error during threaded processing: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Read Back Data and Deduplicate\r\n",
        "If the code to get data from APIs gets run multiple times we may get duplicate data. The following code will read data, deduplicate it and save back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "metadata": {},
      "source": [
        "def read_from_delta(delta_table_path: str):\r\n",
        "    try:\r\n",
        "        df = spark.read.format(\"delta\").load(delta_table_path)\r\n",
        "        print(f\"Successfully read data from Delta table at: {delta_table_path}\")\r\n",
        "        return df\r\n",
        "    except Exception as e:\r\n",
        "        print(f\"Failed to read from Delta table at {delta_table_path}: {e}\")\r\n",
        "        return None\r\n",
        "\r\n",
        "def deduplicate_and_overwrite_all_columns(delta_table_path: str):\r\n",
        "    df = read_from_delta(delta_table_path)\r\n",
        "    if df is not None:\r\n",
        "        # Deduplicate across all columns\r\n",
        "        df_deduped = df.dropDuplicates()\r\n",
        "        try:\r\n",
        "            df_deduped.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\r\n",
        "            print(f\"Successfully overwrote Delta table at: {delta_table_path} with deduplicated data\")\r\n",
        "        except Exception as e:\r\n",
        "            print(f\"Failed to overwrite Delta table at {delta_table_path}: {e}\")\r\n",
        "\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Deduplicate with Total Sessions Rank\r\n",
        "We see duplicates if the number of sessions changes during multiple runs. Hence we take the highest number of sessions.\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.window import Window\r\n",
        "from pyspark.sql.functions import row_number, col\r\n",
        "\r\n",
        "def deduplicate_with_top_rank(delta_table_path: str):\r\n",
        "    df_wolverine_api = read_from_delta(delta_table_path)\r\n",
        "    # Define the window specification\r\n",
        "    window_spec = Window.partitionBy(\"BrandCountryKey\", \"calday\", \"device_type\") \\\r\n",
        "                        .orderBy(col(\"sessions\").desc())\r\n",
        "\r\n",
        "    # Assign row numbers within each partition\r\n",
        "    ranked_df = df_wolverine_api.withColumn(\"row_num\", row_number().over(window_spec))\r\n",
        "\r\n",
        "    # Filter to keep only the top row per group\r\n",
        "    df_top_sessions = ranked_df.filter(col(\"row_num\") == 1).drop(\"row_num\")\r\n",
        "    try:\r\n",
        "        df_top_sessions.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\r\n",
        "        print(f\"Successfully overwrote Delta table at: {delta_table_path} with deduplicated data\")\r\n",
        "    except Exception as e:\r\n",
        "        print(f\"Failed to overwrite Delta table at {delta_table_path}: {e}\")\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "metadata": {},
      "source": [
        "deduplicate_and_overwrite_all_columns(gold_delta_table_path_wolverine_API)\r\n",
        "deduplicate_with_top_rank(gold_delta_table_path_wolverine_API)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Duplicate Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import col\r\n",
        "\r\n",
        "def verify_no_duplicates(delta_table_path: str):\r\n",
        "    try:\r\n",
        "        df = spark.read.format(\"delta\").load(delta_table_path)\r\n",
        "        total_count = df.count()\r\n",
        "        distinct_count = df.distinct().count()\r\n",
        "        \r\n",
        "        print(f\"Total rows: {total_count}\")\r\n",
        "        print(f\"Distinct rows: {distinct_count}\")\r\n",
        "        \r\n",
        "        if total_count == distinct_count:\r\n",
        "            print(\"No duplicates found in the Delta table.\")\r\n",
        "        else:\r\n",
        "            print(\"Duplicates still exist in the Delta table.\")\r\n",
        "    except Exception as e:\r\n",
        "        print(f\"Error verifying duplicates in Delta table at {delta_table_path}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [],
      "metadata": {},
      "source": [
        "verify_no_duplicates(gold_delta_table_path_wolverine_API)"
      ]
    }
  ],
  "metadata": {
    "description": "ingest ga4 data using data api",
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}