{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import concurrent.futures\n",
        "from delta import *\n",
        "from pyspark.sql.types import StructType, StructField, ArrayType, StringType, LongType, DoubleType, BooleanType, MapType,IntegerType\n",
        "from pyspark.sql.functions import *\n",
        "from functools import reduce\n",
        "from pyspark.sql.dataframe import DataFrame\n",
        "import pyspark.sql.functions as F\n",
        "import json\n",
        "import base64\n",
        "from datetime import datetime,timedelta\n",
        "from time import sleep\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"DYNAMIC\")\n",
        "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from pyspark.sql.functions import max as spark_max"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Run Common Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run /utils/common_functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "account_name = raw_adls_path.split('@')[1].split('.')[0]\n",
        "sb_raw_base_folder = \"GA4_SweatyBetty/analytics_292120381/events\"\n",
        "gold_container = 'gold'\n",
        "bronze_container = 'bronze'\n",
        "gold_sb_sessions_folder = 'GA4/Sessions_SweatyBetty'\n",
        "bronze_sb_unique_sessions_folder = 'GA4/Sessions_SweatyBetty_unique_sessions'\n",
        "\n",
        "gold_delta_table_path_sb = f\"abfss://{gold_container}@{account_name}.dfs.core.windows.net/{gold_sb_sessions_folder}\"\n",
        "bronze_delta_table_path_unique_sessions_sb = f\"abfss://{bronze_container}@{account_name}.dfs.core.windows.net/{bronze_sb_unique_sessions_folder}\"\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(f\"account_name: {account_name}\")\n",
        "print(f\"sb_raw_base_folder: {sb_raw_base_folder}\")\n",
        "print(f\"gold_container: {gold_container}\")\n",
        "print(f\"gold_sb_sessions_folder: {gold_sb_sessions_folder}\")\n",
        "print(f\"bronze_sb_unique_sessions_folder: {bronze_sb_unique_sessions_folder}\")\n",
        "print(f\"gold_delta_table_path_sb: {gold_delta_table_path_sb}\")\n",
        "print(f\"bronze_delta_table_path_unique_sessions_sb: {bronze_delta_table_path_unique_sessions_sb}\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "keyvult_key = 'storage-key'\n",
        "account_key = mssparkutils.credentials.getSecret(kv_name , keyvult_key,'ls_kv_adap' )\n",
        "storage_account_name = raw_adls_path.split('@')[1].split('.')[0]\n",
        "container_name = 'raw'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Read raw data and get unique sessions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#New Logic - But Separate from DBT\n",
        "\n",
        "from pyspark.sql.functions import (\n",
        "    col, from_unixtime, from_utc_timestamp,\n",
        "    expr, lower, concat_ws, when, count, min, lit\n",
        ")\n",
        "\n",
        "def ga4_events_get_unique_sessions(df):\n",
        "    # Set brand to static value\n",
        "    brand_col = lit(\"sweatybetty\")\n",
        "\n",
        "    # Convert timestamps\n",
        "    est_col = from_utc_timestamp(from_unixtime(col(\"event_timestamp\") / 1_000_000), \"America/New_York\")\n",
        "    gmt_col = from_unixtime(col(\"event_timestamp\") / 1_000_000)\n",
        "\n",
        "    # Handle country null or empty\n",
        "    country_col = when(\n",
        "        (col(\"geo.country\").isNull()) | (col(\"geo.country\") == \"\"), \n",
        "        \"none\"\n",
        "    ).otherwise(col(\"geo.country\")).alias(\"country\")\n",
        "\n",
        "    # Define country code column\n",
        "    country_code_col = when(col(\"geo.country\") == \"Canada\", \"CA\") \\\n",
        "        .when(col(\"geo.country\") == \"Ireland\", \"IE\") \\\n",
        "        .when(col(\"geo.country\") == \"United States\", \"US\") \\\n",
        "        .when(col(\"geo.country\") == \"United Kingdom\", \"UK\") \\\n",
        "        .otherwise(\"ROW\") \\\n",
        "        .alias(\"CountryCode\")\n",
        "\n",
        "    # Build base DataFrame\n",
        "    df_sessions = df.select(\n",
        "        col(\"event_date\").alias(\"calday\"),\n",
        "        col(\"user_pseudo_id\"),\n",
        "        col(\"event_params\")[\"ga_session_id\"][\"int_value\"].alias(\"session_id\"),\n",
        "        col(\"device.category\").alias(\"device_type\"),\n",
        "        country_col,\n",
        "        country_code_col,\n",
        "        est_col.alias(\"est_datetime\"),\n",
        "        gmt_col.alias(\"gmt_datetime\"),\n",
        "        concat_ws(\"-\", brand_col, col(\"CountryCode\")).alias(\"BrandCountryKey\"),\n",
        "        col(\"traffic_source.source\").alias(\"source\"),\n",
        "        col(\"traffic_source.medium\").alias(\"medium\")\n",
        "    )\n",
        "\n",
        "    # Drop null session_id and deduplicate\n",
        "    df_unique_sessions = df_sessions.dropna(subset=[\"session_id\"]) \\\n",
        "        .dropDuplicates([\"user_pseudo_id\", \"session_id\", \"calday\"])\n",
        "\n",
        "    # Aggregate\n",
        "    '''\n",
        "    df_result = df_unique_sessions.groupBy(\n",
        "        \"BrandCountryKey\", \"calday\", \"device_type\", \"source\", \"medium\", \"country\", \"CountryCode\"\n",
        "    ).agg(\n",
        "        count(\"*\").alias(\"sessions\"),\n",
        "        min(\"est_datetime\").alias(\"est_date\"),\n",
        "        min(\"gmt_datetime\").alias(\"gmt_date\")\n",
        "    )\n",
        "    '''\n",
        "\n",
        "    # Select final columns including user_pseudo_id and session_id\n",
        "    df_result = df_unique_sessions.select(\n",
        "        \"BrandCountryKey\",\n",
        "        \"calday\",\n",
        "        col(\"est_datetime\").alias(\"est_date\"),\n",
        "        col(\"gmt_datetime\").alias(\"gmt_date\"),\n",
        "        \"user_pseudo_id\",\n",
        "        \"session_id\",\n",
        "        \"device_type\",\n",
        "        \"source\",\n",
        "        \"medium\",\n",
        "        \"country\",\n",
        "        \"CountryCode\"\n",
        "    )\n",
        "\n",
        "    return df_result\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define function to Save to Delta Lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def write_to_delta(df, delta_table_path: str):\n",
        "    try:\n",
        "        df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
        "        print(f\"Successfully wrote data to Delta table at: {delta_table_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to write to Delta table at {delta_table_path}: {e}\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Function to Iterate over Sweaty Betty Raw Data\n",
        "Save Unique Sessions to Bronze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Full Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(f\"bronze_delta_table_path_unique_sessions_sb: {bronze_delta_table_path_unique_sessions_sb}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import re\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.dataframe import DataFrame\n",
        "\n",
        "def process_SB_tables(raw_adls_path: str, base_folder: str, spark: SparkSession, bronze_delta_table_path_unique_sessions_sb: str):\n",
        "\n",
        "    full_path = f\"{raw_adls_path}{base_folder}\".rstrip(\"/\")\n",
        "\n",
        "    try:\n",
        "        entries = mssparkutils.fs.ls(full_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error listing files in path: {full_path}\\n{e}\")\n",
        "        return\n",
        "\n",
        "    for entry in entries:\n",
        "        folder_name = entry.name.rstrip(\"/\")\n",
        "        if re.match(r\"events_\\d{8}\", folder_name):\n",
        "            table_path = f\"{full_path}/{folder_name}\"\n",
        "            try:\n",
        "                df: DataFrame = spark.read.format(\"parquet\").load(table_path)\n",
        "\n",
        "                # === PLACEHOLDER FOR CUSTOM LOGIC ===\n",
        "                #transformed_df = transform_ga4_events(df)\n",
        "                unique_sessions_df = ga4_events_get_unique_sessions(df)\n",
        "                print(f\"Processing table to bronze: {folder_name}\")\n",
        "                write_to_delta(unique_sessions_df,bronze_delta_table_path_unique_sessions_sb)\n",
        "                \n",
        "\n",
        "            except Exception as read_err:\n",
        "                print(f\"Failed to process table {folder_name}: {read_err}\")\n",
        "                break\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "# This is full Processing Function call. It will process all the Raw files that are there, to the bronze\n",
        "'''\n",
        "process_SB_tables(\n",
        "    raw_adls_path=raw_adls_path,\n",
        "    base_folder=sb_raw_base_folder,\n",
        "    spark=spark,\n",
        "    bronze_delta_table_path_unique_sessions_sb = bronze_delta_table_path_unique_sessions_sb\n",
        ")\n",
        "'''\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Incremental Function for Sweaty Betty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import re\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.dataframe import DataFrame\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "\n",
        "def process_SB_tables_incremental(raw_adls_path: str, base_folder: str, spark: SparkSession,\n",
        "                                   bronze_delta_table_path_unique_sessions_sb: str,\n",
        "                                   storage_account_name: str, account_key: str):\n",
        "\n",
        "    # Read watermark\n",
        "    watermark_blob_path = \"GA4/sb_sessions_watermark.json\"\n",
        "    blob_service_client = BlobServiceClient(account_url=f\"https://{storage_account_name}.blob.core.windows.net\",\n",
        "                                            credential=account_key)\n",
        "    container_client = blob_service_client.get_container_client(\"raw\")\n",
        "    blob_client = container_client.get_blob_client(watermark_blob_path)\n",
        "\n",
        "    try:\n",
        "        blob_data = blob_client.download_blob().readall()\n",
        "        blob_content = json.loads(blob_data)\n",
        "        last_processed = blob_content.get(\"last_processed_folder\", \"\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not read watermark. Defaulting to no previous folder.\")\n",
        "        last_processed = \"\"\n",
        "\n",
        "    full_path = f\"{raw_adls_path}{base_folder}\".rstrip(\"/\")\n",
        "\n",
        "    try:\n",
        "        entries = mssparkutils.fs.ls(full_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error listing files in path: {full_path}\\n{e}\")\n",
        "        return\n",
        "\n",
        "    # Filter and sort folder list\n",
        "    valid_folders = sorted([\n",
        "        entry.name.rstrip(\"/\") for entry in entries if re.match(r\"events_\\d{8}\", entry.name)\n",
        "    ])\n",
        "\n",
        "    # Only process folders after watermark\n",
        "    folders_to_process = [f for f in valid_folders if f > last_processed]\n",
        "\n",
        "    if not folders_to_process:\n",
        "        print(\"No new folders to process.\")\n",
        "        return\n",
        "\n",
        "    for folder_name in folders_to_process:\n",
        "        table_path = f\"{full_path}/{folder_name}\"\n",
        "        try:\n",
        "            df: DataFrame = spark.read.format(\"parquet\").load(table_path)\n",
        "            unique_sessions_df = ga4_events_get_unique_sessions(df)\n",
        "            print(f\"Processing table to bronze: {folder_name}\")\n",
        "            write_to_delta(unique_sessions_df,bronze_delta_table_path_unique_sessions_sb)\n",
        "\n",
        "            # Update watermark after successful processing\n",
        "            watermark_data = json.dumps({\"last_processed_folder\": folder_name})\n",
        "            blob_client.upload_blob(watermark_data, overwrite=True)\n",
        "            print(f\"Watermark updated to: {folder_name}\")\n",
        "\n",
        "        except Exception as read_err:\n",
        "            print(f\"Failed to process table {folder_name}: {read_err}\")\n",
        "            break\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(f\"account_name: {account_name}\")\n",
        "print(f\"sb_raw_base_folder: {sb_raw_base_folder}\")\n",
        "print(f\"gold_container: {gold_container}\")\n",
        "print(f\"gold_sb_sessions_folder: {gold_sb_sessions_folder}\")\n",
        "print(f\"bronze_sb_unique_sessions_folder: {bronze_sb_unique_sessions_folder}\")\n",
        "print(f\"gold_delta_table_path_sb: {gold_delta_table_path_sb}\")\n",
        "print(f\"bronze_delta_table_path_unique_sessions_sb: {bronze_delta_table_path_unique_sessions_sb}\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Call Sweaty Betty Incremental function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "process_SB_tables_incremental(raw_adls_path = raw_adls_path,\n",
        "                              base_folder=sb_raw_base_folder,\n",
        "                              bronze_delta_table_path_unique_sessions_sb = bronze_delta_table_path_unique_sessions_sb,\n",
        "                              storage_account_name = storage_account_name,\n",
        "                              account_key=account_key,\n",
        "                              spark=spark\n",
        "                              )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Deduplicate Bronze data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def read_from_delta(delta_table_path: str):\n",
        "    try:\n",
        "        df = spark.read.format(\"delta\").load(delta_table_path)\n",
        "        print(f\"Successfully read data from Delta table at: {delta_table_path}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to read from Delta table at {delta_table_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def deduplicate_and_overwrite_all_columns(delta_table_path: str):\n",
        "    df = read_from_delta(delta_table_path)\n",
        "    if df is not None:\n",
        "        # Deduplicate across all columns\n",
        "        df_deduped = df.dropDuplicates()\n",
        "        try:\n",
        "            df_deduped.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
        "            print(f\"Successfully overwrote Delta table at: {delta_table_path} with deduplicated data\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to overwrite Delta table at {delta_table_path}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(bronze_delta_table_path_unique_sessions_sb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format(\"delta\").load(bronze_delta_table_path_unique_sessions_sb)\n",
        "\n",
        "# Print the schema\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "deduplicate_and_overwrite_all_columns(bronze_delta_table_path_unique_sessions_sb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#deduplicate based on calday\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, col\n",
        "\n",
        "def deduplicate_on_specific_columns(delta_table_path: str):\n",
        "    df = read_from_delta(delta_table_path)\n",
        "    if df is not None:\n",
        "        # Use calday instead of est_date\n",
        "        window_spec = Window.partitionBy(\"calday\", \"session_id\", \"device_type\",\"source\") \\\n",
        "                            .orderBy(col(\"calday\").desc())\n",
        "\n",
        "        # Add a row number within each partition\n",
        "        df_ranked = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
        "\n",
        "        # Filter to keep only the first row per group\n",
        "        df_deduped = df_ranked.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
        "\n",
        "        try:\n",
        "            df_deduped.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
        "            print(f\"Successfully overwrote Delta table at: {delta_table_path} with deduplicated data\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to overwrite Delta table at {delta_table_path}: {e}\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "deduplicate_on_specific_columns(bronze_delta_table_path_unique_sessions_sb)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Process Gold Aggregate from Bronze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.functions import count, min\n",
        "\n",
        "def read_from_delta(delta_table_path: str) -> DataFrame:\n",
        "    try:\n",
        "        df = spark.read.format(\"delta\").load(delta_table_path)\n",
        "        print(f\"Successfully read data from Delta table at: {delta_table_path}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to read from Delta table at {delta_table_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def write_to_delta_overwrite(df: DataFrame, delta_table_path: str):\n",
        "    try:\n",
        "        df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
        "        print(f\"Successfully wrote data to Delta table at: {delta_table_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to write to Delta table at {delta_table_path}: {e}\")\n",
        "\n",
        "def process_sessions_and_write_to_gold(bronze_path: str, gold_path: str):\n",
        "    df = read_from_delta(bronze_path)\n",
        "    if df is None:\n",
        "        print(\"Aborting: Failed to read from bronze path.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        df_result = df.groupBy(\n",
        "            \"BrandCountryKey\", \"calday\", \"device_type\", \"source\", \"medium\", \"country\", \"CountryCode\"\n",
        "        ).agg(\n",
        "            count(\"*\").alias(\"sessions\"),\n",
        "            min(\"est_date\").alias(\"est_date\"),\n",
        "            min(\"gmt_date\").alias(\"gmt_date\")\n",
        "        )\n",
        "        write_to_delta_overwrite(df_result, gold_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed during processing or writing: {e}\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(bronze_delta_table_path_unique_sessions_sb)\n",
        "print(gold_delta_table_path_sb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "process_sessions_and_write_to_gold(\n",
        "    bronze_delta_table_path_unique_sessions_sb,\n",
        "    gold_delta_table_path_sb\n",
        ")\n",
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}